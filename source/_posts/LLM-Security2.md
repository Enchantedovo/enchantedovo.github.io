---
title: LLM for Security 论文记录 0919
tags:
  - 大模型
  - 网络安全
  - 大模型安全
  - 生成式人工智能
categories:
  - [大模型,大模型&安全]
  - [大模型,论文阅读]
description: LLM for Security 论文记录 0919
cover: https://s3.bmp.ovh/imgs/2024/04/08/2368e73f30658fe8.png
date: 2024-09-19 10:54:14
---
> 放一些大模型与安全相关的论文（涵盖范围比较广，比较杂）

# 相关文章

**GitHub Copilot 在编程方面表现出色，但它是否确保了负责任的输出？**

简介：大语言模型（LLMs）的快速发展极大地推动了代码补全工具（LCCTs）的进化。这些工具通过整合多种信息源并优先提供代码建议，与传统LLMs不同，它们在安全方面面临独特挑战。特别是，LCCTs在训练时依赖专有代码数据集，这可能增加敏感数据泄露的风险。研究表明，LCCTs存在显著的安全漏洞，例如，针对GitHub Copilot的越狱攻击成功率高达99.4%，而从其中提取敏感用户数据的成功率也相当高。这些发现强调了LCCTs在安全方面的挑战，并为加强其安全框架提供了重要方向。

链接：https://arxiv.org/abs/2408.11006

<br>

**语言模型应用程序中的数据泄露：对 OpenAI 的 GPT 产品的深入调查**

简介：研究者们在探索大语言模型（LLM）应用的数据实践透明度时，以OpenAI的GPT应用生态系统作为案例研究。他们开发了一个基于LLM的框架，对GPT及其动作（外部服务）的源代码进行了静态分析，以揭示其数据收集的做法。研究发现，这些动作收集了大量用户数据，包括OpenAI明令禁止的敏感信息，如密码。此外，一些与广告和分析相关的动作嵌入在多个GPT中，使得它们能够跨平台追踪用户活动。动作的共现还可能导致用户数据的进一步暴露，增加了隐私风险。研究者还开发了一个基于LLM的隐私政策分析框架，用以自动检查动作的数据收集是否与隐私政策中的披露相一致。结果显示，大多数收集的数据类型在隐私政策中并未明确披露，仅有5.8%的动作清晰地说明了它们的数据收集实践。这一发现强调了LLM应用在数据实践透明度方面存在的问题，并指出了加强隐私保护措施的必要性。

链接：https://arxiv.org/abs/2408.13247

<br>

**去伪存真：利用执行反馈对生成的代码候选进行排序**

简介：大语言模型（LLMs）如GPT-4、StarCoder和CodeLlama正在改变编程方式，通过自然语言描述自动生成代码。尽管如此，生成正确代码仍然具有挑战性。为了提高正确代码的生成率，开发者通常使用LLMs生成多个候选解决方案，然后进行代码排名，即从多个候选代码中选择正确的一个。现有的代码排名研究主要分为基于执行和非基于执行的方法。基于执行的方法虽然有效，但受限于高质量单元测试的稀缺和潜在的安全风险。而非基于执行的方法，如CodeRanker，主要依赖分类标签进行训练，难以捕捉细微错误和提供错误洞察。

为了克服这些限制，研究者提出了一种新的方法——RankEF。RankEF是一种创新的代码排名方法，它利用执行反馈并通过多任务学习整合代码分类与执行反馈生成。这种方法使模型能够在不执行代码的情况下，理解错误代码的原因，并区分正确与错误的解决方案。在三个代码生成基准上的实验显示，RankEF显著优于现有的CodeRanker，展现出在代码排名方面的高效性和准确性。

链接：https://arxiv.org/abs/2408.13976

<br>

**调查贝叶斯垃圾邮件过滤器在检测经语言模型修改的垃圾邮件中的有效性**

简介：垃圾邮件和网络钓鱼是网络安全的重大威胁，贝叶斯垃圾邮件过滤器如 SpamAssassin 是重要防御工具。但大语言模型的出现带来新挑战，因其强大、易获取且成本低，可能被用于制作复杂垃圾邮件逃避传统过滤器。研究者开发管道测试 SpamAssassin 对经语言模型修改邮件的有效性，结果显示其会将高达 73.7%的此类邮件误分类为合法邮件，而简单字典替换攻击成功率仅 0.4%。这凸显了经语言模型修改的垃圾邮件的重大威胁及成本效益。该研究为当前垃圾邮件过滤器的漏洞及网络安全措施的持续改进提供了关键见解。

链接：https://arxiv.org/abs/2408.14293

<br>

**检测人工智能缺陷：针对语言模型内部故障的目标驱动攻击**

简介：大语言模型（LLMs）在人工智能领域的重要性日益凸显，但这些模型在预训练语料中可能包含有害内容，导致生成不适当的输出。为了提高LLMs的安全性，研究人员探索了检测模型内部缺陷的方法。目前的研究主要集中在越狱攻击上，这些攻击通过构建对抗性内容来诱导模型产生意外响应。然而，这些方法依赖于提示工程，既耗时又需要特别设计的问题。

为了解决这些挑战，研究人员提出了一种新的攻击范式，即目标驱动的攻击，它专注于直接引出目标响应，而不是优化提示。研究中引入了名为ToxDet的LLM，作为有毒内容的检测器。ToxDet能够根据目标有毒响应生成可能的问题和初步答案，以诱导目标模型产生与提供含义相当的有毒响应。ToxDet通过与目标LLM交互并接收奖励信号进行训练，利用强化学习优化过程。尽管ToxDet主要针对开源LLMs，但经过微调后，它也可以转移到攻击如GPT-4o这样的黑盒模型，并取得了显著结果。在AdvBench和HH-Harmless数据集上的实验结果证明了该方法在检测目标LLMs生成有害响应倾向方面的有效性。这不仅揭示了LLMs的潜在漏洞，还为研究人员提供了加强模型抵御此类攻击的宝贵资源。

链接：https://arxiv.org/abs/2408.14853

<br>

**参数高效的量化专家混合体与视觉 - 语言指令调优在半导体电子显微图像分析中的应用**

简介：研究者指出半导体在基础模型中研究不足，需增强半导体器件技术。为此，他们推出了 sLAVA，一个针对半导体制造的小型视觉语言助手，专注于电子显微镜图像分析。采用师生范式，以 GPT-4 等基础视觉语言模型为教师，为 sLAVA 创建遵循指令的多模态数据，解决数据稀缺问题，可在预算有限的消费级硬件上进行任务，且企业能在自身基础设施内用专有数据安全微调框架以保护知识产权。严格实验表明，该框架超越传统方法，能处理数据偏移并实现高通量筛选，有助于半导体制造中的电子显微镜图像分析，为企业提供了一种有效的解决方案，也为半导体技术发展提供了新的思路和方法。

链接：https://arxiv.org/abs/2408.15305
<!-- {% asset_img p1.png p1 %} -->
<!-- <img src="p1.png"  style="zoom:55%;" /> -->

