---
title: 深度学习
date: 2022-10-03 14:58:20
tags: 
  - 人工智能
categories: [国科大课程笔记,高级人工智能]
description: 国科大《高级人工智能》笔记3 —— 深度学习
cover: https://s3.bmp.ovh/imgs/2022/09/03/f51220518329aecb.jpg
---


# 一、玻尔兹曼机系列
## Hopfield
Hopfield网络是反馈类型，其神经元的结构功能在网络中的地位是一样的。其学习是基于灌输式学习，即网络的权值不是通过训练出来的，而是按照一定规则计算出来的，**将求解的问题转换成优化问题的能量函数，网络的稳定状态是优化问题的解，其权值一旦确定就不再改变了**。

简单的来讲就是，Hopfield网络的主要功能是联想记忆。既然如此，首先应该让网络实现“记忆”，我们需要一些数据，然后训练网络，训完练完成之后，可以得到一组可用的权值信息，形成网络的“记忆”功能。当输入数据不完整时，根据训练得到的权重去运算，得到一个稳定的输出状态，这就是联想功能。

【BM与Hopfield有一定共性，看一下有助理解。】
{% asset_img p1.png p1 %}
## BM（玻尔兹曼机）

离散Hopfield神经网络+模拟退火+隐单元=Boltzman机

Boltzmann机结合多层前馈神经网络和离散Hopfield网络在网络结构、学习算法和动态运行机制方面的优点。它是建立在离散Hopfield网基础上的，具有学习能力，它在神经元状态变化中引入了统计概率，网络的平衡状态服从Boltzmann分布，能够通过一个模拟退火过程寻求最优解。不过，其训练时间比BP网络要长。

{% asset_img p3.png p3 %}

原理：
1.上文提到Hopfield神经元的结构功能在网络中的地位是一样的,BM中一部分神经元与外部相连接，可以起到网络的输入输出作用，或者严格的说可以受到外部条件的约束，另一部分神经元不与外部相连，因而属于隐单元（相对于外部）。
2.每个神经元只有1/0两个状态：状态为1代表神经元处于激活(连接)状态，0表示非激活（断开）状态。

## RBM（受限玻尔兹曼机）
RBM是BM的一个变体，层间全连接，层内无连接，网络中的神经元是随机神经元。限定模型必须为二分图，学习的目标是极大似然。
{% asset_img p2.png p2 %}

## DBN（深度置信网络）
DBN模型由若干个RBM堆叠而成，通过非监督的预学习和监督微调训练参数。
训练时通过从底到高逐层训练这些RBM来实现：

1.底部RBM以原始输入数据训练；
2.将底部RBM抽取的特征作为顶部RBM的输入训练；
3.过程（1）和（2）可以重复训练所需的尽可能多的参数。

{% asset_img p4.png p4 %}

# CNN卷积神经网络
{% asset_img p5.png p5 %}

## 特点
**主要特点**：局部链接、参数共享、空间或时间上的子采样、（非逐层贪婪训练）。这些特性使得卷积神经网络具有一定程度上的平移、缩放和扭曲不变性。

**解释**：CNN神经元之间的连接是非全链接，同一层中神经元之间的链接权重是共享的——减少了权值的数量，降低了网络模型的复杂度。CNN的一个卷积层中，一般包含若干个特征平面，每个特征平面由一些矩阵形排列的神经元组成，同一特征平面神经元共享权值。
每个卷积层之后，通常立即会有一个非线性层（激活层），目的是给一个卷积层中刚经过线性计算操作的系统引入非线性特征。

## 各层介绍
- 池化层pooling：逐渐降低数据空间尺寸，有效减少网络中参数；
- 卷积层cov：通过卷积操作对输入图像进行降维和特征抽取；
- 全连接层：整个网络中分类器的作用；
- Relu：1.采用Sigmoid计算量较大，而Relu激活函数可以减少计算过程计算量；2.防止梯度消失；3.Relu会使一部分神经元输出为0，造成网络稀疏性，从而减小参数相互依赖关系，缓解过拟合。

## 附：卷积运算
{% asset_img p10.png p10 %}
本质上是一种加权求和运算，举例：
{% asset_img p9.png p9 %}

# RNN及其变种
## RNN 
**核心思想**：
RNN对前面信息进行记忆并且应用于当前输出计算中，隐藏层节点之间存在链接，并且隐藏层输入不仅包含输入层输出还包含上一层隐含层的输入。（可以看成是权值共享的多层前向网络）

**特点**：
- 分布式隐藏状态，可以有效存储过去大量信息；
- 以非线性动态方式更新隐藏状态。

**参数学习算法BPTT**（Backpropagation through time）：
（实现权值一致）
- 前向传递：每个时间步长各单元的输出入栈；
- 后向传递：状态出栈，计算每个时间步长误差函数的导数；
- 将每个权重的所有时刻导数加和。（所有时刻的损失相加 = 总损失）

**存在问题**：
- 梯度消失/爆炸问题；
- 长期依赖问题：距当前节点越远的节点对当前节点处理影响越小，无法建模长时间的依赖。

——>内部单元改进及变形：LSTM、GRU

{% asset_img p6.png p6 %}

## LSTM
LSTM通过门结构来除去和增加“细胞状态”的信息，实现了对重要内容的保留或对不重要内容的去除（长期记忆）。通过sigmoid层输入0到1之间概率值，描述有多少信息通过。
门结构包括：遗忘门、信息增加门（输入门）和输出门。【长期记忆】
{% asset_img p7.png p7 %}

## GRU
2门控（重置门、更新门）
GRU是LSTM的变体，相比LSTM有更简单的结构。GRU包括了重置门和更新门（输入门+遗忘门）。【计算速度快、远距离传递、更容易训练、容易创建较大的网络】

{% asset_img p8.png p8 %}
## BRNN
每个时刻都有一个正向输入的隐层和一个反向输入隐层，两个隐层分别可以表示一个词的上文信息和下文信息。即：每个词对应一个输出，同时用到了同一个词前后的信息。
缺点：需要完整的数据序列，你才能预测任意位置。
## DRNNs
深度双向RNN采用多个隐层，每个隐层向后一层传递序列信息。

# GAN对抗网络
## 核心思想
GAN的核心思想来源于博弈论的**纳什均衡**——对抗达到平衡（共同进步）：
- 生成器（生成一个数据，会被判别结果优化）：生成器的目的是尽量去学习真实的数据分布。把噪声数据z（也就是我们说的假数据）通过生成模型G，伪装成了真实数据x；
- 判别器（判断是否是生成器生成的）：判别器的目的是尽量正确判别输入数据是来自真实数据还是来自生成器。
各自提高自己的生成能力和判别能力，这个学习优化过程就是寻找二者之间的一个纳什均衡。

**基本原理**：有一个判别器和一个生成器，生成器生成图片让判别器判别，生成器提升自己让判别器无法判别，判别器则提升自己努力识别出生成器生成的图片/序列，双方对抗达到平衡。
{% asset_img p11.png p11 %}

## 学习算法
{% asset_img p12.png p12 %}
1.固定生成器G0，训练判别器，提升判别器的判别能力得到D1；【G*=arg min max V(G,D)】
2.固定判别器D1，训练生成器，提升生成器的生成能力，目标让判别器无法识别，得到G1；【D*=arg max V(G,D)】
3.再回到1中用G1训练判别器得到D2，……，依次迭代，直至两者平衡。

{% asset_img p13.png p13 %}
解释：
第一步训练D，D希望V(G、D)越大越好，所以需要加上梯度。（我希望我判断能力越来越好）。
第二步训练G，G希望V(G，D)越小越好，所以要减去梯度。（希望让判别模糊，我希望自己的欺骗能力越来越好。
整个训练过程由上面两步交替进行。
