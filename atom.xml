<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Enchanted to Meet U</title>
  
  
  <link href="https://enchantedovo.cn/atom.xml" rel="self"/>
  
  <link href="https://enchantedovo.cn/"/>
  <updated>2024-11-14T09:21:13.921Z</updated>
  <id>https://enchantedovo.cn/</id>
  
  <author>
    <name>Die Hu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读笔记 - Web Agent</title>
    <link href="https://enchantedovo.cn/2024/11/14/LLM-Learning6/"/>
    <id>https://enchantedovo.cn/2024/11/14/LLM-Learning6/</id>
    <published>2024-11-14T03:21:31.000Z</published>
    <updated>2024-11-14T09:21:13.921Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>争取每月把论文笔记更新一次在博客中💪💪💪</p></blockquote><h1 id="Agent-in-Computer-Control">Agent in Computer Control</h1><p>主要还是 Web Agent 的相关论文分享</p><!-- ## 标题二 --><!--  --><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="p1.png"  style="zoom:55%;" /> --><h2 id="SeeAct-ICML-2024">SeeAct [ICML 2024]</h2><p>GPT-4V(ision) is a Generalist Web Agent, if Grounded</p><blockquote><p>论文：<a href="https://arxiv.org/abs/2401.01614">https://arxiv.org/abs/2401.01614</a><br>代码：<a href="https://github.com/OSU-NLP-Group/SeeAct">https://github.com/OSU-NLP-Group/SeeAct</a></p></blockquote><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Learning6/seeact.png" style="zoom:80%;"><p><strong>研究内容</strong><br>论文探索了 LMM 作为一种能够理解自然语言指令并完成任何网站上任务的通用网络智能体的潜力。SEEACT 结合了<strong>多模态大模型</strong>的能力以提升对网站内容的视觉理解和交互效果，以<strong>将语言模型的输出转化为具体的网页操作（研究的内容）<strong>来工作。论文在 MIND2WEB 上进行了离线评估；另外，通过开发一个运行 Web Agent 的工具，完成了</strong>在线评测</strong>。结果表明，如果手动将 GPT-4V 的文本计划转化为网站操作，它可以在实际网站上成功完成 50% 的任务。这大大优于纯文本 LLM 或专为网络智能体微调的较小模型 (FLAN-T5 和 BLIP-2)。</p><p><Mark>创新点：能够真正生成可执行浏览器的操作，提出几种元素定位的方法：使用html元素属性；提供元素的文本表示作为选择项，让模型做选择题；使用图像标记来定位目标元素（具体使用的算法并没有写）。并且设计了一种在线测评方法。</Mark></p><p><strong>方法</strong><br>🪄<strong>问题描述：动作生成</strong><br>动作生成涉及到从自然语言指令中提取出具体的网页动作。这意味着SEEACT模型需要理解用户的指令，并将其转化为可以在网页上执行的具体操作，如点击链接、填写表单等。Prompt要点包括：1）之前的动作历史和当前网页截图；2）之前动作分析；3）截图细节分析；4）下一步行动：基于分析考虑人类的网络浏览习惯和网页设计逻辑，决定下一步操作，每次只生成一个行动。</p><p>本文研究的问题：将动作描述 <code>a =(e, o, v)</code> (元素,操作,操作所需的附加值) 转换为环境中的可执行动作 <code>a~</code>。从动作描述 <code>a~</code> 导出操作类型 <code>o</code> 和值 <code>v</code> 可以通过字符串解析很好地解决。关键的挑战是<Mark>从生成的 <code>e~</code> 中识别能真正生成浏览器事件的目标元素 <code>e</code>，作者将其称为<strong>Element Grounding（元素定位）</strong></Mark></p><p>🪄<strong>元素定位</strong><br>元素定位是指识别并定位网页上与执行任务相关的具体HTML元素。这个过程需要模型能够准确识别并与特定任务相关联的网页元素，比如按钮、输入框等，以确保正确执行动作。</p><p>文章测试了三种类型的定位方法(如图)：1）元素属性定位：模型生成目标元素的详细属性，如文本和类型，然后使用这些属性在DOM元素中进行匹配，以找到正确的元素。2）文本选择定位：为模型提供元素的文本表示作为选择项，模型从给定的多个选择中确定其意图的元素。3）图像注释定位：在每个候选元素上覆盖边界框和字母标签，模型需要生成对应于目标元素的标签。</p><p><strong>实验结果</strong><br>如果提供了准确的元素定位，模型可以在在线评估中成功完成50%的任务。在各种定位策略中，结合HTML文本和视觉信息的方法表现最佳，显著优于仅使用图像注释的策略。在线评估与离线评估之间存在显著差异。</p><h2 id="WebVoyager-ACL-2024-main">WebVoyager [ACL 2024 (main)]</h2><p>WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models</p><blockquote><p>论文：<a href="https://arxiv.org/abs/2401.13919">https://arxiv.org/abs/2401.13919</a><br>代码：<a href="https://github.com/MinorJerry/WebVoyager">https://github.com/MinorJerry/WebVoyager</a></p></blockquote><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Learning6/webvoyager.png" style="zoom:80%;"><p><strong>研究内容</strong><br>Web Agent：借助 GPT-4V 这样的多模态模型，与开放网络中的网站交互，自动化完成用户的各项指令。<br>目标：构建一个能够在没有任何人工干预的情况下，<strong>自主浏览开放网页</strong>并<strong>完成用户指令</strong>的Agent。</p><p><mark>创新点：考虑视觉，使用了SoM(Set-of-Mark Prompting)提高大模型对屏幕截图的理解能力。</mark></p><p><strong>研究动机</strong><br>现有的Agent通常用于处理复杂且冗长的HTML文本这一<strong>单一输入模态</strong>，而忽视了可以将HTML渲染为<strong>视觉网页</strong>这一要点，并且仅在<strong>简化</strong>的网络模拟器或<strong>静态</strong>网络快照中进行评估，很大程度上限制了Agent在现实世界场景中的适用性。</p><p><strong>作者的工作</strong><br>作者提出了一种新的<strong>多模态 Web Agent</strong>——WebVoyager，旨在以<strong>端到端</strong>的方式<strong>在线</strong>处理网络任务，即在没有人工介入的情况下从开始到结束自主管理整个过程。</p><p>构建了一个<strong>新的评估数据集</strong>，包含来自15个常用网站的300个网络任务；并提出了一种自动评估的方法，在在线导航过程中保存屏幕截图，使用GPT-4V自动评估轨迹和最终结果，同时进行人工评估以验证结果。验证了GPT-4V评估器与人类评估的一致性，从而减轻人工评估员的负担，显著降低实验的成本。</p><p>WebVoyager 的任务成功率为55.7%， 显著优于GPT-4（All Tools）的32.7%和纯文本设置的39.0%。</p><p><strong>方法</strong></p><p>WebVoyager 整体工作流程如上所示。具体来说：根据指令，WebVoyager 首先实例化一个 Web 浏览器，然后使用来自 Web 的视觉（即屏幕截图）和文本（即 HTML 元素）信号执行操作。Agent根据每一步的输入生成一个操作，然后在浏览器环境中执行。该过程将持续进行，直到Agent决定停止。 WebVoyager的详细设计，包括环境、交互周期、观察空间和动作空间如下。</p><p><strong>1. 浏览环境的构建</strong><br>工作流程始于构建一个自动化的网页浏览环境。使用<strong>Selenium工具</strong>，Agent可以模拟用户的浏览行为，访问真实的网页。允许Agent探索开放的网络环境，面对实时更新、浮动广告等挑战。</p><p><strong>2. 交互的公式化表示和决策过程</strong><br>决策过程是一个交互周期，Agent在每一步接收<strong>输入（包括截图和辅助文本）</strong>，然后产生一个<strong>动作</strong>，该动作在浏览器环境中<strong>执行</strong>。如果动作执行期间出现异常，还会将错误信息纳入提示中，要求模型重新生成响应。这个过程持续进行，直到模型生成终止动作或达到最大步数为止。</p><ul><li>使用 ReAct Prompting范式，提示 Agent 在生成 Action 代码之前首先生成一个 Reason 的推理过程。</li><li>为了避免长时间的网页导航导致的混淆，执行<strong>上下文裁剪</strong>，只保留最近的3个截图，并保留所有思考和动作的历史记录。</li></ul><p><strong>3.观察空间</strong><br>在每一步中，代理都会接收<strong>当前屏幕截图、辅助文本和历史记录</strong>作为输入。（屏幕截图能够避免处理 HTML DOM 树或可访问性树来描绘网页整体结构的长上下文负担）</p><p>具体来说，受 <strong>Set-of-Mark Prompting</strong> (Yang et al., 2023a) 的启发，作者在网站上<strong>叠加交互元素的边界框</strong>，以更好地指导 Agent 的动作预测。与利用目标检测模块不同，作者利用 <strong>GPT-4V-ACT5</strong>（一种 Javascript 工具）根据 Web 元素类型提取交互元素，然后在元素的相应区域上覆盖带有数字标签的边界框。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Learning6/webvoyager-1.png" style="zoom:80%;"><p>网页的性质使其能够使用此工具<strong>精确定位和概述每个交互元素</strong>。分配给每个元素的<strong>数字标签</strong>对于模型识别需要交互的元素也至关重要，从而促进准确的动作确定。凭经验选择黑色作为标签的边框和背景，以提高清晰度。作者还为 Agent 提供<strong>辅助文本</strong>作为输入，包括嵌入交互元素中的文本内容、元素的类型，以及 aria-label 属性中可能的一些注释文本。为了简化观察，作者禁用了多个选项卡。<br>通过在网页上的交互元素上叠加边框和数字标签，Agent能够更准确地确定需要交互的元素，并执行相应的动作。</p><blockquote><p><strong>Set-of-Mark Prompting</strong>：来自微软、香港科技大学等机构的研究者提出的一种新的视觉 prompt 方法 Set-of-Mark（SoM），来<strong>解决 GPT-4V 在细粒度视觉任务上的问题</strong>。SoM的机制非常简单，简而言之，通过使用语义分割将图像划分为多个区域并标记每个区域，“我们可以让 GPT-4V 更容易地识别每个对象的位置关系”，仅此而已。</p></blockquote><p><strong>这项研究的重点是，只要对输入到 GPT-4V 的图像稍作修改，就能提高 GPT-4V 的图像识别能力。</strong></p><p><strong>4.行动空间</strong><br>形式：<code>操作+数字标签</code><br>行动空间<strong>定义</strong>了Agent可以执行的动作（有限的），如点击、输入、滚动、等待、返回、跳转到搜索引擎等，这都是最常用的鼠标和键盘操作，足以让Agent浏览各种网页并找到任务所需的内容。通过在屏幕截图中使用数字标签，使Agent能够以简洁的行为格式进行响应。</p><p><strong>实验设计</strong></p><p>1.选取的网站和任务多样性<br>本文选择了15个流行且具有代表性的网站，涵盖了日常生活的不同方面:包括Allrecipes、Amazon、Apple、ArXiv、BBC News、Booking、Cambridge Dictionary、Coursera、ESPN、GitHub、Google Flights、Google Map、Google Search、Huggingface和Wolfram Alpha。此外，Google Search作为一个通用网站，可以作为任何网站的起点。</p><p>2.数据集</p><ul><li>自建数据集：数据集的构建采用了自我指导（self-instruct）方法和人工验证相结合的方式。首先，从Mind2Web中<strong>手动采样</strong>并<strong>重写</strong>了一些任务，然后使用GPT-4 Turbo生成约100个新任务，并对每个生成的任务进行<strong>人工验证</strong>和必要的<strong>重写</strong>。最终收集了每个网站20个任务，共计300个任务。</li><li>GAIA 数据集：从中提取了90个网页浏览任务（Level 1 和 Level 2），并使用 SeeAct 中的50项任务。指示Agent从谷歌搜索开始搜索答案。采用主要的评估指标为任务成功率，不考虑逐步执行是否最优。每个任务的最大交互次数设置为15次。</li></ul><blockquote><p>SeeAct Agent 依赖于微调的交叉编码器模型来选择交互的候选元素。相比之下，WebVoyager 不需要任何额外的模块。</p></blockquote><p>3.评估方法</p><ul><li>人类评估：主要的评估指标，特别是针对那些具有开放式答案的任务。</li><li>自动评估：使用GPT-4V作为自动评估器。</li></ul><p>4.基线模型</p><ul><li>GPT-4 (All Tools)，将视觉、网页浏览、代码分析和各种插件集成在一个模型中。</li><li>考虑了一个仅接收网站可访问性树作为输入并进行行为预测的文本基线。</li></ul>]]></content>
    
    
    <summary type="html">论文阅读笔记 —— LLM Agent for Computer Control 1114</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="论文阅读" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="Agent" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/Agent/"/>
    
    
    <category term="生成式人工智能" scheme="https://enchantedovo.cn/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="Agent" scheme="https://enchantedovo.cn/tags/Agent/"/>
    
  </entry>
  
  <entry>
    <title>LLM for Security 论文记录 1114</title>
    <link href="https://enchantedovo.cn/2024/11/14/LLM-Security4/"/>
    <id>https://enchantedovo.cn/2024/11/14/LLM-Security4/</id>
    <published>2024-11-14T02:57:54.000Z</published>
    <updated>2024-11-20T09:51:00.646Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>更新一下论文阅读记录，一直写在了本地的编辑器上，现在更新到博客中（持续更新ing，打算以月为单位更新博客了）</p></blockquote><h1 id="Agent-in-Security">Agent in Security</h1><!--  --><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="p1.png"  style="zoom:55%;" /> --><h2 id="PentestGPT（USENIX）">PentestGPT（USENIX）</h2><p>PentestGPT: An LLM-empowered Automatic Penetration Testing Tool</p><blockquote><p>论文：<a href="https://arxiv.org/abs/2308.06782">https://arxiv.org/abs/2308.06782</a><br>代码：<a href="https://github.com/binarybrain-009/AUTOATTACKER">https://github.com/binarybrain-009/AUTOATTACKER</a> （核心代码放的是一个注释，差评）</p></blockquote><p><strong>研究内容</strong><br>研究<strong>大模型的自动化渗透测试能力</strong>（比较早的一个工作）。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Security4/pentestgpt.png" style="zoom:80%;"><p><strong>研究动机/挑战</strong><br>针对渗透测试领域引入了独特的挑战，包括：<br>1）现今的渗透测试较依赖人工操作和专业知识，限制了高效安全评估的进化需求；<br>2）现有的渗透测试基准测试不够全面，无法全面评估渗透测试进展。</p><p>动机: 鉴于渗透测试当前限制和LLMs的能力，作者希望探究LLMs在渗透测试任务中的应用价值，提高自动化渗透测试的效率和有效性。</p><p><strong>贡献</strong><br>1）针对LLM+渗透测试进行了一些先验研究：</p><ul><li>RQ1(Capability): 法学硕士可以在多大程度上执行渗透测试任务？<ul><li>Finding 1: LLM熟练执行端到端渗透测试任务，但难以应对更困难的目标；</li><li>Finding 2: LLMs可以有效地使用渗透测试工具，识别常见漏洞，并解释源代码以识别漏洞。</li></ul></li><li>RQ2(Comparative Analysis): 人类渗透测试员和法学硕士的问题解决策略有何不同？<ul><li>Finding 3: LLMs很难维持长期记忆，而这对有效链接漏洞和制定利用策略至关重要；</li><li>Finding 4: LLMs倾向最近的任务和深度优先搜索，这会导致过度关注某一任务并忘记之前的发现；</li><li>Finding 5: LLMs可能会产生不准确的操作或命令。</li></ul></li></ul><p>2）设计了交互式的PentestGPT。一方面优化LLMs在渗透测试中的使用（<mark>prompt设计，渗透计划树记录规划</mark>），另一方面利用LLMs的优势提高自动渗透测试的效率和有效性（给人提供指导）。</p><p>3）构建了一个渗透测试任务的基准测试（HackTheBox中的夺旗任务）</p><p><strong>方法</strong><br>首先，作者设计了一个全面的全自动渗透测试工具MALISM（<strong>PentestGPT模块是该框架中的一部分</strong>）。这个网络安全认知引擎（没有开源代码，可能只是设计）主要包括以下三个模块：</p><ul><li>EXPLOITFLOW：通过捕获每个离散动作后的系统状态来生成安全开发路线(利用流)；</li><li><strong>PENTESTGPT：利用LLMs为每个给定的离散状态启发式生成测试指导；</strong></li><li>PENTESTPERF：一个综合渗透测试基准，用于评估渗透测试器和自动化工具在广泛的测试目标上的性能。</li></ul><p>针对先验研究发现的：<strong>LLMs具有一定的渗透测试能力，但难以保持长期记忆和解决复杂问题</strong>，作者设计了交互式的PentestGPT。一方面优化LLMs在渗透测试中的使用（设计prompt，渗透计划树等），另一方面利用LLMs的优势提高自动渗透测试的效率和有效性（给人指导）。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Security4/pentestgpt1.png" style="zoom:80%;"><p>在推理模块中对于渗透流程的规划，通过渗透计划树：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Security4/pentestgpt2.png" style="zoom:80%;"><p><strong>思考</strong><br><mark>思考：提供了LLM应用到渗透测试的一个思路，比如<strong>渗透计划树的方式记录规划</strong>（后面的AutoAttack也是在PentestGPT基础上改的），但是缺点也很明显，PentestGPT本质上还是一个针对渗透测试提供建议的问答模型，其中人工干预较多，不是一个真正意义上的Agent，所以说后面的一些工作会弥补掉这个Gap，能够直接执行一些渗透命令，并反馈继续</mark></p><h2 id="AUTOATTACKER">AUTOATTACKER</h2><p>AUTOATTACKER: A Large Language Model Guided System to Implement Automatic Cyber-attacks</p><blockquote><p>论文：<a href="https://arxiv.org/abs/2403.01038">https://arxiv.org/abs/2403.01038</a><br>代码：<a href="https://github.com/binarybrain-009/AUTOATTACKER">https://github.com/binarybrain-009/AUTOATTACKER</a> （核心代码放的是一个注释，差评）</p></blockquote><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/11/14/LLM-Security4/autoattack.png" style="zoom:80%;"><p><strong>研究内容</strong><br>利用基于 LLM 的系统来模拟在各种攻击技术和环境下通常为人为操作或“手动键盘”攻击的攻击后阶段。</p><p><strong>研究意义</strong></p><ul><li>首先，基于 LLM 的自动化漏洞后利用框架可以<strong>帮助分析师快速测试</strong>并持续改进其组织的网络安全态势，以抵御以前未曾见过的攻击。</li><li>其次，基于LLM的渗透测试系统可以通过有限数量的分析师来<strong>扩展红队的有效性</strong>。</li><li>最后，这项研究可以帮助防御系统和团队在实际使用之前学会<strong>先发制人地检测新的攻击行为</strong>。</li></ul><p><strong>研究动机/挑战</strong><br>基于问题引入了独特的挑战，包括：<br>1）<strong>复杂的攻击任务链</strong>：高级攻击可能需要许多子任务，甚至一个失败的子任务会破坏整个链；<br>2）<strong>动作空间的高密度可变性</strong>：bash或Metasploit中的命令具有许多参数，其中一些参数与系统信息或文件夹路径密切相关，其中一个拼写错误可能会破坏攻击命令。</p><p><strong>贡献</strong><br>1）提出了一种<strong>模块化</strong> Agent 设计，以在不同点利用LLM的不同功能，例如规划、总结和代码生成，即使在生成单个攻击命令时也是如此。通过这种设计，我们可以更好地利用 LLM 来得出精确的答案。<br>2）借用**检索增强生成（RAG）**的想法，在生成下一个动作之前用先前攻击动作（称为经验）的知识库来增强LLM，因此成功攻击的机会会增加，因为它们的组合子任务可以重复使用。</p><p><strong>方法</strong><br>设计了 4 个模块，即<strong>总结器</strong>、<strong>规划器</strong>、<strong>导航器</strong>和<strong>经验管理器</strong>，以与 LLM 迭代交互。还精心设计了每个模块的<strong>提示</strong>模板，因此LLM的回答具有高度可控性。</p><p><strong>思考</strong><br><mark>创新点：完成了渗透的闭环，与pentestGpt仅仅给人提供指导相比，AutoAttack能够推理后执行指令，而无需人的参与。其中为了更好地进行计划的决策，引入了基于RAG的经验管理。缺点就是核心代码不全。</mark></p>]]></content>
    
    
    <summary type="html">LLM for Security 论文记录 1114</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="论文阅读" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="大模型&amp;安全" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%AE%89%E5%85%A8/"/>
    
    <category term="Agent" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/Agent/"/>
    
    
    <category term="生成式人工智能" scheme="https://enchantedovo.cn/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="网络安全" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
    <category term="大模型安全" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>风云卫安全大模型的一些理解</title>
    <link href="https://enchantedovo.cn/2024/11/14/LLM-Security3/"/>
    <id>https://enchantedovo.cn/2024/11/14/LLM-Security3/</id>
    <published>2024-11-14T02:43:52.000Z</published>
    <updated>2024-11-14T03:03:58.453Z</updated>
    
    <content type="html"><![CDATA[<!-- # 标题一## 标题二 --><!--  --><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="p1.png"  style="zoom:55%;" /> --><blockquote><p>2023年TechWorld绿盟科技智慧安全大会思考总结</p></blockquote><h1 id="背景">背景</h1><p>绿盟科技应用AI实现安全行业工业范式重塑，通过大模型解决<strong>三</strong>大问题：<strong>实战态势指挥调度、红蓝对抗辅助决策以及安全运营效能提升</strong>。</p><h1 id="基础：高质量语料来源">基础：高质量语料来源</h1><p>绿盟科技经过多年的积累和转化，从过去分散在各处的单点数据逐步构建情报体系，形成知识图谱，最终积累到现在的高质量语料数据体系，覆盖攻、防、情报、知识等多个方面，是目前看到市场上覆盖最全面的<strong>数据体系</strong>。绿盟科技的语料是与绿盟科技的安全工作、组织架构融合在一起的，可以在各类实验室研究、攻防团队积累、安全运营过程中源源不断地生成。</p><h1 id="场景化应用">场景化应用</h1><h2 id="界面自动生成">界面自动生成</h2><h3 id="简介">简介</h3><p><strong>效率提升：周-月级 -&gt; 分钟级</strong></p><p>安全运营的一个重点工作就是<strong>事件监控</strong>，不同场景、不同阶段、不同安全成熟度的客户对于安全运营的需求存在差异，带来大量安全管理平台的界面定制。绿盟风云卫大模型接入，只需完成对应接口开发和组件开发，<strong>由大模型实现页面调度和编排</strong>，几分钟即可生成新界面。</p><h3 id="理解">理解</h3><p>大模型在这一过程中的作用包括：</p><ul><li>自然语言理解： 大模型可以理解用户的自然语言输入，识别他们的需求。例如，用户可以简单地描述想要的界面功能，模型能够解析这些需求并提取关键信息。</li><li>模板匹配和生成： 大模型可以从大量的现有界面模板中学习，识别哪些模板适合特定需求，并在需要时生成新的模板。通过深度学习技术，模型能够自动生成与需求相匹配的用户界面。</li><li>智能推荐： 基于用户的历史行为和偏好，大模型可以智能推荐界面布局和组件。例如，如果用户经常使用某种功能，模型可以自动将其放置在更显眼的位置。</li><li>实时调整与反馈： 大模型可以根据用户的实时反馈进行动态调整。当用户与界面交互时，模型可以捕捉到用户的使用模式，及时更新和优化界面。</li></ul><h3 id="示例">示例</h3><p>假设一家公司需要创建一个监控仪表板，使用大模型可以这样进行：</p><ul><li>用户输入：安全分析师输入“我需要一个显示网络流量和警报的仪表板”。</li><li>模型处理：大模型解析该需求，选择适合的模板，并自动生成相关组件（如流量图表、警报列表）。</li><li>布局生成：模型将这些组件整合到一个界面中，并根据重要性调整其位置，例如将警报放在显眼位置。</li><li>优化反馈：用户使用仪表板后，如果发现某个组件不够突出，模型可以根据这个反馈进行调整，提升用户体验。</li></ul><h2 id="检测规则自动生成">检测规则自动生成</h2><h3 id="简介-2">简介</h3><p><strong>效率提升：1天 -&gt; 分钟级</strong></p><p>漏洞被披露后，安全厂商生产漏洞检测和防护规则包、客户下载升级并应用周期较长。绿盟风云卫大模型可以<strong>提交对应POC或恶意样本，自动编排检测规则，自动下发</strong>，几分钟完成漏洞应急工作，大大降低风险。</p><h3 id="理解-2">理解</h3><p>在网络安全领域，<strong>漏洞利用代码（Proof of Concept, POC）</strong> 是指一个小型的程序或代码片段，展示如何利用特定漏洞进行攻击。<strong>恶意样本</strong> 则是指攻击者实际使用的恶意软件或代码，用于破坏系统或窃取数据。</p><p>传统情况下，当一个新的漏洞被公开（例如一个影响软件系统的安全漏洞），安全厂商需要开发相应的检测和防护规则，来帮助企业应对该漏洞。这包括：</p><ol><li><strong>分析漏洞</strong>：专家需要详细研究漏洞是如何工作的。</li><li><strong>编写检测规则</strong>：基于漏洞的行为模式，安全专家编写检测规则，这些规则告诉安全设备如何识别出受到攻击的系统。</li><li><strong>发布规则更新</strong>：厂商将这些规则打包成更新包，客户下载并应用到他们的安全系统中。</li></ol><p>这一过程通常需要花费一天甚至更长的时间，漏洞在这个周期中可能带来风险，因为没有足够快的防护手段。</p><p><strong>风云卫大模型的作用</strong>：</p><p>使用大模型后，整个流程可以被极大简化，效率显著提升。以下是详细的步骤，结合一个例子说明：</p><h3 id="示例：零日漏洞的应急处理">示例：零日漏洞的应急处理</h3><ol><li><strong>漏洞曝光</strong>： 假设一个新的零日漏洞（从未被公开过的漏洞）被披露了，它影响某个常用的企业软件。攻击者通过一个特定的输入序列触发了软件的缓冲区溢出，从而能够远程执行恶意代码。</li><li><strong>提交漏洞利用代码（POC）或恶意样本</strong>： 安全研究人员或安全厂商从漏洞公开的公告中获取了漏洞利用代码（POC），这是一个展示如何利用该漏洞的代码片段。该POC被提交给绿盟风云卫大模型。</li><li><strong>自动生成检测规则</strong>： 大模型接收到POC后，利用其内置的知识和分析能力，自动对漏洞行为进行解析。模型会识别出这个POC的关键特征，例如：哪些网络流量或系统行为与漏洞利用有关（比如特定的数据包格式或特定的系统调用）。接着，它会根据这些特征自动生成对应的<strong>检测规则</strong>，这些规则能用于检测和防御与该漏洞相关的攻击行为。</li><li><strong>发布和应用检测规则</strong>： 在生成规则后，模型会自动下发这些规则到客户的安全设备上（例如入侵检测系统、入侵防御系统等）。整个过程只需几分钟的时间，客户系统即可开始防护。</li></ol><blockquote><p>缓冲区溢出（Buffer Overflow）是一种常见的安全漏洞，它发生在程序试图将数据写入一个固定大小的内存缓冲区时，如果输入的数据超出了缓冲区的大小，程序就会覆盖相邻的内存区域。</p><p>具体过程：</p><ol><li><strong>输入数据</strong>： 攻击者构造一个特定的输入序列，这个序列包含了超出程序预期的数据长度。例如，假设一个程序为用户输入分配了100个字节的缓冲区，但攻击者输入了200个字节的数据。</li><li><strong>触发溢出</strong>： 当程序处理这个输入时，由于没有适当的边界检查，它会将超出部分的数据写入内存的其他区域，这就造成了缓冲区溢出。</li><li><strong>覆盖内存</strong>： 溢出的数据可能会覆盖重要的控制信息，如返回地址或函数指针。返回地址指向程序执行后续代码的地址，攻击者可以精确控制这一地址。</li><li><strong>远程执行恶意代码</strong>： 攻击者可以在输入数据中插入恶意代码（通常称为“payload”），并修改返回地址，使程序在执行完当前函数后跳转到恶意代码的地址。这意味着一旦程序运行到那个点，就会执行攻击者注入的代码，从而实现远程执行恶意代码的目的。</li></ol><p>通过缓冲区溢出，攻击者可以利用软件中的漏洞，改变程序的执行流，实现对系统的控制和恶意操作。这种攻击方式通常用于获取未经授权的访问、数据窃取或其他恶意行为。</p></blockquote><h2 id="分析研判处置自动化">分析研判处置自动化</h2><h3 id="简介-3">简介</h3><p><strong>效率提升：6小时 -&gt; 分钟级</strong></p><p>高级威胁、复杂事件分析研判响应需要高级专家数天时间。绿盟风云卫大模型能够<strong>生成式引导</strong>运营人员去做分析和响应，降低安全运营门槛。另一方面，<strong>大模型能够将分析此类型事件的固定模式</strong>给记录下来，实现后续自动化分析和响应全过程。同时可以自动化生成SOAR响应处置脚本，自动应用到各类检测和防护设备中。</p><h3 id="理解-3">理解</h3><p>具体过程：</p><ul><li>事件监测： 安全系统实时监测网络流量、用户行为、系统日志等，识别潜在的安全事件。这些事件可能包括入侵尝试、恶意软件活动或异常流量。</li><li>事件分类和优先级排序： 一旦识别出安全事件，模型会根据预设的规则和学习到的模式，对事件进行分类和优先级排序。例如，将高风险的事件（如针对关键资产的攻击）标记为高优先级，而低风险的事件（如一般的登录失败）标记为低优先级。</li><li>引导分析： 对于高优先级事件，模型可以生成分析指导，提示操作人员需要关注的关键指标和可能的攻击路径。例如，如果检测到异常流量，模型可能建议检查特定用户的活动记录和流量来源。</li><li>自动化响应： 在分析过程中，模型可以自动生成应对策略。例如，当识别到某种类型的攻击时，模型可以自动创建和部署安全编排响应（SOAR）脚本，这些脚本指示安全设备如何响应。例如，立即封锁可疑IP地址、增加特定用户的安全监控等。</li><li>实时反馈与优化： 在事件处置过程中，模型能够实时收集操作人员的反馈，并根据这些反馈优化后续的分析和响应策略。这样，随着时间的推移，模型的准确性和效率会不断提高。</li></ul><blockquote><p>安全编排响应（SOAR）脚本是一种自动化脚本，用于在安全事件发生时协调和执行响应措施。这些脚本将不同的安全工具和流程整合在一起，以实现快速、有效的响应。<br>SOAR脚本的主要功能包括：</p><ol><li><strong>自动化任务</strong>： SOAR脚本可以自动执行各种安全任务，如封锁可疑IP地址、隔离受感染的设备、生成安全事件报告等，减少人工干预。</li><li><strong>事件响应流程</strong>： 脚本定义了一系列预设的响应步骤，当特定事件发生时，系统会按照这些步骤自动执行。例如，如果检测到某种类型的攻击，SOAR脚本会自动触发相应的响应措施。</li><li><strong>集成不同工具</strong>： SOAR脚本能够与多种安全工具（如防火墙、入侵检测系统、SIEM等）进行集成，确保各个系统之间的数据和信息能够流畅传递。</li><li><strong>信息共享与报告</strong>： 在响应过程中，SOAR脚本可以自动收集数据并生成报告，便于安全团队后续分析和审计。</li></ol><p><strong>示例：</strong><br>假设某公司检测到异常登录尝试，SOAR脚本的执行过程可能如下：</p><ol><li><strong>触发事件</strong>： 检测到来自特定IP的多次失败登录尝试。</li><li><strong>自动化响应</strong>： SOAR脚本自动执行以下步骤：<ul><li>封锁该IP地址。</li><li>发送通知给安全团队。</li><li>在SIEM系统中记录事件。</li><li>进行用户活动的进一步审查。</li></ul></li><li><strong>报告生成</strong>： 在事件处理结束后，脚本自动生成一份事件响应报告，总结事件详情和采取的措施。</li></ol><p>SOAR脚本通过自动化响应流程，提高了安全事件的处理速度和效率，减轻了安全团队的负担，使他们能更专注于复杂的安全挑战。</p></blockquote><h2 id="渗透-评估自动化">渗透/评估自动化</h2><h3 id="简介-4">简介</h3><p><strong>效率提升：1-3天 -&gt; 分钟级</strong></p><p>安全服务人员渗透测试工作需要一天甚至几天时间，绿盟风云卫大模型可以辅助服务人员进行渗透测试，自动化生成测试报告，提升安全服务效率。</p><h3 id="理解-4">理解</h3><p>假设一个公司希望对其网络安全进行全面评估。安全服务人员通常会按照以下步骤进行渗透测试：</p><ol><li><strong>信息收集</strong>：手动查找目标公司的公开信息，包括域名、IP地址、员工信息等。</li><li><strong>漏洞扫描</strong>：使用工具扫描目标系统，识别潜在的漏洞。</li><li><strong>攻击模拟</strong>：尝试利用识别出的漏洞，进行攻击以评估系统的脆弱性。</li><li><strong>报告生成</strong>：整理测试结果，编写详细的测试报告，提供改进建议。</li></ol><p>在这个过程中，绿盟风云卫大模型可以发挥以下作用：</p><ul><li><strong>自动化信息收集</strong>：模型可以自动化从网络上收集目标公司的相关信息，节省人力和时间。</li><li><strong>智能漏洞识别</strong>：通过大数据分析，模型能够迅速识别常见漏洞，并提供相关的攻击向量。</li><li><strong>模拟攻击</strong>：模型可以根据已有数据模拟攻击，验证系统的防御能力。</li><li><strong>自动生成报告</strong>：测试完成后，模型能够自动整理测试结果，生成结构化的报告，包括发现的漏洞、攻击路径及改进建议，减少服务人员的文书工作。</li></ul><h2 id="红队工作支撑">红队工作支撑</h2><h3 id="简介-5">简介</h3><p><strong>效率提升：高级专家3-5天 -&gt; 小时级</strong></p><p>红队工具、红队样本、红队路径等生成的工作，红队需要大量时间做数据准备，工具生成等相关工作，通过绿盟风云卫大模型的辅助，快速生成相应红队工具。</p><h3 id="理解-5">理解</h3><p>在网络安全中，红队负责模拟攻击，以测试系统和网络的安全性。他们需要使用各种工具和技术来执行攻击，发现安全漏洞。这个过程通常需要大量的准备工作，包括生成攻击工具、创建样本和确定攻击路径。<br><strong>具体过程</strong>：</p><ol><li><strong>红队工具的定义</strong>： 红队工具是用于进行渗透测试和模拟攻击的软件或脚本，例如密码破解工具、网络扫描器、漏洞利用框架等。</li><li><strong>红队样本的创建</strong>： 红队样本是指攻击者可能使用的恶意代码或负载，用于测试系统的防御能力。这些样本可以包括特定的恶意软件或攻击脚本。</li><li><strong>红队路径的确定</strong>： 红队路径是攻击者在实施攻击时可能采取的步骤或策略，包括如何突破防御、获取访问权限和提升权限的具体方法。</li></ol><p><strong>使用大模型的优势</strong>：<br>通过风云卫大模型，红队可以快速生成所需的工具、样本和路径，具体步骤如下：</p><ol><li><strong>快速生成工具</strong>： 假设红队需要一个新的网络扫描工具。团队可以提供需求，模型会根据最新的安全漏洞和攻击技术自动生成一个符合需求的扫描工具。这可能包括特定的功能，如发现开放端口、识别运行的服务等。</li><li><strong>生成攻击样本</strong>： 红队希望模拟一种新型的网络攻击。通过输入攻击类型（如DDoS攻击或钓鱼攻击），模型可以自动生成相关的恶意样本，这些样本能够模仿真实攻击者的行为。</li><li><strong>确定攻击路径</strong>： 红队想要评估一个新的目标系统的安全性。模型可以分析目标系统的结构，基于现有的安全研究，自动生成可能的攻击路径，包括初始访问、横向移动和权限提升的步骤。</li></ol><h3 id="示例-2">示例</h3><p>假设一家金融机构邀请红队进行渗透测试，以下是如何通过绿盟风云卫大模型进行准备的示例：</p><ol><li><strong>需求输入</strong>： 红队成员输入需要模拟的攻击类型和目标系统的基本信息。</li><li><strong>工具生成</strong>： 模型根据输入生成一个定制的网络扫描工具，具备识别金融系统常见漏洞的能力。</li><li><strong>样本生成</strong>： 模型自动创建一个钓鱼邮件样本，模仿该金融机构的品牌，增加成功率。</li><li><strong>攻击路径规划</strong>： 模型分析金融系统的架构，生成一个详细的攻击路径，包括从钓鱼邮件到获取用户凭证，再到横向移动到管理系统的步骤。</li></ol><p>通过绿盟风云卫大模型，红队能够快速、有效地生成所需的工具、样本和攻击路径，显著缩短准备时间，提高渗透测试的效率和准确性。这种自动化的支持使红队能够专注于实际的测试和评估，而不是耗费时间在准备工作上。</p>]]></content>
    
    
    <summary type="html">安全大模型产品-绿盟风云卫理解</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="大模型&amp;安全" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%AE%89%E5%85%A8/"/>
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="网络安全" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
    <category term="安全大模型" scheme="https://enchantedovo.cn/tags/%E5%AE%89%E5%85%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>TrafficLLM 复现 - Evaluation &amp; Server</title>
    <link href="https://enchantedovo.cn/2024/10/17/LLM-Learning5/"/>
    <id>https://enchantedovo.cn/2024/10/17/LLM-Learning5/</id>
    <published>2024-10-17T06:37:32.000Z</published>
    <updated>2024-10-17T07:25:09.440Z</updated>
    
    <content type="html"><![CDATA[<!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/p1.png" class="" title="p1"> --><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="p1.png"  style="zoom:55%;" /> --><p>复现 TrafficLLM 中 Evaluation + Web Server（不涉及微调、训练）</p><h1 id="论文简介">论文简介</h1><p>TrafficLLM 是中关村实验室崔天宇博士和清华大学徐恪教授、李琦教授团队共同发布的流量大模型工作。<br>TrafficLLM 是一种用于网络流量分析的通用LLM自适应框架。TrafficLLM介绍了一种通用解决方案，以回答如何通过增强对异构流量数据、跨不同任务的多模式学习以及网络流量的新环境的泛化性来实现鲁棒的<strong>流量表示</strong>。</p><blockquote><p>论文：<a href="https://arxiv.org/abs/2408.10390">https://arxiv.org/abs/2408.10390</a><br>代码：<a href="https://github.com/ZGC-LLM-Safety/TrafficLLM/">https://github.com/ZGC-LLM-Safety/TrafficLLM/</a></p></blockquote><h2 id="研究背景">研究背景</h2><p>基于<strong>机器学习</strong>的流量分析方法具有以下<strong>局限性</strong>，导致其泛化能力较差：</p><ul><li><strong>跨各种任务的泛化。</strong> 在流量分析任务的每个子领域，现有的方法通常使用手工制作的特征和监督标签进行学习，为特定任务开发复杂的机器学习模型，模型很难在不同的任务之间共享。要涵盖所有任务的模型开发成本将非常庞大。</li><li><strong>对未知数据的泛化。</strong> 当面对概念漂移和0-day攻击等未知的数据场景时，ML模型的泛化性能往往较差。</li></ul><p>因此，开发一个更通用的模型对于提高不同任务和数据分布的泛化能力具有重要意义。最近，大型语言模型（LLMs）在许多复杂任务中表现出了出色的性能。得益于其模式挖掘、对未知数据的泛化以及跨不同任务的可复用性，LLM可以在各种下游任务中释放出非凡的能力，这激发了开发用于网络流量分析的专业大模型的可能性。</p><p>例如，LLM的模式挖掘和推理能力可用于学习流量数据中IP属性、标志和数据报长度背后的鲁棒表示。此外，泛化能力使LLM能够适应不同的网络环境和攻击场景。</p><p>因此，LLM可以作为一种更强大的ML模型，提供具有强大泛化能力的鲁棒流量表示。</p><h2 id="LLM应用到流量分析的限制性">LLM应用到流量分析的限制性</h2><p>然而，流量领域的特性给实现LLM在流量分析任务上的泛化在三个方面留下了限制问题。</p><ul><li><strong>限制1: 对流量数据的异构输入的泛化。</strong> 流量数据由数据包中的结构化元数据（如IP和端口）和流中的统计特征（如流中的数据包长度和时间间隔）组成。然而，<strong>大多数LLM被认为是处理纯文本的专用模型，这与流量数据存在巨大差距。</strong> 在输入LLM之前，数据使用标准令牌生成器将文本转换为语言token。这些令牌生成器通常在大规模文本语料库上使用WordPiece和字节对编码（BPE）等算法进行训练。他们很少见过这些异构的流量数据。因此，LLM可能无法直接将流量数据转换为文本格式，并使用默认的令牌化方法加载它们。</li><li><strong>限制2: 对跨不同任务的多模式学习的泛化。</strong> 网络流量分析涵盖了广泛的特定任务，例如在不同场景下检测和模拟攻击流量（例如MTD任务）。它涉及指令中不同任务特定的知识，以提示LLM进行不同的工作。此外，这些下游任务通常指向不同的网络环境，这涉及多种类型的流量特征模式（例如，加密应用程序分类（EAC）中的数据包长度序列和web攻击检测（WAD）中的HTTP请求头）。<strong>在进行多模式学习时，指令和流量模式的复杂性很容易使LLM感到困惑</strong>。</li><li><strong>限制3: 模型更新扩展到新环境的泛化。</strong> LLM的适应成本相当高，因为它需要用大量数据集训练大规模参数。然而，许多流量分析任务通常需要更新模型以应对动态场景，这是由应用程序版本更新（例如概念漂移）和攻击方法变更（例如APT攻击）引起的。LLM的高适应成本阻碍了模型能力在新场景下的更新。</li></ul><h2 id="TrafficLLM方法">TrafficLLM方法</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/image.png" style="zoom:90%;">TrafficLLM是使用自然语言和流量数据的流量大模型微调框架。具体来说，TrafficLLM中主要提出了以下技术，以提高大型语言模型在网络流量分析中的实用性：<h3 id="流量领域令牌生成">流量领域令牌生成</h3><p>为了克服自然语言和异构流量数据之间的模态差距，TrafficLLM引入了<strong>流量领域令牌生成器标记化</strong>来处理流量检测和生成任务的不同输入以适应LLM。该机制通过在大规模流量域语料库上训练专门的Tokenizer，有效地扩展了LLM的原生令牌生成模型。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/method-1.png" style="zoom:90%;"><h3 id="双阶段微调">双阶段微调</h3><p>TrafficLLM采用双阶段微调方案来实现LLM在不同流量域任务中的鲁棒表示学习。该方式在不同阶段分别训练LLM理解指令并学习与任务相关的流量模式，以此建立TrafficLLM对不同流量检测和生成任务的任务理解和流量模式推理能力。</p><p>主要关注LLM微调的两个挑战：<br>（i）如何帮助LLM理解与任务相关的自然语言，以确定应该执行哪个任务；（ii）如何学习不同任务之间的特定任务流量模式。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/method-2.png" style="zoom:90%;"><p>在双阶段微调的第一阶段，引入<strong>自然语言指令微调</strong>，将网络安全领域的专业任务描述文本注入LLM。第二阶段是针对特定任务的流量微调。在理解了任务后，迫使TrafficLLM<strong>学习下游任务下的流量模式</strong>。在这个阶段，使用流量数据和标签样本对对LLM进行微调，以在流量检测任务和流量生成任务下对流量表示进行建模。</p><h3 id="基于参数有效微调的可扩展适应（EA-PEFT）">基于参数有效微调的可扩展适应（EA-PEFT）</h3><p>为了使LLM适应新的流量环境，TrafficLLM提出了一种<strong>具有参数有效微调（EA-PEFT）的可扩展自适应方法</strong>，以低开销更新模型参数。该技术将模型能力拆分为不同的PEFT模型，这有助于最大限度地降低流量模式变化引起的动态场景的适应成本。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/method-3.png" style="zoom:90%;"><p>在EA-PEFT中，TrafficLLM Adaptor允许灵活的操作来更新旧模型或注册新任务。例如，当面对客户端版本升级（例如应用程序版本漂移）或攻击方法变更（例如HTTP请求体变更）引起的WAD任务中的流量更新时，Adaptor可以调用Model_update并提供新的EAC或WAD数据集来更新特定的PEFT模型。此外，TrafficLLM可以轻松添加新的流量分析场景。Adaptor可以调度Model_Insert来训练新的PEFT模型，并将其插入EA-PEFT框架中。在此基础上，TrafficLLM可以通过EA-PEFT的轻量级自适应方案轻松扩展到各种流量领域任务。</p><h1 id="准备工作">准备工作</h1><p>显卡配置：<br><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/p1.png" style="zoom:80%;"></p><p>配置环境</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conda create -n trafficllm python=3.9</span><br><span class="line"></span><br><span class="line">conda activate trafficllm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clone our TrafficLLM</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/ZGC-LLM-Safety/TrafficLLM.git</span><br><span class="line"><span class="built_in">cd</span> TrafficLLM</span><br><span class="line"><span class="comment"># Install required libraries</span></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"><span class="comment"># training的话还需安装下面的包</span></span><br><span class="line"><span class="comment"># pip install rouge_chinese nltk jieba datasets</span></span><br></pre></td></tr></table></figure><p>下载数据集（这里只使用ustc-tfc-2016数据集）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;drive.google.com&#x2F;drive&#x2F;folders&#x2F;1RZAOPcNKq73-quA8KG_lkAo_EqlwhlQb</span><br></pre></td></tr></table></figure><p>使用魔搭下载基座模型（这里为chatglm2-6b）：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope</span><br><span class="line">modelscope download --model ZhipuAI/chatglm2-6b <span class="comment"># (默认cache地址)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># modelscope download --model &#x27;Qwen/Qwen2-7b&#x27; --local_dir &#x27;path/to/dir&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="Evauation">Evauation</h1><p><code>evaluation.py</code> 文件主要是用来评估 TrafficLLM 的效果。它支持检测任务 (detection) 和生成任务 (generation)。会根据不同任务类型（检测或生成），加载模型并对测试集进行评估，最终输出模型的预测结果以及评估指标（如准确率、精度、F1分数等）</p><p>具体代码解析：</p><ol><li><strong>导入相关库</strong>：<ul><li>主要使用了 <code>transformers</code> 库来加载和处理预训练模型。</li><li><code>sklearn.metrics</code> 用于计算评估指标如准确率、精度、召回率等。</li><li><code>tqdm</code> 用于显示进度条，帮助跟踪任务的处理进度。</li><li><code>fire</code> 库可以方便地将 Python 函数暴露为命令行接口。</li></ul></li><li><strong><code>test_set_to_prompt</code> 函数</strong>：<ul><li>这个函数用于将测试集的数据转换为模型需要的输入格式 (<code>test_prompts</code>) 和对应的目标输出 (<code>target_responses</code>)。</li><li>它从 <code>test_set</code> 中读取每个测试数据的 <code>instruction</code> 和 <code>output</code> 字段，并分别作为输入和目标。</li></ul></li><li><strong><code>td_evaluation</code> 函数</strong>：<ul><li>这是检测任务的评估函数 (<code>traffic_task == &quot;detection&quot;</code>)。它根据模型的输出和目标标签（<code>target_responses</code>），计算准确率、精度、召回率、F1分数，并打印混淆矩阵和分类报告。</li><li>通过 <code>label_file</code> 中的标签字典，将模型生成的响应和目标响应映射到相应的标签。</li></ul></li><li><strong><code>tg_evaluation</code> 函数</strong>：<ul><li>这是生成任务的评估函数 (<code>traffic_task == &quot;generation&quot;</code>)。它将模型的生成结果按类别进行整理，并将其写入一个 JSON 文件，方便后续分析。</li></ul></li><li><strong><code>main</code> 函数</strong>：<ul><li><strong>模型加载</strong>：根据 <code>model_name</code> 和 <code>ptuning_path</code> 参数，加载指定的预训练模型。如果提供了 <code>ptuning_path</code>，则会加载微调后的模型参数。</li><li><strong>评估逻辑</strong>：<ul><li>从 <code>test_file</code> 中读取测试集，并通过 <code>test_set_to_prompt</code> 函数转换为输入和目标。</li><li>对每个输入（<code>test_prompt</code>）调用模型的 <code>chat</code> 方法，获取模型的预测输出。</li><li>最后根据 <code>traffic_task</code> 的不同，调用相应的评估函数（<code>td_evaluation</code> 或 <code>tg_evaluation</code>）来评估模型的性能。</li></ul></li></ul></li><li><strong>命令行接口</strong>：<ul><li>通过 <code>fire.Fire(main)</code>，将 <code>main</code> 函数作为命令行入口。你可以通过命令行传递参数来运行该评估脚本。</li></ul></li></ol><p>执行 Evaluation 的命令:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python evaluation.py --model_name /Path/ZhipuAI/chatglm2-6b --traffic_task detection --test_file /home/ustc-tfc-2016/ustc-tfc-2016_detection_packet_test.json --label_file /Path/ustc-tfc-2016/ustc-tfc-2016_label.json --ptuning_path /Path/TrafficLLM-master/models/chatglm2/peft/ustc-tfc-2016-detection-packet/checkpoint-10000/</span><br></pre></td></tr></table></figure><p>最开始会报错没有<code>fire</code>包，直接 pip install 一下就行了。</p><p>注意，这里需要配置基座模型的路径以及PEFT微调后的模型路径，是因为微调模型依赖基础模型提供的大部分权重和结构，所以在运行时需要同时加载两者。</p><p>总结来说：</p><ul><li><strong>基础模型（<code>--model_name</code>）</strong> ：提供了模型的完整结构和大部分预训练权重。<ul><li>提供了模型的核心能力（通用的语言理解、生成能力等）。</li></ul></li><li><strong>微调模型（<code>--ptuning_path</code>）</strong> 只包含在特定任务上微调的少量参数（如前缀编码器的权重），这些参数必须在基础模型的上下文中才能工作。<ul><li>提供针对特定任务的增强部分，是对这些能力的专门调整和优化，用于在特定任务上表现得更好。</li></ul></li></ul><p>结果：<br><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/p2.png" style="zoom:80%;"></p><h1 id="Web-Server">Web Server</h1><p>先修改配置文件<code>config.json</code>，<code>model_path</code>改成自己下载的路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;model_path&quot;: &quot;models&#x2F;chatglm2&#x2F;chatglm2-6b&#x2F;&quot;,</span><br><span class="line">    &quot;peft_path&quot;: &quot;models&#x2F;chatglm2&#x2F;peft&#x2F;&quot;,</span><br><span class="line">    &quot;peft_set&quot;: &#123;</span><br><span class="line">      &quot;NLP&quot;: &quot;instruction&#x2F;checkpoint-8000&#x2F;&quot;,</span><br><span class="line">      &quot;MTD&quot;: &quot;ustc-tfc-2016-detection-packet&#x2F;checkpoint-10000&#x2F;&quot;,</span><br><span class="line">      &quot;BND&quot;: &quot;iscx-botnet-2014-detection-packet&#x2F;checkpoint-5000&#x2F;&quot;,</span><br><span class="line">      &quot;WAD&quot;: &quot;csic-2010-detection-packet&#x2F;checkpoint-6000&#x2F;&quot;,</span><br><span class="line">      &quot;AAD&quot;: &quot;dapt-2020-detection-packet&#x2F;checkpoint-20000&#x2F;&quot;,</span><br><span class="line">      &quot;EVD&quot;: &quot;iscx-vpn-2016-detection-packet&#x2F;checkpoint-4000&#x2F;&quot;,</span><br><span class="line">      &quot;TBD&quot;: &quot;iscx-tor-2016-detection-packet&#x2F;checkpoint-10000&#x2F;&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;tasks&quot;: &#123;</span><br><span class="line">      &quot;Malware Traffic Detection&quot;: &quot;MTD&quot;,</span><br><span class="line">      &quot;Botnet Detection&quot;: &quot;BND&quot;,</span><br><span class="line">      &quot;Web Attack Detection&quot;: &quot;WAD&quot;,</span><br><span class="line">      &quot;APT Attack Detection&quot;: &quot;AAD&quot;,</span><br><span class="line">      &quot;Encrypted VPN Detection&quot;: &quot;EVD&quot;,</span><br><span class="line">      &quot;Tor Behavior Detection&quot;: &quot;TBD&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 streamlit 运行 server：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamlit run trafficllm_server.py</span><br></pre></td></tr></table></figure><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/p3.png" style="zoom:80%;"><p>会给出一个访问地址，浏览器打开：<br><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/10/17/LLM-Learning5/p4.png" style="zoom:80%;"></p>]]></content>
    
    
    <summary type="html">论文复现 —— TrafficLLM</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="应用" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%BA%94%E7%94%A8/"/>
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="流量分析" scheme="https://enchantedovo.cn/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>LLM for Security 论文记录 0919</title>
    <link href="https://enchantedovo.cn/2024/09/19/LLM-Security2/"/>
    <id>https://enchantedovo.cn/2024/09/19/LLM-Security2/</id>
    <published>2024-09-19T02:54:14.000Z</published>
    <updated>2024-11-14T03:17:26.302Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>放一些大模型与安全相关的论文（涵盖范围比较广，比较杂）</p></blockquote><h1 id="相关文章">相关文章</h1><p><strong>GitHub Copilot 在编程方面表现出色，但它是否确保了负责任的输出？</strong></p><p>简介：大语言模型（LLMs）的快速发展极大地推动了代码补全工具（LCCTs）的进化。这些工具通过整合多种信息源并优先提供代码建议，与传统LLMs不同，它们在安全方面面临独特挑战。特别是，LCCTs在训练时依赖专有代码数据集，这可能增加敏感数据泄露的风险。研究表明，LCCTs存在显著的安全漏洞，例如，针对GitHub Copilot的越狱攻击成功率高达99.4%，而从其中提取敏感用户数据的成功率也相当高。这些发现强调了LCCTs在安全方面的挑战，并为加强其安全框架提供了重要方向。</p><p>链接：<a href="https://arxiv.org/abs/2408.11006">https://arxiv.org/abs/2408.11006</a></p><br><p><strong>语言模型应用程序中的数据泄露：对 OpenAI 的 GPT 产品的深入调查</strong></p><p>简介：研究者们在探索大语言模型（LLM）应用的数据实践透明度时，以OpenAI的GPT应用生态系统作为案例研究。他们开发了一个基于LLM的框架，对GPT及其动作（外部服务）的源代码进行了静态分析，以揭示其数据收集的做法。研究发现，这些动作收集了大量用户数据，包括OpenAI明令禁止的敏感信息，如密码。此外，一些与广告和分析相关的动作嵌入在多个GPT中，使得它们能够跨平台追踪用户活动。动作的共现还可能导致用户数据的进一步暴露，增加了隐私风险。研究者还开发了一个基于LLM的隐私政策分析框架，用以自动检查动作的数据收集是否与隐私政策中的披露相一致。结果显示，大多数收集的数据类型在隐私政策中并未明确披露，仅有5.8%的动作清晰地说明了它们的数据收集实践。这一发现强调了LLM应用在数据实践透明度方面存在的问题，并指出了加强隐私保护措施的必要性。</p><p>链接：<a href="https://arxiv.org/abs/2408.13247">https://arxiv.org/abs/2408.13247</a></p><br><p><strong>去伪存真：利用执行反馈对生成的代码候选进行排序</strong></p><p>简介：大语言模型（LLMs）如GPT-4、StarCoder和CodeLlama正在改变编程方式，通过自然语言描述自动生成代码。尽管如此，生成正确代码仍然具有挑战性。为了提高正确代码的生成率，开发者通常使用LLMs生成多个候选解决方案，然后进行代码排名，即从多个候选代码中选择正确的一个。现有的代码排名研究主要分为基于执行和非基于执行的方法。基于执行的方法虽然有效，但受限于高质量单元测试的稀缺和潜在的安全风险。而非基于执行的方法，如CodeRanker，主要依赖分类标签进行训练，难以捕捉细微错误和提供错误洞察。</p><p>为了克服这些限制，研究者提出了一种新的方法——RankEF。RankEF是一种创新的代码排名方法，它利用执行反馈并通过多任务学习整合代码分类与执行反馈生成。这种方法使模型能够在不执行代码的情况下，理解错误代码的原因，并区分正确与错误的解决方案。在三个代码生成基准上的实验显示，RankEF显著优于现有的CodeRanker，展现出在代码排名方面的高效性和准确性。</p><p>链接：<a href="https://arxiv.org/abs/2408.13976">https://arxiv.org/abs/2408.13976</a></p><br><p><strong>调查贝叶斯垃圾邮件过滤器在检测经语言模型修改的垃圾邮件中的有效性</strong></p><p>简介：垃圾邮件和网络钓鱼是网络安全的重大威胁，贝叶斯垃圾邮件过滤器如 SpamAssassin 是重要防御工具。但大语言模型的出现带来新挑战，因其强大、易获取且成本低，可能被用于制作复杂垃圾邮件逃避传统过滤器。研究者开发管道测试 SpamAssassin 对经语言模型修改邮件的有效性，结果显示其会将高达 73.7%的此类邮件误分类为合法邮件，而简单字典替换攻击成功率仅 0.4%。这凸显了经语言模型修改的垃圾邮件的重大威胁及成本效益。该研究为当前垃圾邮件过滤器的漏洞及网络安全措施的持续改进提供了关键见解。</p><p>链接：<a href="https://arxiv.org/abs/2408.14293">https://arxiv.org/abs/2408.14293</a></p><br><p><strong>检测人工智能缺陷：针对语言模型内部故障的目标驱动攻击</strong></p><p>简介：大语言模型（LLMs）在人工智能领域的重要性日益凸显，但这些模型在预训练语料中可能包含有害内容，导致生成不适当的输出。为了提高LLMs的安全性，研究人员探索了检测模型内部缺陷的方法。目前的研究主要集中在越狱攻击上，这些攻击通过构建对抗性内容来诱导模型产生意外响应。然而，这些方法依赖于提示工程，既耗时又需要特别设计的问题。</p><p>为了解决这些挑战，研究人员提出了一种新的攻击范式，即目标驱动的攻击，它专注于直接引出目标响应，而不是优化提示。研究中引入了名为ToxDet的LLM，作为有毒内容的检测器。ToxDet能够根据目标有毒响应生成可能的问题和初步答案，以诱导目标模型产生与提供含义相当的有毒响应。ToxDet通过与目标LLM交互并接收奖励信号进行训练，利用强化学习优化过程。尽管ToxDet主要针对开源LLMs，但经过微调后，它也可以转移到攻击如GPT-4o这样的黑盒模型，并取得了显著结果。在AdvBench和HH-Harmless数据集上的实验结果证明了该方法在检测目标LLMs生成有害响应倾向方面的有效性。这不仅揭示了LLMs的潜在漏洞，还为研究人员提供了加强模型抵御此类攻击的宝贵资源。</p><p>链接：<a href="https://arxiv.org/abs/2408.14853">https://arxiv.org/abs/2408.14853</a></p><br><p><strong>参数高效的量化专家混合体与视觉 - 语言指令调优在半导体电子显微图像分析中的应用</strong></p><p>简介：研究者指出半导体在基础模型中研究不足，需增强半导体器件技术。为此，他们推出了 sLAVA，一个针对半导体制造的小型视觉语言助手，专注于电子显微镜图像分析。采用师生范式，以 GPT-4 等基础视觉语言模型为教师，为 sLAVA 创建遵循指令的多模态数据，解决数据稀缺问题，可在预算有限的消费级硬件上进行任务，且企业能在自身基础设施内用专有数据安全微调框架以保护知识产权。严格实验表明，该框架超越传统方法，能处理数据偏移并实现高通量筛选，有助于半导体制造中的电子显微镜图像分析，为企业提供了一种有效的解决方案，也为半导体技术发展提供了新的思路和方法。</p><p>链接：<a href="https://arxiv.org/abs/2408.15305">https://arxiv.org/abs/2408.15305</a></p><!--  --><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="p1.png"  style="zoom:55%;" /> -->]]></content>
    
    
    <summary type="html">LLM for Security 论文记录 0919</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="论文阅读" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="大模型&amp;安全" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%AE%89%E5%85%A8/"/>
    
    
    <category term="生成式人工智能" scheme="https://enchantedovo.cn/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="网络安全" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
    <category term="大模型安全" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读记录 —— LLM 相关</title>
    <link href="https://enchantedovo.cn/2024/07/20/LLM-Learning4/"/>
    <id>https://enchantedovo.cn/2024/07/20/LLM-Learning4/</id>
    <published>2024-07-20T12:16:39.000Z</published>
    <updated>2024-07-23T11:18:57.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learning-to-Reason-and-Memorize-with-Self-Notes">Learning to Reason and Memorize with Self-Notes</h1><ul><li>原文链接：<a href="https://arxiv.org/abs/2404.14963">Learning to Reason and Memorize with Self-Notes</a></li><li>发表：NeurIPS 2023</li><li>时间：2023</li><li>机构：Meta AI</li></ul> <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/20/LLM-Learning4/p2.png" class title="p2"><h2 id="研究动机（Motivation）">研究动机（Motivation）</h2><p>大型语言模型（LLMs）在<strong>多步推理</strong>任务中表现不佳，并且<strong>无法保留先前的推理步骤</strong>以供将来使用。这限制了它们在复杂问题解答中的性能。</p><h2 id="解决的问题">解决的问题</h2><p>论文提出了一种名为“Self-Notes”的方法，旨在解决上述问题。这种方法允许<strong>模型在阅读上下文时，随时偏离输入内容生成明确的推理标记（即“Self-Notes”）</strong>，以增强其记忆并实现多步推理。</p><h2 id="方法">方法</h2><p>（需要微调）</p><ul><li>Self-Notes：与链式思维（Chain-of-Thought）或草稿板（Scratchpad）方法不同，Self-Notes 允许模型<strong>在任何时候</strong>插入与输入上下文和问题交织的内部推理笔记。</li><li>模型架构：考虑了一个自回归的变换器模型，该模型在生成最终输出之前，可以通过生成“笔记标记”来丰富上下文。</li></ul><h2 id="思考">思考</h2><ul><li>优势：可以帮助语言模型更好地处理长序列或复杂任务，且不需要太多的监督信号。</li><li>关键思路：与草稿板方法不同，本论文中的模型可以随时偏离输入上下文以明确思考。这种方法允许模型在阅读上下文的同时回忆信息并进行推理，从而扩展其记忆并实现多步推理。相比当前领域的研究状况，这篇论文的思路在于通过自我笔记的方式来解决有限上下文记忆和多步推理的问题。</li></ul><h1 id="DUP">DUP</h1><p>Achieving &gt;97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems</p><ul><li>发表：<a href="https://arxiv.org/abs/2404.14963">https://arxiv.org/abs/2404.14963</a></li><li>时间：2024.5.29</li><li>机构：武汉大学 &amp; 悉尼大学 &amp; 南洋理工大学</li></ul> <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/20/LLM-Learning4/p1.png" class title="p1"><h2 id="摘要总结">摘要总结</h2><p>大型语言模型（LLMs）在各种自然语言处理任务中表现出色，但在处理复杂的数学文字题时，它们的推理能力常常不尽人意。这类问题通常包含三种错误：语义理解错误、计算错误和步骤遗漏错误。尽管已有研究关注计算错误和步骤遗漏错误，但语义理解错误作为限制LLMs性能的主要因素，却被忽视了。<br>论文提出了一种简单但有效的方法，称为“Deeply Understanding the Problems”（DUP），通过解决语义理解错误来提高LLMs<strong>解决数学问题</strong>的能力。DUP方法的核心是<strong>鼓励LLMs深入理解问题</strong>，<strong>提取关键的解题信息</strong>，以进行更好的推理。</p><h2 id="研究动机">研究动机</h2><p>尽管 LLMs 在各种自然语言理解和生成任务中表现出色，但它们在推理任务（例如数学推理、常识推理和符号推理）中的表现往往不尽如人意。CoT prompting 方法虽然能够显著提升 LLMs 的推理能力，但它在处理复杂数学问题，尤其是数学应用题时，仍然存在以下三个主要问题：</p><ul><li>语义误解错误: LLMs 无法准确理解问题的核心内容，导致无法提取出解决问题的关键信息。</li><li>计算错误: LLMs 在进行计算时可能出现错误，导致最终答案不正确。</li><li>步骤缺失错误: LLMs 可能会遗漏解决问题的某个步骤，导致推理过程不完整。<br>现有的研究主要集中在解决计算错误和步骤缺失错误，而<strong>对语义误解错误关注较少</strong>。然而，语义误解错误是限制 LLMs 推理能力的主要因素。<br>因此，文章提出了一种名为 Deeply Understanding the Problems (DUP) 的方法，旨在通过解决语义误解错误来提升 LLMs 的数学问题解决能力。</li></ul><h2 id="方法-2">方法</h2><p>DUP（Deeply Understanding the Problems）方法是一种针对大型语言模型（LLMs）的提示策略，旨在提高它们解决数学文字题的能力。以下是DUP方法的详细步骤和特点：</p><ol><li><p>揭示核心问题（Reveal the Core Question）<br>这是DUP方法的第一阶段，目的是从复杂和冗长的问题描述中明确问题的核心。通过设计一个提示，要求LLM明确提取问题的核心部分，从而帮助模型集中注意力于问题的目标。</p></li><li><p>提取解题信息（Extract the Problem-solving Information）<br>第二阶段进一步从问题描述中提取对解决核心问题至关重要的信息。这一步骤通过一个提示来实现，该提示要求LLM列出与核心问题直接相关的最有用的信息。</p></li><li><p>生成并提取答案（Generate and Extract the Answer）<br>在前两个阶段的基础上，第三阶段将核心问题和解题信息结合起来，生成详细的回答，并从中提取最终答案。这一步骤通过一个模板提示来实现，该模板明确指出了目标和解决问题所需的必要信息。</p></li></ol><h2 id="实验">实验</h2><ul><li>数据集:<ul><li>算术推理数据集: GSM8K、SVAMP、MultiArith、AddSub、AQuA、SingleEq</li><li>常识推理数据集: CommonsenseQA、StrategyQA</li><li>符号推理数据集: Last Letter、Coin Flip</li></ul></li><li>基线方法：<ul><li>零样本方法: Zero-shot CoT、Least-to-Most、Zero-shot PS+</li><li>少样本方法: Manual-CoT、Auto-CoT</li></ul></li><li>实验设置:<ul><li>LLMs: GPT-3.5-Turbo 和 GPT-4</li><li>解码策略: 具有自一致性（SC）的解码策略（Wang et al., 2023b）</li></ul></li><li>实验结果:<ul><li>DUP 方法在所有推理数据集上均取得了显著优于其他方法的性能。</li><li>DUP 方法的零样本性能甚至超过了少样本方法的性能。</li><li>DUP 方法在 GSM8K 和 SVAMP 数据集上取得了新的 SOTA 结果。</li></ul></li><li>消融实验:<ul><li>探究 DUP 方法中各个阶段的重要性。</li><li>探究如何在不降低性能的情况下降低 DUP 方法的推理成本。（将三阶段提示合并为一个提示）</li></ul></li><li>讨论和分析:<ul><li>分析 DUP 方法与 SC 解码的兼容性。</li><li>验证 DUP 方法是否可以应用于开源 LLMs。</li><li>分析更准确的核心问题和问题解决信息如何提升推理性能。</li><li>进行错误分析，验证 DUP 方法是否有效减少了语义误解错误。</li></ul></li></ul> <!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/20/LLM-Learning4/p1.png" class="" title="p1"> --><h1 id="Re-Reading-Improves-Reasoning-in-Large-Language-Models">Re-Reading Improves Reasoning in Large Language Models</h1><p>重读能提升大模型推理能力</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/20/LLM-Learning4/p3.png" style="zoom:65%;"><h2 id="研究背景">研究背景</h2><p>本文旨在介绍一种简单有效的提示方法“重复阅读(RE2)”以提升大型语言模型(LLMs)的推理能力。这一方法受到人类在学习和解决问题时倾向于<strong>重复阅读问题以提高理解</strong>的观察启发。</p><h2 id="研究目标和假设">研究目标和假设</h2><p>本文旨在探讨重复阅读问题是否能提升LLMs的推理性能。关键假设是，重复阅读问题可以帮助LLMs更深入地理解输入，从而提高推理能力。</p><h2 id="研究方法">研究方法</h2><p>本文介绍了“RE2”<strong>提示</strong>方法，即在提示中重复两次输入问题。这可以作为一个“即插即用”模块，与各种诱导思考的提示策略（如思维链和程序辅助语言）集成使用。</p><h2 id="结果和发现">结果和发现</h2><p>论文评估了RE2在各种推理基准测试(包括算术、常识和符号推理任务)上的有效性。<br>研究结果显示，RE2 能够持续提升LLMs的推理性能，包括ChatGPT等指令微调(IFT)模型和 LLaMA-2 等非IFT模型。RE2与各种诱导思考的提示方法兼容,也可以有效地与少样本提示和自一致性方法相结合。</p><h2 id="思考-2">思考</h2><p>很简单的方法，可以学习其写作技巧。</p>]]></content>
    
    
    <summary type="html">论文阅读笔记 —— LLM 相关</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="论文阅读" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="论文阅读" scheme="https://enchantedovo.cn/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读记录 —— LLM Agent</title>
    <link href="https://enchantedovo.cn/2024/07/19/LLM-Learning3/"/>
    <id>https://enchantedovo.cn/2024/07/19/LLM-Learning3/</id>
    <published>2024-07-19T11:40:33.000Z</published>
    <updated>2024-07-20T12:48:53.317Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ReAct">ReAct</h1><p>ReAct: Synergizing Reasoning and Acting in Language Models</p><ul><li>发表：ICLR</li><li>时间：2023</li><li>机构：普林斯顿大学 &amp; 谷歌</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/19/LLM-Learning3/p1.png" class title="p1"><h2 id="摘要总结">摘要总结</h2><p>虽然大模型在语言理解和交互式决策等任务中表现出色，但它们的推理（例如思维链提示）和行动（例如行动计划生成）能力通常被识别为单独的研究领域。ReAct 论文提出了一种将推理和行动相结合的新方法，<strong>允许 LLM 以交错的方式生成推理轨迹和特定任务的行动</strong>。推理轨迹帮助模型归纳、跟踪和更新行动计划以及处理异常，而Action允许它与外部源（例如知识库或环境）交互并收集附加信息。ReAct 在多个任务上取得了成功，并在表现、可解释性和可信度方面优于现有基线。</p><h2 id="研究动机">研究动机</h2><ul><li>人脑智能的启发： 人类能够将任务导向的行动与语言推理（或内心独白）无缝结合，这在人类认知中发挥着重要作用，例如自我调节、策略规划和保持工作记忆。</li><li>现有方法的局限性：<ul><li>链式思维推理（CoT）： CoT 虽然能进行推理，但通常是<strong>静态</strong>的，<strong>无法与外部世界交互</strong>，导致推理过程中出现事实虚构和错误传播等问题。</li><li>行动生成： 最近的行动生成方法通常将多模态观察转换为文本，使用语言模型生成特定领域的动作或计划，但<strong>缺乏对高级目标进行抽象推理</strong>或<strong>维持工作记忆</strong>的能力。</li></ul></li></ul><h2 id="主要贡献">主要贡献</h2><ul><li>提出了一种新的提示方法ReAct 模型，<strong>将推理和行动相结合</strong>，使 LLM 能够以交错的方式生成推理轨迹和特定任务的行动，从而实现更有效的推理和决策。</li><li>多基准评估： 在多个基准测试（包括问答、事实验证、文本游戏和网页导航）中进行了广泛的实验，证明了 ReAct 在少量学习设置下优于仅进行推理或行动生成的现有方法。</li><li>通过系统地进行消融实验和分析，研究了行动在推理任务中的重要性以及推理在交互任务中的重要性。</li><li>分析了 ReAct 在提示设置下的局限性（即推理和行动行为的有限支持），并进行了初步的微调实验，展示了 ReAct 在更多训练数据下改进的潜力。</li></ul><h2 id="叙事逻辑">叙事逻辑</h2><p>该文章首先从人类智能出发，指出人类能够将任务导向的行动与语言推理相结合，从而进行有效的认知活动。随后，文章指出现有方法（如 CoT 和行动生成）的局限性，例如 CoT 缺乏外部世界交互导致虚构事实，行动生成缺乏高级目标推理能力。接着，文章提出了 ReAct 模型，该模型通过将推理和行动相结合，使 LLM 能够更有效地进行推理、规划和决策，并与外部环境进行交互。文章通过跨任务评估证明了 ReAct 的有效性，并分析了其可解释性和可控性。最后，文章探讨了 ReAct 的局限性，并提出了改进方向，例如模型细化和使用更多训练数据。</p><h2 id="方法">方法</h2><p>ReAct 通过将 LLM 的行动空间扩展到包括语言行动（推理轨迹），允许模型进行动态推理和规划，同时与外部环境进行交互。首先，使用Few-Shot（人类轨迹）指导 LLM 生成推理轨迹和行动，这些示例包括分解目标、提取信息、应用常识等。接着，LLM 可以通过交互式地与外部环境（如维基百科）API 进行交互来支持推理，例如检索信息和更新行动计划。</p><h2 id="实验">实验</h2><ul><li>数据集和任务：<ul><li>知识密集型推理任务：<ul><li>多跳问答： HotpotQA，需要推理两个或更多维基百科段落来回答问题。</li><li>事实验证： Fever，需要根据维基百科段落判断陈述是否被支持、反驳或信息不足。</li></ul></li><li>决策制定任务：<ul><li>基于文本的游戏： ALFWorld，需要通过文本行动与模拟家居环境交互，实现高层次目标。</li><li>网页导航： WebShop，需要根据用户指令进行网页交互，购买满足要求的产品。</li></ul></li></ul></li><li>行动空间：<ul><li>知识密集型推理任务： 简单的维基百科 web API，包括搜索实体、查找字符串和完成回答三种行动。</li><li>决策制定任务： 根据任务类型设计，例如 WebShop 包含搜索、选择产品、选择选项和购买等行动。</li></ul></li><li>基线方法：<ul><li>标准提示： 移除所有推理轨迹和行动，只提供问题和观察。</li><li>思维链提示 (CoT)： 移除行动和观察，只进行推理。</li><li>自我一致性 (CoT-SC)： 使用少量 CoT 轨迹作为样本，并选择多数答案。</li><li>仅行动提示： 移除推理轨迹，模型只与外部环境交互。</li></ul></li><li>评估指标：<ul><li>知识密集型推理任务： 准确匹配、幻觉、推理错误、搜索结果错误、幻觉。</li><li>决策制定任务： 成功率、得分、成功率和专家人类的表现。</li></ul></li><li>其他实验：<ul><li>GPT-3 实验： 验证 ReAct 提示的有效性在不同大型语言模型上的通用性。</li><li>人类在环 (HITL) 行为纠正： 允许人类编辑推理轨迹，纠正模型行为。</li><li>微调： 使用正确答案轨迹微调较小的语言模型，使其能够解码推理轨迹和行动。</li></ul></li></ul><h2 id="评价">评价</h2><p>后面agent范式的基准，很多都是基于这个进行改进的</p><h1 id="Reflexion">Reflexion</h1><p>Reflexion: Language Agents with Verbal Reinforcement Learning</p><ul><li>发表：NeurIPS</li><li>时间：2023.10.10</li><li>机构：美国东北大学</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/19/LLM-Learning3/p2.png" class title="p2"><h2 id="摘要总结-2">摘要总结</h2><p>agent越来越多地用于与外部环境进行交互。然而，对 agent 来说，进行传统的强化学习方法需要大量的训练样本和昂贵的模型微调。论文提出了 Reflexion，不通过更新权重，而是<strong>通过语言反馈来增强语言 agent</strong>。Reflexion 足够灵活，可以合并各种类型和来源的反馈信号，并在跨不同任务（顺序决策、编码、语言推理）的基准代理上获得显着改进。例如，Reflexion 在 HumanEval 编码基准上实现了 91% 的 pass@1 准确率，超过了之前最先进的 GPT-4 的 80%。我们还使用不同的反馈信号、反思合并方法和 agent 类型进行消融分析。</p><h2 id="研究动机-2">研究动机</h2><p>LLM-based Agent的在与外界环境交互、自主决策方面展现出巨大潜力，而传统强化学习方法在 agent 应用上具有局限性。</p><h2 id="主要贡献-2">主要贡献</h2><ul><li>提出了 Reflexion，一种“口头”强化的新范式，它将策略参数化为代理的记忆编码与 LLM 参数选择配对。</li><li>探索了 LLM 中自我反思的这一新兴特性，并根据经验表明，自我反思对于通过少量试验学习复杂任务非常有用。</li><li>引入了 LeetcodeHardGym 数据集：一个包含 40 个具有挑战性的 Leetcode 困难问题的代码生成 RL gym 环境。用于测试代码生成代理的能力，并提供一个新的基准。</li><li>实验表明 Reflexion 在多个任务的强大基线上实现了改进，并在各种代码生成基准上取得了最先进的结果。</li></ul><h2 id="叙事逻辑-2">叙事逻辑</h2><ol><li>提出问题：LLMs 作为语言代理在处理外部环境方面展现出潜力，但传统强化学习方法学习效率低下。</li><li>提出解决方案：提出 Reflexion 框架，利用语言代理的自我反思能力进行强化学习；通过语言反馈而非参数更新来增强代理，并存储反思文本以指导后续决策。</li><li>论述 Reflexion 的优势：相比于传统强化学习方法，Reflexion 更轻量级，无需微调 LLM；允许更细微的反馈形式，并提供更明确和可解释的情境记忆。</li><li>实验验证：在决策、推理和编程任务中，Reflexion 代理的表现都优于基线方法；Reflexion 在 LeetcodeHardGym 数据集上也取得了优异的成绩。</li><li>讨论局限性：Reflexion 可能陷入局部最优解；需要更强的 LLM 进行自我评估；没有成功的正式保证；需要更多的安全性和道德考量；可能使自主 agent 更具可解释性。</li><li>总结与展望：Reflexion 框架为语言代理的快速有效学习提供了一种新的思路；未来可以探索更多高级技术，例如价值学习和离策略探索技术。</li></ol><h2 id="实现">实现</h2><ul><li>Actor：角色，使用 LLM 实现，它基于可观察的状态，利用<strong>提示生成文本和动作</strong>。</li><li>Evaluator：评估器，用于评估 Actor 输出的质量。<strong>将生成的轨迹作为输入，计算在给定任务上下文中的奖励分数</strong>。</li><li>Self-reflection：自我反思，<strong>使用 LLM 实现</strong>，用于生成基于语言的反思。在给出稀疏奖励信号，如二元状态（成功/失败），当前轨迹及其持久记忆内存。自我反思模型会生成<strong>细致入微且具体的反馈</strong>，这种反馈相比标量奖励提供更多信息，然后被存储在代理的内存 (mem) 中。</li><li>Memory：内存组件，为 Agent 提供额外的上下文。它提供<strong>短期记忆和长期记忆</strong>。在推理时，Actor 根据短期和长期记忆做出决定，在强化学习设置中，<strong>轨迹历史充当短期记忆，而自我反思模型的输出则存储在长期记忆中</strong>。</li><li><strong>过程</strong>：在第一次试验中，Actor 通过与环境交互产生轨迹 τ0。然后评估器产生一个分数 r0；rt 是标量奖励；为了放大 r0，自我反思模型分析 {τ0, r0} 集合以生成存储在内存 mem 中的摘要 sr0。srt 是对试验 t 的语言反馈。Actor、Evaluator 和 Self-Reflection 模型通过循环试验协同工作，直到 Evaluator 认为 τt 是正确的。每次试验后 t、srt 都会附加存入 mem。在实践中，通过存储经验的最大数量 Ω（通常设置为 1-3）来限制 mem，从而不超过 LLM 的上下文限制。</li></ul><h2 id="实验-2">实验</h2><ul><li>任务类型： 选择了三种类型的任务来评估 Reflexion 的效果：<ul><li>决策任务： 在 AlfWorld 环境中进行多步决策，例如寻找隐藏物体、移动物体等。</li><li>推理任务： 在 HotPotQA 数据集上进行基于 Wikipedia 的问答，需要理解和推理多个支持文档。</li><li>编程任务： 在 HumanEval、MBPP 和 LeetcodeHardGym 数据集上进行 Python 和 Rust 代码生成。</li></ul></li><li>基线方法： 与多种基线方法进行比较，包括：<ul><li>ReAct： 基于大型语言模型的决策模型。</li><li>Chain-of-Thought (CoT)： 基于大型语言模型的推理模型。</li><li>CodeT： 基于大型语言模型的代码生成模型。</li></ul></li><li>评估指标： 根据任务类型选择不同的评估指标，例如：<ul><li>决策任务： 成功解决任务的百分比。</li><li>推理任务： 答案与正确答案完全匹配的百分比。</li><li>编程任务： 函数体生成的准确率，以及代码是否通过单元测试。</li></ul></li></ul><h2 id="评价-2">评价</h2><ul><li>论文提出了一种强化学习方法。传统的调优主要是通过训练调整网络参数，而文中提出的方法则是<strong>分析错误，形成反思的文字并保存</strong>，在之后的决策中，将其<strong>作为上下文</strong>以帮助决策。</li><li>它<strong>利用大模型</strong>构造了角色的行为、对结果的评价、当不能达成目标时，利用大模型来反思执行过程中具体哪一步出了问题，并将其作为反思存储。这样就构造了<strong>基于当前环境的短期存储</strong>，和<strong>基于反思的长期存储</strong>，结合二者使模型在未来做出更好的决策。</li><li>提供了完整的代码</li><li>局限：依赖于 LLM 的自我评估能力（或启发式方法）；无法保证成功。</li></ul><h1 id="Retroformer">Retroformer</h1><p>Retroformer: Retrospective large language agents with policy gradient optimization</p><ul><li>发表：ICLR</li><li>时间：2024</li><li>机构：Salesforce AI</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/19/LLM-Learning3/p3.png" class title="p3"><h2 id="摘要总结-3">摘要总结</h2><p>一些agent通过口头反馈以完成迭代的反馈学习，他们并没有以基于reward的梯度学习来进行推理和规划。本文介绍了一种新型框架Retroformer ，旨在通过学习回顾模型来强化LLM agent。该框架利用策略梯度优化基于环境反馈的提示，从而自动调整agent的行为，使其能够独立执行面向目标的多步骤任务。与现有的agent不同，Retroformer 能够从环境反馈中学习，并利用<strong>梯度信号</strong>进行推理和规划。实验结果表明，Retroformer 代理在多个任务上表现出优异的性能，证明了其有效性。</p><h2 id="研究动机-3">研究动机</h2><p>回顾性学习的优势显现 + 手动提示工程在更新的局限性</p><h2 id="主要贡献-3">主要贡献</h2><ul><li>提出了 Retroformer，它根据环境反馈迭代微调更新LLM agent的提示。采用<strong>策略梯度</strong>方法，将 Actor LLM 作为环境的一部分，允许从各种任务的广泛奖励信号中学习。</li><li>Retroformer关注微调 agent 架构中的回顾模型（retrospective model），<strong>无需访问Actor LLM 参数</strong>或通过它传播梯度。 Retroformer 的不可知特性（agnostic），使其成为各种基于云的 LLM 的灵活插件模块，例如 OpenAI GPT 或 Google Bard。</li></ul><h2 id="叙事逻辑-3">叙事逻辑</h2><p>首先指出 LLM agent在利用环境反馈和奖励方面存在的局限性，然后介绍了现有的 LLM agent和 Transformer 强化学习技术，并将 Retroformer 与相关研究进行比较。接着，论文定义了 LLM agent、环境和回顾模型的相关符号和公式，并详细介绍了 Retroformer 框架的组成部分，包括actor模型、回顾模型（retrospective model）、记忆模块和奖励构造。随后，论文解释了如何使用策略梯度优化来微调retrospective model，并生成更有利于任务完成的提示。最后，论文在 HotPotQA、AlfWorld 和 WebShop 等真实世界数据集上进行了实验，并将 Retroformer 与其他 agent 进行了比较，结果表明 Retroformer 能够显著提高 LLM 代理的任务完成效率和学习速度。</p><h2 id="方法-2">方法</h2><p>文章提出Retroformer,用策略梯度的方式调优prompt，更好的利用环境的reward。大体思路是学习一个retrospective LLM，将之前的轨迹和得分作为输入，得到一个新的prompt，这个prompt综合分析了之前的经验，从而提供一个更好的prompt。然后不断和环境交互，用PPO训练retrospective LLM。具体的，整个架构包括Actor Model，Retrospective Model和Memory Module。</p><ul><li>Actor Model是一个固定参数的LLM，用来输入prompt生成动作。</li><li>Retrospective Model用来根据之前的经验生成新的prompt（Its primary function is to produce self-reflections, offering valuable feedback for diagnosing a possible reason for prior failure and devising a new, concise, high-level plan that aims to mitigate same failure.）。</li><li>Memory Module存储长短时记忆。其中Short-term memory指当前episode，Long-term memory指Retrospective Model输出的总结了之前的失败经验的prompt。</li></ul><h2 id="实验-3">实验</h2><ul><li>设置：实验使用 GPT-3 和 GPT-4 作为冻结的actor模型，LongChat 作为回顾模型，并使用 LoRA 进行微调。</li><li>在 HotPotQA 中，Retroformer 通过分析文本匹配度来评估答案的准确性；</li><li>在 AlfWorld 中，使用二元成功/失败状态作为奖励；</li><li>在 WebShop 中，根据商品属性和选项匹配度以及价格限制来计算奖励。</li></ul><h2 id="评价-3">评价</h2><p>真正在agent中做了一个强化学习的工作，虽然是在prompt上。另外，当前使用的是基于文本的记忆模块，可以考虑使用更复杂的记忆模块，例如知识图谱，以更好地存储和利用 LLM 代理的长期记忆。</p><h1 id="ExpeL">ExpeL</h1><h1 id="MemoryBank">MemoryBank</h1><p>MemoryBank: Enhancing Large Language Models with Long-Term Memory</p><ul><li>发表：arxiv</li><li>时间：2023.05</li><li>机构：中山大学</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/19/LLM-Learning3/p4.png" class title="p4"><h2 id="摘要总结-4">摘要总结</h2><p>MemoryBank 是一种新型记忆机制，旨在解决 LLM 缺乏长期记忆的问题。它通过存储、检索和更新记忆，并总结出用户画像，使 LLM 能够更好地理解用户并适应其个性，从而提供更个性化、更具同理心的交互体验，作为一个优秀的 AI 伴侣。</p><h2 id="研究动机-4">研究动机</h2><p>艾宾浩斯（Ebbinghaus，1964）的遗忘曲线理论中记忆随时间的保留和遗忘模式</p><h2 id="主要贡献-4">主要贡献</h2><ul><li>提出 MemoryBank： 一种新型记忆机制，为 LLM 提供长期记忆能力，使其能够存储、检索和更新记忆，并绘制用户画像。（<strong>方法</strong>）</li><li>展示 MemoryBank 的实用性： 通过开发 SiliconFriend 聊天机器人，证明 MemoryBank 可以使 LLM 更好地理解用户并适应其个性，提供更个性化、更具同理心的交互体验。（<strong>用处</strong>）</li><li>验证 MemoryBank 的有效性： 通过定性和定量分析评估 MemoryBank 的有效性，结果表明其在记忆回忆、提供同理心的陪伴和理解用户画像方面表现出色。（<strong>性能</strong>）</li><li>MemoryBank 的通用性： MemoryBank 可以与开源和闭源 LLM 配合使用，并支持中英双语，具有广泛的适用性。（<strong>通用、泛化</strong>）</li></ul><h2 id="叙事逻辑-4">叙事逻辑</h2><p>首先指出了 LLM 缺乏长期记忆机制的问题，然后介绍了 MemoryBank 这种新型记忆机制，接着详细解释了 MemoryBank 的组成部分和工作原理，并通过 SiliconFriend 聊天机器人的应用案例展示了 MemoryBank 的有效性，最后通过定性和定量分析评估了 MemoryBank 的性能。</p><h2 id="方法-3">方法</h2><p>MemoryBank 通过以下几个步骤实现：</p><ul><li>记忆存储： 将用户与 LLM 的对话记录存储下来，并生成事件摘要和用户画像。</li><li>记忆检索： 使用嵌入模型和索引技术，根据当前对话内容检索相关的记忆片段。</li><li>记忆更新： 基于艾宾浩斯遗忘曲线理论，根据时间衰减和记忆强度更新记忆，使 LLM 能够模拟人类的遗忘和记忆强化过程。</li></ul><h2 id="实验-4">实验</h2><p>设计了 194 个探索性问题来评估模型是否能够成功回忆起相关记忆并提供适当的回答。实验结果展示了SiliconFriend在记忆回忆、提供同理心陪伴和理解用户画像方面的能力。</p><h2 id="评价-4">评价</h2><p>Motivation很好，但是应用在了AI伴侣上，而且实验比较主观，都是基于人类打分。</p>]]></content>
    
    
    <summary type="html">论文阅读笔记 —— Agent相关</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="Agent" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/Agent/"/>
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="Agent" scheme="https://enchantedovo.cn/tags/Agent/"/>
    
  </entry>
  
  <entry>
    <title>文本生成模型的对抗攻击</title>
    <link href="https://enchantedovo.cn/2024/07/11/AI-Security1/"/>
    <id>https://enchantedovo.cn/2024/07/11/AI-Security1/</id>
    <published>2024-07-11T05:26:16.000Z</published>
    <updated>2024-07-11T08:32:48.043Z</updated>
    
    <content type="html"><![CDATA[<p>对抗攻击（也称为对抗样本生成）是智能算法安全的子领域，最初是针对图像所提出，在计算机视觉领域取得了丰硕的研究成果，提出了很多实用的攻击算法。近几年，研究人员在不断寻找新的应用场景，积极探索对抗攻击在其他领域的应用，针对文本的对抗攻击已取得一些进展。</p><h1 id="基本概念">基本概念</h1><h2 id="对抗样本">对抗样本</h2><p>对抗样本的概念最初是在2014年提出的，指的是一类人为构造的样本，通过对原始的样本数据添加针对性的微小扰动所得到，其不会影响人类的感知，但会使深度学习模型产生错误的判断[1]。对抗攻击即指构造对抗样本的过程。</p><p>举个例子:</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/11/AI-Security1/p1.png" class title="p1"><p>语句（1）为原始样本，语句（2）为经过几个字符变换后得到的对抗样本。深度学习模型能正确地将原始样本判为正面评论，而将对抗样本误判为负面评论。而显然，这种微小扰动并不会影响人类的判断。</p><p>关于对抗样本存在的原因，有学者认为是由于模型的<strong>高度非线性</strong>和<strong>过拟合</strong>，有学者认为是由于<strong>特征维度过高</strong>和<strong>模型的线性性质</strong>，至今还未达成共识，研究人员一般都会根据自己的研究成果来进行解释，每个人提出的观点往往仅适用于局部现象。但不管是线性解释还是非线性解释，究其<strong>本质是由于模型没有学到完美的判别规则，模型的判断边界与真实的决策边界不一致</strong>。深度学习模型由于能够自动学习特征的能力而得到广泛应用，但是这种由数据出发进行自主学习，所得到的特征并不一定就是我们所希望的特征，模型对数据的理解与人的理解有着很大的差异。因而模型学习到的特征，极有可能并非是人理解事物的特征，即对抗样本的存在是深度学习模型的固有缺陷。</p><h2 id="文本对抗和图像对抗">文本对抗和图像对抗</h2><p>文本数据与图像数据的不同，为文本领域的对抗攻击研究带来了巨大挑战。[2]</p><ol><li><strong>离散VS连续</strong>（Discrete VS Continucous）</li></ol><p>图像数据是连续的，易编码为数值向量，预处理操作线性、可微，通常使用lp范数来度量原始样本与对抗样本间的距离；而文本数据是符号化的数据，是<strong>离散</strong>的，预处理操作<strong>非线性、不可微</strong>，很难定义文本上的扰动及度量文本序列改变前后的差异。</p><ol start="2"><li><strong>易感知VS不易感知</strong>（Preceivable VS Unperceivable）</li></ol><p>人类通常不容易察觉到图像像素的微小变化，因此图像的对抗样本不会改变人类的判断力，只会影响深度学习模型的判别结果；而文本上的变化则很<strong>容易影响文本可读性</strong>，在将文本数据输入DNN模型之前通过拼写检查和语法检查来识别或纠正更改，极有可能导致攻击失败。</p><ol start="3"><li><strong>富有语义VS无语义</strong>（Semanic VS Semanic-less）</li></ol><p>像素的微小变化不会改变图像的语义，但对文本的扰动<strong>可轻易改变单词和句子的语义</strong>。例如，干扰单个像素不会将图像从猫变为另一种动物，而删除否定词将改变句子的情感。更改样本的语义有悖于对抗样本的定义，文本领域的对抗样本应在使深度学习模型发生误判的同时保持数据样本的真实标签不变。</p><h2 id="算法分类">算法分类</h2><h3 id="模型访问权限">模型访问权限</h3><p>根据模型访问权限可以分为白盒攻击和黑盒攻击，白盒攻击需要获取模型的结构和参数等详细信息；而黑盒攻击不需要模型知识，只需访问模型获取输入的对应输出即可。</p><h3 id="攻击目标">攻击目标</h3><p>根据攻击目标设定可以分为有目标攻击和无目标攻击，无目标攻击旨在使模型的输出为偏离正确结果的任意错误预测；而有目标攻击旨在使模型的输出为某一特定结果。</p><h2 id="添加扰动的粒度">添加扰动的粒度</h2><p>根据添加扰动时所操作的文本粒度可以分为字符级、单词级和语句级攻击。字符级攻击通过插入、删除或替换字符，以及交换字符顺序实现；单词级攻击主要通过替换单词实现，基于近义词、形近词、错误拼写等建立候选词库；语句级攻击主要通过文本复述或插入句子实现。</p><h2 id="攻击策略">攻击策略</h2><p>根据攻击策略可以分为<strong>Image-to-Text</strong>（借鉴图像领域的经典算法）、<strong>基于优化</strong>的攻击、<strong>基于重要性</strong>的攻击以及<strong>基于神经网络</strong>的攻击。</p><ul><li>部分学者通过<strong>将文本数据映射到连续空间</strong>，然后借鉴图像领域的一些经典算法如FGSM、JSMA等，生成对抗样本；</li><li>基于优化的攻击将对抗攻击表述为<strong>带约束的优化问题</strong>，利用现有的优化技术求解，如梯度优化、遗传算法优化；</li><li>基于重要性的攻击通常首先利用梯度或文本特性设计评分函数<strong>锁定关键词</strong>，然后通过<strong>文本编辑</strong>添加扰动；</li><li>基于神经网络的攻击训练神经网络模型<strong>自动学习对抗样本的特征</strong>，从而实现对抗样本的自动化生成。</li></ul><h1 id="代表性算法">代表性算法</h1><p>文本领域的常见任务有文本分类、情感分析、机器翻译、阅读理解、问答系统、对话生成、文本蕴含等，其中文本分类与情感分析任务使用分类器模型，其他任务使用seq2seq模型。针对分类任务的研究较多，下文介绍几种代表性算法。</p><table><thead><tr><th>算法</th><th>访问权限</th><th>攻击形式</th><th>操作粒度</th><th>策略</th><th>应用场景</th></tr></thead><tbody><tr><td>Papernot et al.</td><td>白盒</td><td>无目标攻击</td><td>单词</td><td>JSMA-based, FGSM-based</td><td>文本分类</td></tr><tr><td>TextFool</td><td>白盒/黑盒</td><td>有目标攻击</td><td>单词</td><td>FGSM-based</td><td>文本分类</td></tr><tr><td>HotFlip</td><td>白盒</td><td>无目标攻击</td><td>字符</td><td>Gradient-Optimization</td><td>文本分类</td></tr><tr><td>Alzantot et al.</td><td>黑盒</td><td>有目标攻击</td><td>单词</td><td>GA-Optimization</td><td>情感分析</td></tr><tr><td>DeepWordBug</td><td>黑盒</td><td>无目标攻击</td><td>字符</td><td>Importance-based</td><td>文本分类</td></tr><tr><td>TextBugger</td><td>白盒/黑盒</td><td>字符/单词</td><td>字符</td><td>Importance-based</td><td>情感分析</td></tr><tr><td>DISPTFLIP</td><td>白盒</td><td>无目标攻击</td><td>字符</td><td>Neural Network</td><td>文本分类</td></tr><tr><td>Zhao et al.</td><td>黑盒</td><td>无目标攻击</td><td>单词</td><td>GAN-based</td><td>文本分类/机器翻译</td></tr></tbody></table><p>Papernot等人[3]最先研究了文本领域的对抗样本问题，提出了<strong>生成对抗性输入序列</strong>的概念。作者将图像对抗领域的JSMA算法迁移到文本领域，利用计算图展开技术来评估与单词序列的嵌入输入有关的前向导数，构建雅可比矩阵，并借鉴<strong>FGSM</strong>的思想计算对抗性扰动。由于词向量不能取任意实数值，作者建立了一个特定的词典来选择单词以替换原始序列中的随机词。</p><p>Liang等人[4]提出了TextFool方法，首先针对白盒模型和黑盒模型使用不同的策略<strong>识别出对分类具有重要贡献的文本项</strong>（HTP、HSP），然后对这些重要的文本项通过单一或混合使用插入、修改和删除三种<strong>扰动</strong>策略，生成对抗样本。对于白盒模型，作者借鉴FGSM的思想来估算文本项的重要度，但是通过<strong>损失函数的梯度大小</strong>而不是梯度符号来度量；对于黑盒模型，通过<strong>遮挡文本</strong>的策略来识别重要文本项。</p><p>Ebrahimi 等人[5]提出了HotFlip方法，<strong>基于one-hot表示的梯度来有效估计单个操作所造成的最大损失的变化</strong>，通过原子翻转操作（将一个字符替换为另一个字符）生成对抗样本，并通过一系列的字符翻转来支持插入和删除操作。考虑到梯度优化的局限性，Alzantot等人[6]提出使用最优化技术中的遗传算法（Genetic Algorithm, GA）来生成与原始样本具有相似语义和语法的对抗样本。</p><p>Gao等人[7]提出了DeepWordBug方法，将对抗样本的生成分为两个阶段。首先使用针对文本数据特性设计的评分函数来<strong>识别关键的Token</strong>，根据重要性进行排名；然后对排名最高的m个Token通过简单的<strong>字符级操作</strong>（交换、替换、删除和插入）进行扰动，改变分类结果。</p><p>Li等人[8]提出了TextBugger方法，首先针对白盒和黑盒模型通过不同策略<strong>识别影响模型分类结果的重要词</strong>，然后采取插入、删除、字符交换、字符替换、单词替换等五种扰动策略分别生成扰动从中选择一个最优扰动。在白盒场景下，通过计算分类器的雅可比矩阵来找到重要词；在黑盒场景下，首先根据分类置信度找到重要的句子，然后使用评分函数来找到重要单词。</p><p>Gil 等人[9]提出了HotFlip的派生方法DISTFLIP，该算法<strong>提取HotFlip优化过程中的知识训练神经网络模型</strong>来模拟攻击从而生成对抗样本，极大地节省了运行时间，并可以迁移到黑盒场景下进行攻击。</p><p>Zhao等人[10]设计的用于生成对抗样本的模型，首先使用一个逆变器<strong>将原始数据映射到向量空间</strong>，在数据对应的<strong>稠密向量空间</strong>中进行搜索<strong>添加扰动</strong>得到对抗样本；然后使用GAN作为生成器将向量空间中得到的对抗样本<strong>映射回原始数据类型</strong>。</p><h1 id="参考文献">参考文献</h1><p>[1]Szegedy C, Zaremba W, Sutskever I, et al. Intriguing Properties of Neural Networks[C] // Proceedings of the 2th International Conference on Learning Representations, 2014.<br>[2]Zhang W E, Sheng Q Z, Alhazmi A, et al. Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey[J]. ACM Transactions on Intelligent Systems and Technology (TIST). 2020, 11(3): 1-41.<br>[3]Papernot N, McDaniel P, Swami A, et al. Crafting Adversarial Input Sequences for Recurrent Neural Networks[C]// Proceedings of MILCOM 2016-2016 IEEE Military Communications Conference. IEEE, 2016: 49-54.<br>[4]Liang B, Li H, Su M, et al. Deep Text Classification Can be Fooled[C]// Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence(IJCAI). 2018: 4208-4215.<br>[5]Ebrahimi J, Rao A, Lowd D, et al. HotFlip: White-Box Adversarial Examples for Text Classification[C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2018: 31-36.<br>[6]Alzantot M, Sharma Y, Elgohary A, et al. Generating Natural Language Adversarial Examples[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 2890-2896.<br>[7]Gao J, Lanchantin J, Soffa M L, et al. Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers[C]// Proceedings of 2018 IEEE Security and Privacy Workshops (SPW). IEEE, 2018: 50-56.<br>[8]Li J, Ji S, Du T, et al. TextBugger: Generating Adversarial Text Against Real-world Applications[C]// Proceedings of the 26th Annual Network and Distributed System Security Symposium. 2019.<br>[9]Gil Y, Chai Y, Gorodissky O, et al. White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019: 1373-1379.<br>[10]Zhao Z, Dua D, Singh S. Generating Natural Adversarial Examples[C]// Proceedings of the International Conference on Learning Representations. 2018.<br>参考博客: <a href="https://www.secrss.com/articles/25644">https://www.secrss.com/articles/25644</a></p><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/11/AI-Security1/p1.png" class="" title="p1"> --><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="p1.png"  style="zoom:60%;" /> -->]]></content>
    
    
    <summary type="html">智能算法安全笔记1 —— 文本对抗攻击概念</summary>
    
    
    
    <category term="智能算法安全" scheme="https://enchantedovo.cn/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8/"/>
    
    <category term="对抗攻击" scheme="https://enchantedovo.cn/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"/>
    
    
    <category term="智能算法安全" scheme="https://enchantedovo.cn/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8/"/>
    
    <category term="生成式人工智能" scheme="https://enchantedovo.cn/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>大模型的对抗攻击与防御</title>
    <link href="https://enchantedovo.cn/2024/07/10/LLM-Security1/"/>
    <id>https://enchantedovo.cn/2024/07/10/LLM-Security1/</id>
    <published>2024-07-10T10:50:49.000Z</published>
    <updated>2024-11-14T03:11:20.304Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文：OpenAI 安全系统（Safety Systems）团队负责人 Lilian Weng 发布的一篇博客文章 <a href="https://lilianweng.github.io/lil-log/2023/11/07/adversarial-attacks-on-llms.html">《Adversarial Attacks on LLMs》</a></p></blockquote><p>随着 ChatGPT 的发布，大型语言模型应用正在加速大范围铺开。OpenAI 的安全系统团队已经投入了大量资源，研究如何在对齐过程中为模型构建默认的安全行为。但是，对抗攻击或 prompt 越狱依然有可能让模型输出我们不期望看到的内容。</p><p>目前在对抗攻击方面的研究很多集中在图像方面，也就是在连续的高维空间。而对于文本这样的离散数据，由于缺乏梯度信号，人们普遍认为攻击会困难得多。Lilian Weng 之前曾写过一篇文章<a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/">《Controllable Text Generation》</a>探讨过这一主题。简单来说：攻击 LLM 本质上就是控制该模型输出特定类项的（不安全）内容。</p><p>此外，还有研究者试图通过攻击大语言模型来提取其预训练数据、私有信息或者通过数据毒化手段来干扰模型的训练过程。但这些并非本文要探讨的主题。</p><h1 id="基础">基础</h1><h2 id="威胁模型">威胁模型</h2><p>对抗攻击是<strong>对输入样本进行微小难以察觉的修改，诱使模型输出我们不期望的内容</strong>。许多早期研究关注的重点是分类任务，而近期的工作则开始更多关注生成模型的输出。本文探讨的是大型语言模型，并且假定攻击<strong>仅发生在推理阶段</strong>，也就是说模型权重是固定的。</p><!-- <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p1.png" class="" title="p1"> --><blockquote><p>对抗样本的定义:对抗样本是指能够欺骗模型，使其做出错误预测的输入样本，但与原始样本在语义上仍然相似，不易被人察觉。</p></blockquote><h3 id="分类">分类</h3><p>在过去，研究者更关注的是对分类器进行对抗攻击，并且许多是在图像领域。LLM 也可被用于分类，给定一个输入 $x$ 和一个分类器 $f(.)$，我们希望找到该输入的一个差异细微的对抗版本 $x_{adv}$，使得 $f(x_{adv})≠f(x)$。</p><h3 id="文本生成">文本生成</h3><p>给定一个输入$x$和一个生成模型 $p(.)$，该模型可输出一个样本 $y \sim p(.|x)$。这里的对抗攻击是找到一个$p(x)$，使得 $y$ 会违反该模型内置的安全行为，比如输出非法主题的不安全内容、泄漏隐私信息或模型训练数据。对<strong>生成任务</strong>而言，判断一次攻击成功与否并非易事，这<strong>需要一个超高质量的分类器来判断 $y$ 是否安全或需要人类来进行审查</strong>。</p><h3 id="白盒攻击和黑盒攻击">白盒攻击和黑盒攻击</h3><ul><li><p><strong>白盒攻击</strong>（White-box attacks）假设攻击者可以<strong>完全访问</strong>模型权重、架构和训练工作流程，这样一来攻击者就可以获得<strong>梯度信号</strong>。这里我们并不假设攻击者能获得全部训练数据。这仅适用于开源模型。</p></li><li><p><strong>黑盒攻击</strong>（Black-box attacks）则是假设攻击者只能访问 API 类型的服务 —— 攻击者可以提供输入 $x$ 并获取反馈的样本 $y$，而不知道有关模型的更多信息。</p></li></ul><h1 id="对抗性攻击的种类">对抗性攻击的种类</h1><p>为了让大语言模型 (LLMs) 做出错误的输出，研究人员发明了多种对抗性输入方法。这里，我们介绍五种主要策略。</p><table><thead><tr><th>攻击方式</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>Token操作</td><td>黑盒</td><td>微调输入文本的少量 Token，引发模型失效，同时保留原文的含义。</td></tr><tr><td>基于梯度的攻击</td><td>白盒</td><td>利用梯度信息来制定出有效的攻击策略。</td></tr><tr><td>越狱式提示</td><td>黑盒</td><td>常用一些基于直觉的提示来绕过模型内建的安全机制。</td></tr><tr><td>人类参与的红队策略</td><td>黑盒</td><td>人工对模型进行攻击，可能会借助其他模型的协助。</td></tr><tr><td>模型红队攻击</td><td>黑盒</td><td>一个模型对另一个模型进行攻击，攻击者模型可以根据需要进行调整。</td></tr></tbody></table><h2 id="Token操作">Token操作</h2><p>给定一段包含一个 token 序列的文本输入，我们可以使用<strong>简单的token操作</strong>（比如<strong>替换成同义词</strong>）来诱使模型给出错误预测。</p><blockquote><p>想象一下，如果我们有一段文本，里面充满了各种Token（就是文字或词汇），我们可以对这些Token进行一些小把戏，比如换个同义词，就能让智能模型搞不清楚状况，做出错误的判断。这种在所谓的黑盒环境下的“Token 操纵术”是可行的。</p></blockquote><p>基于 token 操作的攻击属于黑盒攻击。在 Python 框架中，Morris et al. 2020 的论文《TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP》实现了许多词和 token 操作攻击方法，可用于为 NLP 模型创建对抗样本。这一领域的许多实验主要针对分类和文本蕴含。</p><p>举个例子，Ribeiro et al (2018) 的研究《Semantically Equivalent Adversarial Rules for Debugging NLP models》依赖于人工提出的「语义等价式对抗规则（SEAR）」，其可以通过尽可能少的 token 操作来让模型无法生成正确答案。比如，其中的规则包括将 What 换成 Which、将 was 换为 is。另外，还有其他研究者提出的替换关键词、用同义词替换等方法。</p><h2 id="基于梯度的攻击">基于梯度的攻击</h2><p>如果是白盒攻击，则攻击者可以获取所有的模型参数和架构。因此，攻击者就可以<strong>依靠梯度下降</strong>来通过编程方式学习最有效的攻击手段。这种基于梯度的攻击手法，<strong>只适用于可以完全访问内部结构的白盒环境</strong>，比如开源 LLM。</p><h3 id="GBDA">GBDA</h3><p>Guo et al. 2021 的论文《Gradient-based Adversarial Attacks against Text Transformers》提出的基于梯度的分布式攻击（GBDA）使用了 Gumbel-Softmax 近似法,来使对抗损失优化可微。在这种方法中，BERTScore 和困惑度（perplexity）用来衡量生成的文本既关联又流畅。然而，Gumbel-softmax 技巧难以扩展用于 token 删除或增添，它受限于 token 替换操作。</p><ul><li><p><strong>核心思想</strong>：<strong>寻找一个对抗分布</strong>，而不是单个对抗样本。这个对抗分布参数化地表示一系列可能的对抗样本，并使用连续的矩阵$\theta$进行控制。通过优化$\theta$，我们可以调整对抗分布，使其包含更多能够欺骗目标模型 h 的样本。</p></li><li><p><strong>优化过程</strong>：</p><ul><li>GBDA 使用 Gumbel-softmax 技术将对抗分布的采样过程变得可微分，从而可以使用梯度下降等优化算法进行优化。</li><li>优化过程中，目标是最小化一个包含对抗损失、流畅性约束和语义相似性约束的损失函数。</li><li>对抗损失鼓励模型对样本做出错误预测。</li><li>流畅性约束使用语言模型来评估样本的流畅程度，并鼓励生成更自然的文本。</li><li>语义相似性约束使用 BERTScore 来评估样本与原始样本在语义上的相似程度，并鼓励生成语义上更接近原始样本的对抗样本。</li></ul></li><li><p><strong>生成对抗样本</strong>：优化完成后，从对抗分布 PΘ 中采样即可得到对抗样本。由于对抗分布中包含多种可能的对抗样本，我们可以从中选择最适合当前任务和目标模型的样本。</p></li></ul><h3 id="HotFlip">HotFlip</h3><p>Ebrahimi et al. 2018 在论文《HotFlip: White-Box Adversarial Examples for Text Classification》 中则是<strong>将文本操作看作是向量空间中的输入</strong>，度量的是损失在这些向量上的导数。基于one-hot表示的梯度来有效估计单个操作所造成的最大损失的变化，通过原子翻转操作（将一个字符替换为另一个字符）生成对抗样本，并通过一系列的字符翻转来支持插入和删除操作。HotFlip 可以扩展用于 token 删除或增添。</p><h3 id="UTA">UTA</h3><p>Wallace et al. (2019) 的论文《Universal Adversarial Triggers for Attacking and Analyzing NLP》提出了一种<strong>在 token 上进行梯度引导式搜索</strong>的方法，可以找到诱使模型输出特定预测结果的短序列，这个短序列被称为 Universal Adversarial Triggers （UAT，通用对抗触发器）。UAT 不受输入的影响，这意味着这些触发器可以<strong>作为前缀（或后缀）连接</strong>到来自数据集的任意输入上。</p><p>上面的 token 搜索方法可以使用<strong>波束搜索增强</strong>。当寻找最优的 token 嵌入时，可以选取 top-k 个候选项，而不是单独一个，在当前数据批上从左到右搜索，并根据 $L_{adv}$ 为每个波束评分。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p1.png" style="zoom:45%;"><p>UAT 的损失 $L_adv$  需要针对具体任务而设计。分类或阅读理解依赖于交叉熵。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p3.png" style="zoom:45%;"><p>UAT 为何有效？这是一个很有趣的问题。因为 UAT 是与输入无关的，并且可以在有不同嵌入、token 化方案和架构的模型之间迁移，因此它们也许可以有效地利用训练数据中的偏差，毕竟这种偏差已经融入到了模型的全局行为中。</p><p>使用 UAT 攻击有一个缺点：<strong>很容易检测出来</strong>。原因是所学习到的触发器往往是毫无意义的。Mehrabi et al. (2022) 在论文《Robust Conversational Agents against Imperceptible Toxicity Triggers》中研究了 UAT 的两种变体，它们会促使所学到的触发器在多轮对话语境中难以察觉。其目标是创建能在给定对话中有效触发有毒响应的攻击性消息，同时保证该攻击在对话中是流畅、连贯和一致的。</p><h3 id="AutoPrompt">AutoPrompt</h3><p>Shin et al., 2020 的《AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts》使用了同样的<strong>基于梯度的搜索策略</strong>来为多样化的任务寻找最有效的 <strong>prompt</strong> 模板。</p><h3 id="通用对抗触发-token-作为后缀连接到输入">通用对抗触发 token 作为后缀连接到输入</h3><p>Zou et al. (2023) 的论文《Robust Conversational Agents against Imperceptible Toxicity Triggers》也研究了将通用对抗触发 token 作为后缀连接到输入请求上的情况。他们具体研究了对 LLM 的恶意请求 —— 对此模型应当拒绝回答。事实上，拒绝不被允许的内容类别（比如犯罪建议）是 GPT-4 内置的一个重要的安全措施。对抗性的目标是，<strong>即使面对应该被拒绝的请求，也能操纵语言模型给出 肯定的回应</strong>。例如，面对一个恶意请求，模型可能会回答说：“Sure, here is how to …”。为了避免仅仅通过改变话题来引出一个 “sure” 回答，预期中的肯定回应也被设定为会重复用户的部分提示。而计算这种回应的可能性时使用的损失函数非常简单，就是目标回应的负对数似然 (NLL)。</p><h3 id="GCG">GCG</h3><p>为了在多个输入上引发模型给出肯定回应，研究人员在两个不同的模型 Vicuna-7b 和 Vicuna-13b 上进行了实验。他们使用了一种称为贪婪坐标梯度 (GCG) 的搜索技术，来找到能在所有可能的单个 Token 替换中减少损失最多的候选 Token。鉴于实际上不可能评估所有 Token 的替换，研究者采用了一种基于梯度的 Token 搜索策略，这种策略与 UAT 和 AutoPrompt 相似，可以<strong>为每个 Token 找出能够最大化减少损失函数梯度的最佳候选 Token</strong>。</p><p>尽管他们的攻击序列完全是基于开源模型训练的，但它们却出乎意料地可以移植用于其它商用模型，这表明对开源模型的白盒攻击对私有模型也有效，尤其是当低层的训练数据有所重叠时。注意 Vicuna 的训练使用了从 GPT-3.5-turbo 收集的数据（通过 shareGPT），这本质上是蒸馏，因此这种攻击更像是白盒攻击。</p><h3 id="ARCA">ARCA</h3><p>Jones et al. 2023 等提出的自回归随机坐标上升（ARCA）,ARCA 考虑了一系列更广泛的优化问题，用以寻找符合某种特定行为模式的输入输出对 $(x,y)$；例如，输入是以 “Barack Obama” 开头的无害内容，但可能产生有害的输出。</p><h2 id="越狱式提示">越狱式提示</h2><p>Jailbreak 提示技术是一种利用弱点强制大语言模型（LLMs）产出那些本应被过滤的有害内容。这种技术属于黑盒攻击，其措辞组合的选取是基于经验法则和手动尝试。</p><p>Wei et al. (2023) 的论文《Jailbroken: How Does LLM Safety Training Fail?》提出了 LLM 安全的两种失败模式，可用于指导越狱攻击的设计:</p><ol><li><p><strong>目标冲突</strong>：当一个模型在执行能力（比如“必须严格执行指令”）和安全目标之间出现矛盾时，我们称之为目标冲突。举例来说，有些人可能会利用这种冲突来实施所谓的“越狱攻击”：</p><ul><li>前缀引导：让模型以肯定答复开始对话。</li><li>拒绝抑制：详细指示模型避免使用拒绝的表达方式。</li><li>风格限制：要求模型不使用复杂词汇，这样它就无法使用专业术语或详细解释来拒绝某些请求。</li><li>角色扮演：例如，扮演 DAN（立即行动），AIM（总是表现出高智能和机智）等角色。</li></ul></li><li><p><strong>泛化失配</strong>：安全训练未能覆盖到模型实际具备能力的领域。这种情况通常发生在模型的安全训练数据未涉及到的领域，但这些输入却在它广泛的预训练资料库中。比如：</p><ul><li>特殊编码：使用 Base64 编码来构建对抗性输入。</li><li>字符变换：ROT13 密码、火星文或脑残体（用视觉上相似的数字和符号替换字母）、摩尔斯电码。</li><li>词变换：Pig Latin（用同义词替换敏感词，比如用「窃」替换「偷」）、拆分关键词（即所谓的 token smuggling，也就是把敏感词分成几部分）。</li><li>prompt 层面的混淆：翻译成其它语言、要求模型以其能理解的方式进行混淆。</li></ul></li></ol><p>Wei et al. (2023) 实验了大量越狱方法，包括由以上原理构建的组合型策略。</p><ul><li>combination_1 组合了前缀引导、拒绝抑制和 Base64 攻击。</li><li>combination_2 加入了风格限制。</li><li>combination_3 又添加了生成网站内容和格式化限制条件。</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p4.png" style="zoom:35%;"><p>Greshake et al. (2023) 的论文《Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection》则在较高层面上观察了 prompt 注入攻击。其中指出，即便当攻击无法提供详细的方法而仅仅提供一个目标时，模型也有可能自动去实现它。当模型可以访问外部 API 和工具时，对更多信息（甚至是专有信息）的获取可能导致更大的钓鱼攻击和私密窥探攻击风险。</p><h2 id="有人类参与的红队策略">有人类参与的红队策略</h2><p>由 Wallace 等人 (2019) 提出的“人类参与的敌对生成”策略，旨在<strong>开发工具辅助人类“智破”人工智能模型</strong>。在他们的研究中，以 QuizBowl QA 数据集 为基础，他们设计了一套敌对性写作界面。这个界面可以引导用户编写类似《危险边缘》节目风格的问题，目的是让人工智能模型产生错误的判断。系统会根据每个词的重要度来进行颜色高亮显示，这个重要度是通过移除该词后模型预测概率的变化来确定的，具体是通过计算模型对词嵌入的梯度来近似得到的。<br><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p5.png" style="zoom:45%;"><br>界面由（左上角）模型的前五个预测和（右下角）根据词语重要性高亮显示的用户问题组成。</p><p>在 Ziegler 等人 (2022) 的一个实验中，研究人员指导人类参与者寻找针对暴力内容的安全分类器的潜在失败点。他们开发了一种工具，帮助参与者更快速、更有效地发现并解决分类器的漏洞。使用工具辅助的改写比全手工改写更高效，把处理每个案例的时间从 20 分钟缩短至 13 分钟。具体来说，他们为人类作者引入了两种辅助特性：</p><ul><li>特征 1: <strong>每个词语的显著性分数展示</strong>。工具界面会突出那些一旦被移除就可能影响分类器判定结果的词语。一个词语的显著性分数是根据这个词语的嵌入对分类器输出的梯度幅值来确定的，这和 Wallace 等人 (2019) 的做法是一样的。</li><li>特征 2: <strong>词语替换与添加</strong>。这项功能让用户通过 BERT-Attack 轻松进行词语的操作。接着，由人类作者对词语更新进行复核。只要点击文本片段中的某个词语，就会弹出一个下拉菜单，里面按照能够降低当前模型分数的程度，列出了新的词语选项。目的是减少模型预测这些输入为暴力内容的可能性。</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p6.png" style="zoom:35%;"><p>Xu et al. 2021 的《Bot-Adversarial Dialogue for Safe Conversational Agents》提出了 Bot-Adversarial Dialogue（BAD），该框架可以引导人类去诱使模型犯错（比如输出不安全的内容）。他们收集了 5000 多组模型与众包工作者的对话。每一组对话都包含 14 轮，然后他们根据不安全对话轮次的数量给模型打分。他们最终得到了 BAD 数据集，其中包含大约 2500 组带有攻击性标签的对话。</p><p>Anthropic 的红队数据集包含接近 4 万个对抗攻击，它们收集自人类红队者与 LLM 的对话。他们发现，RLHF 的规模越大，就越难以攻击。OpenAI 发布的大模型（比如 GPT-4 和 DALL-E 3）普遍使用了人类专家红队，以确保系统的安全性。</p><h2 id="模型红队攻击">模型红队攻击</h2><p>人类红队策略很强大，但是难以大规模实施而且可能需要大量经过培训的专业人士。现在想象一下：我们可以学习一个红队模型 red 来与目标 LLM 进行对抗，以触发其给出不安全响应。对于基于模型的红队策略，主要的难题是<strong>如何判断一次攻击是否成功</strong>；只有知道了这一点，我们才能构建用于训练红队模型的合适学习信号。</p><p>假设我们已经有一个高质量的分类器，能够判断模型的输出是否有害，我们就可以将其用作奖励来训练红队模型，以得到一些能最大化分类器在目标模型输出上的分数的输入。令 $r(x, y)$ 是一个这样的红队分类器，其可以判断在给定测试输入$x$时，输出 $y$ 是否有害(基于Perez et al. 2022 的论文《Red Teaming Language Models with Language Models》)。</p><p>Casper et al. (2023) 的论文《Explore, Establish, Exploit: Red Teaming Language Models from Scratch》设计了一种有人类参与的红队过程。其与 Perez et al. (2022) 的主要不同之处在于其明确地为目标模型设置了一个数据采样阶段，这样就可以收集其上的人类标签来训练针对特定任务的红队分类器。其包含探索（Explore）、建立（Establish）和利用（Exploit）三个阶段。使用强化学习培养一个对抗性的提示生成器，这个生成器能够触发一系列多样化的有害输出。</p><p>Mehrabi et al. (2023) 的论文《FLIRT: Feedback Loop In-context Red Teaming》则是依靠红队 LM $p_red$ 的上下文学习来攻击图像或文本生成模型 $p$，使其输出不安全的内容。红队语言模型 $p_{red}$ 创造了一个根据示例定制的对抗性提示 $ x \sim {p_{red}{(.∣{examples})}}$ 这些初步的示例是由人工仔细打造的；生成模型$p$会以这个提示为条件，输出图像或文字 $y \sim p(.∣x)$。然后，生成的作品 $y$ 会被检测，比如使用分类器来判定其是否安全；如果结果是不安全的，那么这个触发提示 $x$ 就会用来更新上下文的案例，从而让 $p_{red}$ 按照某种策略制作出新的对抗性提示。</p><h1 id="如何应对攻击">如何应对攻击</h1><h2 id="鞍点问题">鞍点问题</h2><p>Madry et al. 2017 的《Towards Deep Learning Models Resistant to Adversarial Attacks》提出了一个很不错的对抗稳健性（adversarial robustness）框架，即将对抗稳健性建模成一个鞍点问题，这样就变成了一个稳健优化（robust optimization）问题。该框架是为分类任务的连续输入而提出的，但它用相当简洁的数学公式描述了双层优化过程。其目标由一个<strong>内部最大化</strong>问题和一个<strong>外部最小化</strong>问题组成：</p><ul><li>内部最大化：寻找能导致高损失的最有效的对抗数据点 𝐱+𝜹。所有对抗性攻击方法最终都可归结为如何最大化这个内部过程的损失。</li><li>外部最小化：寻找最佳的模型参数化方案，使得由内部最大化过程找到的最有效攻击的损失能被最小化。要训练出稳健的模型，一个简单方法是将每个数据点替换为其扰动版本，这些版本可以是一个数据点的多个对抗变体。<br><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/07/10/LLM-Security1/p7.png" style="zoom:45%;"></li></ul><h2 id="LLM-稳健性">LLM 稳健性</h2><p>这里简单谈谈一些有关 LLM 稳健性的研究。</p><p>Xie et al. 2023 的论文《Defending ChatGPT against Jailbreak Attack via Self-Reminder》发现了一种简单直观的保护模型免受对抗攻击的方法：<strong>明确地指示模型成为负责任的模型，不要生成有害内</strong>容。这会极大降低越狱攻击的成功率，但对模型的生成质量会有副作用，这是因为这样的指示<strong>会让模型变得保守</strong>（比如不利于创意写作），或者会在某些情况下错误地解读指令（比如在安全 - 不安全分类时）。</p><p>目前降低恶意攻击风险最常见的方法是<strong>对抗性训练</strong>，即在攻击样本上训练模型。这种方法被认为是最有效的防御手段，但它需要在模型稳健性和性能之间做出权衡。在 Jain 等人 2023 的《Baseline Defenses for Adversarial Attacks Against Aligned Language Models》中，他们测试了两种对抗性训练方案：（1）将有害提示与 “I’m sorry. As a …” 的回应配对进行梯度下降；（2）在每个训练步骤中，对拒绝回应进行一次梯度下降，而对“红队”的不当回应进行一次梯度上升。结果显示方案（2）几乎无效，因为它严重降低了模型生成的质量，而攻击成功率却仅有少量下降。</p><p>白盒攻击通常会产生毫无意义的恶意提示，这些提示可以通过<strong>检查困惑度</strong>来识别。当然，白盒攻击可以通过专门优化以降低困惑度来规避这一检测，比如 UAT-LM，它是 UAT 的变体。不过，这种方法也有其权衡，可能会降低攻击的成功率。</p><p>Jain et al. 2023 还测试了对文本输入进行预处理的方法，使得能在移除对抗性修改的同时维持语义含义。</p><ul><li>解释含义：使用 LLM 来解释输入文本的含义，这可能会对下游任务性能造成较小影响。</li><li>重新 token 化：将 token 拆分开并使用多个更小的 token 来表示它们，比如使用 BPE-dropout（随机丢弃一定比例的 token）。使用这种方法的假设是对抗性 prompt 很可能会利用特定的对抗性 token 组合。这也确实有助于降低攻击成功率，但也有限，比如从 90% 以上降至 40%。</li></ul>]]></content>
    
    
    <summary type="html">大模型安全笔记1 —— 基础概念</summary>
    
    
    
    <category term="智能算法安全" scheme="https://enchantedovo.cn/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8/"/>
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="大模型&amp;安全" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%AE%89%E5%85%A8/"/>
    
    <category term="大模型对抗攻击" scheme="https://enchantedovo.cn/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"/>
    
    
    <category term="智能算法安全" scheme="https://enchantedovo.cn/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8/"/>
    
    <category term="生成式人工智能" scheme="https://enchantedovo.cn/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>RAG开发的痛点以及解决方案</title>
    <link href="https://enchantedovo.cn/2024/04/10/LLM-Learning2/"/>
    <id>https://enchantedovo.cn/2024/04/10/LLM-Learning2/</id>
    <published>2024-04-10T03:43:46.000Z</published>
    <updated>2024-04-10T15:03:05.743Z</updated>
    
    <content type="html"><![CDATA[<div class="hide-toggle" style="border: 1px solid bg"><div class="hide-button toggle-title" style="background-color: bg;color: color"><i class="fas fa-caret-right fa-fw"></i><span>参考</span></div>    <div class="hide-content"><ul><li><a href="https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c">12 RAG Pain Points and Proposed Solutions</a></li><li><a href="https://baoyu.io/translations/rag/12-rag-pain-points-and-proposed-solutions">https://baoyu.io/translations/rag/12-rag-pain-points-and-proposed-solutions</a></li></ul></div></div><h1 id="引言">引言</h1><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/04/10/LLM-Learning2/p1.webp" class title="p1"><p>图源自 Barnett 等人的研究 <a href="https://baoyu.io/translations/ai-paper/2401.05856v1-seven-failure-points-when-engineering-a-retrieval-augmented-generation-system">工程化检索增强生成系统时的七大挑战</a>。后面会介绍上述7大挑战，并且补充5个额外的挑战，以及它们的解决方案：</p><ul><li>痛点 1：缺失内容</li><li>痛点 2：关键文档被遗漏</li><li>痛点 3：文档整合的长度限制 —— 超出上下文</li><li>痛点 4：提取困难</li><li>痛点 5：格式错误</li><li>痛点 6：缺乏具体细节</li><li>痛点 7：回答不全面</li><li>痛点 8：数据摄入的可扩展性问题</li><li>痛点 9：结构化数据的问答</li><li>痛点 10：从复杂 PDF 文档提取数据</li><li>痛点 11：备用模型策略</li><li>痛点 12：大语言模型的安全性</li></ul><h1 id="痛点1：缺失内容">痛点1：缺失内容</h1><p><strong>具体描述</strong>：当提出的问题无法从可用文档中找到答案时，RAG系统可能会回答&quot;对不起，我不知道”，但如果问题与内容相关却没有答案，系统可能会被误导提供错误的回答。</p><p><strong>解决方案</strong>：</p><ul><li><p>数据清洗，构建高质量的数据</p></li><li><p>prompt优化</p><ul><li>例如，通过设置提示：“如果你对答案不确定，就告诉我你不知道”，可以鼓励模型承认其局限性，并更透明地表达不确定性。虽然无法保证百分百的准确率，但在数据清洗之后，设计恰当的提示是提高回答质量的有效手段之一。</li></ul></li></ul><h1 id="痛点2：关键文档被遗漏">痛点2：关键文档被遗漏</h1><p><strong>具体描述</strong>：问题的答案在文档中，但排名没有足够高以返回给用户。（理论上所有文档都会被排名并用于下一步，但实际上只返回排名最高的K个文档）。</p><p><strong>解决方案</strong>：</p><ul><li><p>通过调整<code>chunk_size</code>和<code>similarity_top_k</code>两个参数优化检索效果</p><ul><li><a href="https://medium.com/gitconnected/automating-hyperparameter-tuning-with-llamaindex-72fdd68e3b90?sk=0a29f2025b965040b9b1defd9525e4eb">通过 LlamaIndex 实现超参数自动调整中</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/param_optimizer/param_optimizer.html">对 RAG 进行超参数优化</a></li></ul></li><li><p>检索结果的优化排序（Rerank算法）</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html">优化排序的前后差异</a></li><li>另外，通过各种嵌入技术和排序器，可以对检索性能进行评估和提升，详见<a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83">提升 RAG 性能：挑选最优的嵌入技术和排序模型</a>，由 Ravi Theja 撰写。</li><li>定制化的排序器经过微调后能够实现更优的检索性能，具体实施方法请参阅<a href="https://blog.llamaindex.ai/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b">通过微调 Cohere 排序器与 LlamaIndex 提升检索效果</a></li></ul></li></ul><h1 id="痛点3：文档整合的长度限制一超出上下文">痛点3：文档整合的长度限制一超出上下文</h1><p><strong>具体描述</strong>：数据库检索到包含答案的文档，但这些文档没有被包含在生成答案的上下文中。当数据库返回许多文档时，需要进行整合处理以检索答案。</p><p><strong>解决方案</strong>：</p><ul><li><p>调整检索策略：比如：基础检索、知识图谱检索等</p></li><li><p>微调嵌入技术：如果使用的是开源嵌入模型，对其进行微调是提高检索准确度的有效手段。<a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html">嵌入模型微调指南</a></p></li></ul><h1 id="痛点4：提取困难">痛点4：提取困难</h1><p><strong>具体描述</strong>：答案存在于上下文中，但大型语言模型未能提取出正确的答案。这通常发生在上下文中存在太多噪声或矛盾信息时。</p><p><strong>解决方案</strong>：</p><ul><li><p>数据清洗</p></li><li><p>提示信息压缩，可以参考：<a href="https://rzv2chzpkh.feishu.cn/wiki/FIMJwPPyGiKJmAkuhJGcBrAonKe#CZQ6dQ58HotZswxkNTrcqZiwnGf">LongLLMLingua.pdf</a></p></li><li><p>LongContextReorder（长内容优先排序）：<a href="https://arxiv.org/abs/2307.03172">研究</a>发现，当关键数据被放置在输入内容的开始或结尾时，往往能够获得最佳的性能表现。为了解决信息在输入中间部分“迷失”的问题，LongContextReorder 应运而生，它通过重新排序检索到的节点来优化处理，特别适用于需要处理大量顶级结果的情形。</p></li></ul><h1 id="痛点5：格式错误">痛点5：格式错误</h1><p><strong>具体描述</strong>：问题涉及以特定格式（如表格或列表）提取信息，但大型语言模型忽略了指令。</p><p><strong>解决方案</strong>：</p><ul><li><p>prompt优化</p></li><li><p>输出解析：可以在以下方面帮助确保获得期望的输出：为任何提示/查询提供格式化指令；对大语言模型的输出进行“解析”</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program.html">Pydantic程序</a></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.program <span class="keyword">import</span> OpenAIPydanticProgram</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define output schema (without docstring)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Song</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    title: <span class="built_in">str</span></span><br><span class="line">    length_seconds: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Album</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    artist: <span class="built_in">str</span></span><br><span class="line">    songs: List[Song]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define openai pydantic program</span></span><br><span class="line">prompt_template_str = <span class="string">&quot;&quot;&quot;\</span></span><br><span class="line"><span class="string">Generate an example album, with an artist and a list of songs. \</span></span><br><span class="line"><span class="string">Using the movie &#123;movie_name&#125; as inspiration.\</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">program = OpenAIPydanticProgram.from_defaults(</span><br><span class="line">    output_cls=Album, prompt_template_str=prompt_template_str, verbose=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run program to get structured output</span></span><br><span class="line">output = program(</span><br><span class="line">    movie_name=<span class="string">&quot;The Shining&quot;</span>, description=<span class="string">&quot;Data model for an album.&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="痛点6：缺乏具体细节">痛点6：缺乏具体细节</h1><p><strong>具体描述</strong>：回答在响应中返回，但不够具体或过于具体，无法满足用户的需求。当RAG系统设计者对给定问题有预期结果时（例如针对学生的教学内容），就会发生这种情况。</p><p><strong>解决方案</strong>：</p><ul><li>进阶检索技巧：从小到大的信息聚合检索、递归式检索方法<ul><li>参考文章：<a href="https://towardsdatascience.com/jump-start-your-rag-pipelines-with%E9%AB%98%E7%BA%A7%E6%A3%80%E7%B4%A2-llamapacks-%E5%92%8C-lighthouz-ai-80a09b7c7d9d?sk=14e50a68f9ef825aaa6634365c7d9617">如何通过高级检索 LlamaPacks 优化您的 RAG 流程，并利用 Lighthouz AI 进行性能基准测试</a></li></ul></li></ul><h1 id="痛点7：回答不全面">痛点7：回答不全面</h1><p><strong>具体描述</strong>：回答不完整并不是错误的，但遗漏了一些信息，尽管这些信息在上下文中可用并且可以提取。例如，对于&quot;这些文件A、B和C的关键点是什么？&quot;这样的问题，更好的方法是分别询问这些问题。</p><p><strong>解决方案</strong>：</p><ul><li><p>查询变换的技巧：一个有效提升凡AG处理能力的策略是增设一个查询理解层，即在实际检索知识库之前进行一系列的查询变换。具体来说，我们有以下四种变换方式：</p><ul><li>路由：在不改变原始查询的基础上，识别并定向到相关的工具子集，并将这些工具确定为处理该查询的首选。</li><li>查询改写：在保留选定工具的同时，通过多种方式重构查询语句，以便跨相同的工具集进行应用。</li><li>细分问题：将原查询拆解为若干个更小的问题，每个问题都针对特定的工具进行定向，这些工具是根据它们的元数据来选定的。</li><li>ReAct 代理工具选择：根据原始查询判断最适用的工具，并为在该工具上运行而特别构造的查询。</li></ul></li><li><p>参考文章：<a href="https://towardsdatascience.com/advanced-query-transformations-to-improve-rag-11adca9b19d1">提升 RAG 性能的高级查询变换技巧</a></p></li></ul><h1 id="痛点8：数据摄入的可扩展性问题">痛点8：数据摄入的可扩展性问题</h1><p><strong>具体描述</strong>：在RAG系统中，数据摄入的可扩展性问题指的是系统在有效管理和处理大规模数据时面临的挑战，这可能导致性能瓶颈甚至系统故障。这类问题可能会造成数据摄入时间过长、系统过载、数据质量下降以及可用性受限等现象。</p><p><strong>解决方案</strong>：</p><ul><li>加速文档处理：并行摄取技术，参考：LlamaIndex 提供的<a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb?__s=db5ef5gllwa79ba7a4r2&amp;utm_source=drip&amp;utm_medium=email&amp;utm_campaign=LlamaIndex+news%2C+2024-01-16">详细教程</a>。</li></ul><h1 id="痛点9：结构化数据的问答">痛点9：结构化数据的问答</h1><p><strong>具体描述</strong>：解读用户的查询并准确检索到所需的结构化数据是一项挑战，尤其是当面对复杂或模糊的查询请求时。这一挑战由于文本到SQL的转换不够灵活，以及当前大语言模型在处理这些任务上的局限性而更加复杂。</p><p><strong>解决方案</strong>：</p><ul><li><p>链式思维表格包：适合处理包含多重信息的复杂表格单元问题，通过逐步精确地切割和解析数据，直至找到所需的数据子集，显著提高了对表格数据的查询效率，参考：<a href="https://arxiv.org/abs/2401.04398">链式表格原论文</a></p></li><li><p>混合自洽（多数投票法）查询引擎包：原文见<a href="https://arxiv.org/pdf/2312.16702v1.pdf">重新思考大语言模型如何理解表格数据</a></p></li></ul><h1 id="痛点10：从复杂PDF中提取数据">痛点10：从复杂PDF中提取数据</h1><p><strong>具体描述</strong>：在从复杂PDF文档，如嵌入表格的文档中提取数据进行问答时，传统的检索方法往往无法达到目的。需要一个更高效的方法来处理这种复杂的PDF数据提取需求。</p><p><strong>解决方案</strong>：</p><ul><li>嵌入式表格检索技术：LlamaIndex 提出了一种解决方案——`EmbeddedTablesUnstructuredRetrieverPack</li></ul><h1 id="痛点11：备用模型策略">痛点11：备用模型策略</h1><p><strong>具体描述</strong>：在使用大语言模型过程中，可能会担心模型可能会遇到问题，比如遇到OpenAI模型的访问频率限制错误。这时候就需要一个或多个备用模型作为后备，以防主模型出现故障。</p><p><strong>解决方案</strong>：</p><ul><li><p>Neutrino Router：Neutrino路由器能够通过一个预测模型，智能地选择最适合输入问题的大语言模型。</p></li><li><p>OpenRouter：OpenRouter提供了一个一站式的API接口，能够接入任何大语言模型。不仅能找到市场上任一模型的最低价格，还能在首选的服务提供商遇到问题时，自动切换到其他选项。</p></li></ul><h1 id="痛点12：LLM的安全性：">痛点12：LLM的安全性：</h1><p><strong>具体描述</strong>：在设计和开发 AI 系统时，如何有效防止恶意输入、确保输出安全、保护敏感信息不被泄露等问题，都是每位 AI 设计师和开发者需要面对的重要挑战。</p><p><strong>解决方案</strong>：</p><ul><li>Llama Guard：保护大语言模型的新工具，借鉴7-B Llama 2的技术，Llama Guard旨在通过评估输入（例如分析提问的内容）和输出（即对回答的分类）来帮助大语言模型 (LLMs) 鉴别内容是否安全。它本身也使用了大语言模型技术，能够判断某个问题或回答是否安全，若发现不安全的内容，还会详细列出违反的具体规则。</li></ul>]]></content>
    
    
    <summary type="html">RAG笔记2 —— RAG面临的问题及解决方案</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="RAG" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/RAG/"/>
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="RAG" scheme="https://enchantedovo.cn/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>RAG 应知必会</title>
    <link href="https://enchantedovo.cn/2024/04/08/LLM-Learning1/"/>
    <id>https://enchantedovo.cn/2024/04/08/LLM-Learning1/</id>
    <published>2024-04-08T12:58:56.000Z</published>
    <updated>2024-04-10T03:49:56.528Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考：<a href="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey</a></p></blockquote><h1 id="RAG的发展">RAG的发展</h1><p>RAG技术的发展可以从技术范式的角度分为几个阶段，主要包括：</p><ul><li><strong>朴素RAG(Naive RAG)</strong>:这是RAG技术的基出形式，包括三个基本步骤：索引(将文档库分割成较短的Chunk并通过编码器构建向量索引)、检索(根据问题和chunks的相似度检索相关文档片段)、生成(以检索到的上下文为条件，生成问题的回答)。</li><li><strong>进阶的RAG(Advanced RAG)</strong>:为了解决Naive RAG在检索质量、响应生成质量以及增强过程中的挑战，Advanced RAG:范式被提出。它在数据索引、检索前和检索后都进行了额外处理，如更精细的数据清洗、设计文档结构和添加元数据等，以提升文本的<br>一致性、准确性和检索效率。</li><li><strong>模块化RAG(Modular RAG)</strong>:随着RAG技术的进一步发展，模块化RAG概念被提出。它在结构上更加自由和灵活，引入了更多的具体功能模块，例如查询搜索引擎、融合多个回答。技术上将检索与微调分开，提供了更高的自定义性和灵活性。</li></ul><p>三个阶段之间是继承与发展的关系：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/04/08/LLM-Learning1/p1.png" class title="p1"><h1 id="如何评价RAG">如何评价RAG</h1><blockquote><p>🚩 <big><strong>评估指标</strong></big>：</p><ol><li>上下文相关性一一检索的上下文是否相关</li><li>上下文回溯一评估检索到的上下文是否包含了问题的所有必要信息</li><li>忠实度一一生成的答案是否与检索的内容一致，是否保留了原始信息</li><li>答案相关性一一评估生成的答案是否与问题相关，以及是否提供了有用的信息</li></ol></blockquote><h2 id="评估角度">评估角度</h2><h3 id="RAG三元组">RAG三元组</h3><ul><li><strong>Context Relevance</strong>：衡量召回的Context能够支持Query的程度。如果该得分低，反应出了召回了太多与Qury问题无关的内容，这些错误的召回知识会对LLM的最终回答造成一定影响。</li><li><strong>Groundedness</strong>：衡量LLM的Response遵从召回的Context的程度。如果该得分低，反应出了LLM的回答不遵从召回的知识，那么回答出现幻觉的可能就越大。</li><li><strong>Answer Relevance</strong>：衡量最终的Response回答对Query提问的相关度。如果该得分低，反应出了可能答不对题。</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/04/08/LLM-Learning1/p2.png" class title="p2"><h3 id="基于Ground-truth的指标">基于Ground-truth的指标</h3><ul><li>Ground-truth是回答，那就可以<u>直接比较RAG应用的回答和ground-truth之间的相关性</u>，来端到端地进行衡量。</li><li>让LLM生成评估测试集，来生成query和ground-truth,比如，在ragas的 <a href="https://docs.ragas.io/en/latest/concepts/testset_generation.html">Synthetic Test Data generation</a> 和 <a href="https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html">llama-index QuestionGeneration</a> 中都有一些集成好的方法，可以直接方便地使用。</li><li>Ground-truth是知识文档中的chunks，对比ground-truth doc chunks和召回的contexts之间的相关性。</li></ul><h3 id="基于LLM的定量评估">基于LLM的定量评估</h3><ul><li>有了GPT-4后，其可行性就提高了。我们只需设计好prompt,将要打分的一些文字放入prompt,访问GPT-4,就可以得到一个想要的得分结果。</li></ul><h2 id="评估工具">评估工具</h2><ol><li>RAGAs:<a href="https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html">https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html</a></li><li>TruLens:<a href="https://www.trulens.org/trulens_eval/install/">https://www.trulens.org/trulens_eval/install/</a></li><li>Llama-Index:<a href="https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html">https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html</a></li><li>Phoenix:<a href="https://docs.arize.com/phoenix/">https://docs.arize.com/phoenix/</a></li><li>DeepEval:<a href="https://github.com/confident-ai/deepeval">https://github.com/confident-ai/deepeval</a></li><li>LangSmith:<a href="https://docs.smith.langchain.com/evaluation/evaluator-implementations">https://docs.smith.langchain.com/evaluation/evaluator-implementations</a></li><li>OpenAI Evals:<a href="https://github.com/openai/evals">https://github.com/openai/evals</a></li></ol><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2024/04/08/LLM-Learning1/p3.png" class title="p3"><h1 id="RAG未来发展方向">RAG未来发展方向</h1><h2 id="RAG的垂直优化">RAG的垂直优化</h2><ul><li>长下文长度<ul><li>检索内容过多，超过窗口限制怎么办？</li><li>如果LLMs的上下文窗口不再受限制，RAG应该如何改进？</li></ul></li><li>鲁棒性<ul><li>检索到错误内容怎么处理？</li><li>怎么对检索出来内容进行过滤和验证？</li><li>怎么提高模型抗毒、抗噪声的能力。</li></ul></li><li>与微调协同<ul><li>如何同时发挥RAG和FT的效果，两者怎么协同，怎么组织，是串行、交替还是端到端？</li></ul></li><li>Scaling-Law<ul><li>RAG模型是否满足Scaling Law?</li><li>RAG是否会，或是在什么场景下会出现Inverse Scaling Law的现象？</li></ul></li><li>LLM的角色<ul><li>LLMs可以用于检索(用LLMs的生成代替检索或检索LLMs记忆)、用于生成、用于评估。如何进一步挖掘LLMs在RAG中的潜力？</li></ul></li></ul><h2 id="工程实践">工程实践</h2><ul><li>如何降低超大规模语料的检索时延？</li><li>如何保证检索出来内容不被大模型泄露？</li></ul><h2 id="RAG的多模态的拓展">RAG的多模态的拓展</h2><ul><li>如何将RAG不断发展的技术和思想拓展到图片、音频、视频或代码等其他模态的数据中？</li><li>一方面可以增强单一模态的任务，另一方面可以通过RAG的思想将多模态进行融合。</li></ul><h2 id="RAG的生态">RAG的生态</h2><ul><li>RAG的应用已经不仅仅局限于问答系统，其影响力正在扩展到更多领域。现在，推荐系统、信息抽取和报告生成等多种任务都开始受益于RAG技术的应用。</li><li>与此同时，RAG技术栈也在井喷。除了已知的Langchain和LlamaIndex等工具，市场上涌现出更多针对性的RAG工具，例如：用途定制化，满足更加聚焦场景的需求；使用简易化，进一步降低上手门槛的；功能专业化，逐渐面向生产环境。</li></ul>]]></content>
    
    
    <summary type="html">RAG笔记1 —— 基础</summary>
    
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="RAG" scheme="https://enchantedovo.cn/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/RAG/"/>
    
    
    <category term="大模型" scheme="https://enchantedovo.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="RAG" scheme="https://enchantedovo.cn/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>思考题</title>
    <link href="https://enchantedovo.cn/2022/11/16/OS-LearningNotes5/"/>
    <id>https://enchantedovo.cn/2022/11/16/OS-LearningNotes5/</id>
    <published>2022-11-16T11:54:20.000Z</published>
    <updated>2024-04-08T13:26:00.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="思考题">思考题</h1><h2 id="为什么开始启动计算机的时候，执行的是BIOS代码而不是操作系统自身的代码？（P1，3）">为什么开始启动计算机的时候，执行的是BIOS代码而不是操作系统自身的代码？（P1，3）</h2><p>加电的一瞬间，计算机内存中，准确的说是RAM中，还未初始化，没有任何程序。软盘里虽然有操作系统程序，但CPU的逻辑电路被设计为只能运行内存中的程序，没有能力直接从软盘运行操作系统。这就需要硬件主动加载0xffff0处的BIOS程序，由BIOS准备好中断向量表、中断服务程序，接着通过中断“int 0x19”将引导程序bootsect加载至内存，以及后续的一系列操作，最终操作系统自身代码才能位于内存中，被CPU执行。</p><p>在加电后，BIOS 需要完成一些检测工作，设置实模式下的中断向量表和服务程序，并将操作系统的引导扇区加载至 0x7C00 处，然后将跳转至 0x7C00运行操作系统自身的代码。</p><h2 id="为什么BIOS只加载了一个扇区，后续扇区却是由bootsect代码加载？为什么BIOS没有直接把所有需要加载的扇区都加载？（P6，7）">为什么BIOS只加载了一个扇区，后续扇区却是由bootsect代码加载？为什么BIOS没有直接把所有需要加载的扇区都加载？（P6，7）</h2><p>BIOS和操作系统通常由不同的专业团队开发的，为了能协调工作，需要按固定的规则约定，进行灵活的各自设计相应的部分。<br>因此，现行的方法是定位识别：对BIOS而言，“约定”接到启动操作系统命令，“定位识别”只从启动扇区把代码加载至0x7c00(BOOTSEG)位置，至于该扇区内容是什么，一概不管。而后续扇区则由操作系统自己的bootsect代码加载，这些代码由编写系统的用户负责，与之前BIOS也无关。<br>这样构建的好处是站在整个体系的高度，统一设计安排，简单而有效。且BIOS和操作系统的开发都可遵循这一约定，按照自己的意愿进行各自的设计，包括内存的规划等都更为灵活。</p><p>如果BIOS一次性加载，会存在：<br>1）对于不同的操作系统，其代码长度不一样，可能导致操作系统加载不完全；<br>2）使用BIOS进行加载，且加载完成之后再执行，需要很长的时间，因此Linux采用的是边执行边加载的方法。</p><h2 id="为什么BIOS把bootsect加载到0x07c00，而不是0x00000？加载后又马上挪到0x90000处，是何道理？为什么不一次加载到位？-P4、P5-17">为什么BIOS把bootsect加载到0x07c00，而不是0x00000？加载后又马上挪到0x90000处，是何道理？为什么不一次加载到位？(P4、P5-17)</h2><p>1）0x07c00是历史约定。<br>2）BIOS在从0x00000开始的1KB字节构建了中断向量表，接着的256KB字节内存空间构建了BIOS数据区，这些数据还有用处，所以不能把bootsect加载到0x00000。 0X07c00是BIOS设置的内存地址，不是bootsect能够决定的。<br>3）挪到0x90000处是操作系统内存规划行为，主要为了避免在内核system占据0x000000处时可能将0x07c00(bootsect)覆盖，造成在main中设置根设备时取不到正确数据。将该扇区挪到0x90000，在setup.s中，获取一些硬件数据保存在0x90000~0x901ff处，可以对一些后面内核将要利用的数据，集中保存和管理。</p><h2 id="bootsect、setup、head程序之间是怎么衔接的？给出代码证据。（P16、P24-P25-P26）">bootsect、setup、head程序之间是怎么衔接的？给出代码证据。（P16、P24+P25+P26）</h2><p>1）bootsect跳转至setup程序：jmpi 0, SETUPSEG;<br>解释：通过BIOS的“int 0x13”中断，找到bootsect自身的中断服务程序，将setup加载至SETUPSEG(0x90200)处。同样手法，将system加载至SYSSEG(0x10000)处。bootsect程序任务都已经完成。然后，通过“jmpi 0, SETUPSEG”跳转至setup程序的加载位置，此时CS:IP指向setup程序的第一条指令，意味着现在由setup程序接着bootsect程序继续执行。此时还是实模式。<br>2）setup跳转至head程序：jmpi 0, 8<br>解释：setup通过BIOS提供的中断服务程序提取了系统数据，存储在原来的bootsect位置只保留最后2字节未被覆盖（0x901fc，根设备号）。接着，将IF至0，完成关中断操作。然后，将system移动到0x00000位置，此时head已经占据了0x00000处，同时BIOS中断向量表彻底被覆盖。为此，setup开始为保护模式做准备，设置GDT、IDT并用CPU中专用寄存器IDTR、GDTR看住。接着，打开A20，也就是32位寻址模式，再对可编程中断控制器8259A进行重新编程，并置PE位为1，即设定处理器工作方式为保护模式，以后根据GDT决定执行哪里的程序。最后，通过“jmpi 0,8”跳转到head。“0”表示段内偏移，“8(1000)”是保护模式下的段选择符，最后两位“00”表示内核态，第二位“0”表示GDT，第一位“1”表示GDT表中GDT[1]项（内核代码段），从该项中得知段基址为0x00000000。结合上述偏移0，可知最终跳转至0x0000000处，执行head程序。</p><h2 id="setup程序的最后是jmpi-0-8-，为什么这个8不能简单的当作阿拉伯数字8看待，究竟有什么内涵？-P25">setup程序的最后是jmpi 0,8 ，为什么这个8不能简单的当作阿拉伯数字8看待，究竟有什么内涵？(P25)</h2><p>此时为32位保护模式，0表示段内偏移，8表示段选择符，转化为二进制：1000。<br>最后两位00表示内核特权级，第三位0表示GDT表，第四位1表示所选的表（在此就是GDT表）的1项来确定代码段的段基址和段限长等信息。<br>这样可以得到代码是从段基址0x00000000、偏移为0处开始执行的，即head的开始位置。</p><blockquote><p>补充：</p><ul><li>最后两位表示特权，其中00是内核，11是用户。</li><li>倒数第3位如果是0表示GDT，1则表示LDT。</li><li>前两位表示这个表的第几项。GDT表的0表示空，1表示内核代码段，2表示内核数据段。LDT表的0表示空，1表示用户代码段，2表示用户数据段。</li></ul></blockquote><h2 id="保护模式在“保护”什么？它的“保护”体现在哪里？特权级的目的和意义是什么？分页有“保护”作用吗？-P436-P439、P443">保护模式在“保护”什么？它的“保护”体现在哪里？特权级的目的和意义是什么？分页有“保护”作用吗？(P436-P439、P443)</h2><p>1）打开了保护模式后，CPU的寻址模式发生了变化，需要依赖于GDT去获取代码或数据段的基址。从GDT可以看出，保护模式除了段基址外，还有段限长，这样相当于增加了一个段位寄存器。既有效地防止了对代码或数据段的覆盖，又防止了代码段自身的访问超限，明显增强了保护作用。<br>2）体现：①在GDT、LDT及IDT中，均有自己界限特、权级等属性，这是对描述符所描述的对象的保护；②在不同特权级间访问时，系统会对CPL、RPL、DPL、IOPL 等进行检验，对不同层级的程序进行保护，同还限制某些特殊指令的使用，如 lgdt, lidt,cli等。<br>3）特权级的目的和意义：①为了更好的管理资源并保护系统不受侵害，操作系统利用先机，以时间换取特权，先霸占所有特权；②依托CPU提供的保护模式，着眼于“段”，在所有的段选择符最后两位标示特权级，禁止用户执行cli、sti等对掌控局面至关重要的指令。③操作系统可以把内核设计成最高特权级，把用户进程设计成最低特权级。这样，操作系统可以访问 GDT、LDT、TR，而 GDT、LDT是逻辑地址形成线性地址的关键，因此操作系统可以掌控线性地址。物理地址是由内核将线性地址转换而成的，所以操作系统可以访问任何物理地址，而用户进程只能使用逻辑地址。<br>4）①分页机制中PDE和PTE中的R/W和U/S等，提供了页级保护；②分页机制将线性地址与物理地址加以映射，提供了对物理地址的保护。<br>补充：为什么特权级是基于段的?<br>在操作系统设计中，一般一个段实现的功能相对完整，可以把代码放在一个段，数据放在一个段，并通过段选择符（包括CS、SS、DS、ES、FS和GS）获取段的基址和特权级等信息。特权级基于段，这样当段选择子具有不匹配的特权级时，按照特权级规则判断是否可以访问。特权级基于段，是结合了程序的特点和硬件实现的一种考虑。</p><h2 id="在setup程序里曾经设置过gdt，为什么在head程序中将其废弃，又重新设置了一个？为什么设置两次，而不是一次搞好？-P33">在setup程序里曾经设置过gdt，为什么在head程序中将其废弃，又重新设置了一个？为什么设置两次，而不是一次搞好？(P33)</h2><p>原来GDT所在的位置是设计代码时在setup.s里面设置的数据，将来这个setup模块所在的内存位置会在设计缓冲区时被覆盖。如果不改变位置，将来GDT的内容肯定会被缓冲区覆盖掉，从而影响系统的运行。这样一来，将来整个内存空间中唯一安全的地方就是现在head.s所在的位置了。<br>不能在执行setup程序时直接把GDT的内容复制到head.s所在位置：如果先复制GDT内容，后移动system模块，它就会被后者覆盖；如果先移动system模块，后复制GDT内容，它又会把head.s对应的程序覆盖，而这时head.s还没有执行。所以无论如何都要重新建立GDT。</p><h2 id="进程0的task-struct在哪？具体内容是什么？-P70">进程0的task_struct在哪？具体内容是什么？(P70)</h2><p>进程0的task_struct、内核栈、用户栈都在内核数据段。<br>进程0的task_struct 的具体内容包括状态、信号、pid、alarm、ldt、tss等管理该进程所需的数据。如下代码所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 进程0的task_struct</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_TASK \</span></span><br><span class="line"><span class="comment">/* state etc */</span>&#123; <span class="number">0</span>,<span class="number">15</span>,<span class="number">15</span>, \</span><br><span class="line"><span class="comment">/* signals */</span><span class="number">0</span>,&#123;&#123;&#125;,&#125;,<span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* ec,brk... */</span><span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* pid etc.. */</span><span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* uid etc */</span><span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* alarm */</span><span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* math */</span><span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* fs info */</span><span class="number">-1</span>,<span class="number">0022</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>,<span class="number">0</span>, \</span><br><span class="line"><span class="comment">/* filp */</span>&#123;<span class="literal">NULL</span>,&#125;, \</span><br><span class="line">&#123; \</span><br><span class="line">&#123;<span class="number">0</span>,<span class="number">0</span>&#125;, \</span><br><span class="line"><span class="comment">/* ldt */</span>&#123;<span class="number">0x9f</span>,<span class="number">0xc0fa00</span>&#125;, \</span><br><span class="line">&#123;<span class="number">0x9f</span>,<span class="number">0xc0f200</span>&#125;, \</span><br><span class="line">&#125;, \</span><br><span class="line"><span class="comment">/*tss*/</span>&#123;<span class="number">0</span>,PAGE_SIZE+(<span class="keyword">long</span>)&amp;init_task,<span class="number">0x10</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,(<span class="keyword">long</span>)&amp;pg_dir,\</span><br><span class="line"> <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, \</span><br><span class="line"> <span class="number">0</span>,<span class="number">0</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>, \</span><br><span class="line"> _LDT(<span class="number">0</span>),<span class="number">0x80000000</span>, \</span><br><span class="line">&#123;&#125; \</span><br><span class="line">&#125;, \</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下代码说明user_stack在内核数据段。（0x10）（参考杨炯的内核代码完全注释）一个任务的数据结构与其内核态堆栈是放在同一内存页中。所以进程0的内核栈是跟着task struct后面的，所以进程0的内核栈不是user_stack，user_stack是进程0的用户栈。（P91的图）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> user_stack [ PAGE_SIZE&gt;&gt;<span class="number">2</span> ] ;<span class="comment">//定义用户堆栈4K，指针指向最后一项</span></span><br><span class="line"><span class="comment">//该结构用于设置堆栈ss：esp（数据段选择符）</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line"><span class="keyword">long</span> * a;</span><br><span class="line"><span class="keyword">short</span> b;</span><br><span class="line">&#125; stack_start = &#123; &amp; user_stack [PAGE_SIZE&gt;&gt;<span class="number">2</span>] , <span class="number">0x10</span> &#125;;<span class="comment">//内核数据段</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//每个进程在内核态运行时都有自己的内核态堆栈，这里定义了内核态堆栈的结构</span></span><br><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">task_union</span> &#123;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> <span class="title">task</span>;</span></span><br><span class="line"><span class="keyword">char</span> <span class="built_in">stack</span>[PAGE_SIZE];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">union</span> <span class="title">task_union</span> <span class="title">init_task</span> =</span> &#123;INIT_TASK,&#125;;</span><br></pre></td></tr></table></figure><h2 id="内核的线性地址空间是如何分页的？（P37-38）画出从0x000000开始的7个页（包括页目录表、页表所在页）的挂接关系图，就是页目录表的前四个页目录项、第一个个页表的前7个页表项指向什么位置？（P39）给出代码证据。（P39）">内核的线性地址空间是如何分页的？（P37-38）画出从0x000000开始的7个页（包括页目录表、页表所在页）的挂接关系图，就是页目录表的前四个页目录项、第一个个页表的前7个页表项指向什么位置？（P39）给出代码证据。（P39）</h2><p>1）head.s在setup_paging开始创建分页机制。将页目录表和4个页表放到物理内存的起始位置，从内存起始位置开始的5个页空间内容全部清零（每页4kb），然后设置页目录表的前4项，使之分别指向4个页表。然后开始从高地址向低地址方向填写4个页表，依次指向内存从高地址向低地址方向的各个页面。即将第4个页表的最后一项（pg3+4092指向的位置）指向寻址范围的最后一个页面。即从0xFFF000开始的4kb 大小的内存空间。将第4个页表的倒数第二个页表项（pg3-4+4092）指向倒数第二个页面，即0xFFF000-0x1000开始的4KB字节的内存空间，依此类推。<br>2）图见P39</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/11/16/OS-LearningNotes5/p1.png" class title="p1"><p>3）Head.s中：（P39）\</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">setup_paging: </span><br><span class="line">movl $1024*5,%ecx  &#x2F;* 5 pages - pg_dir+4 page tables *&#x2F; </span><br><span class="line">xorl %eax,%eax </span><br><span class="line">xorl %edi,%edi  &#x2F;* pg_dir is at 0x000 *&#x2F; </span><br><span class="line">cld;rep;stosl</span><br><span class="line"> movl $pg0+7,pg_dir  &#x2F;* set present bit&#x2F;user r&#x2F;w *&#x2F;  </span><br><span class="line">movl $pg1+7,pg_dir+4  &#x2F;*  --------- &quot; &quot; --------- *&#x2F; </span><br><span class="line">movl $pg2+7,pg_dir+8  &#x2F;*  --------- &quot; &quot; --------- *&#x2F;  </span><br><span class="line">movl $pg3+7,pg_dir+12  &#x2F;*  --------- &quot; &quot; --------- *&#x2F; </span><br><span class="line">_pg_dir用于表示内核分页机制完成后的内核起始位置，也就是物理内存的起始位置0x000000，以上四句完成页目录表的前四项与页表1，2,3,4的挂接 </span><br><span class="line">movl $pg3+4092,%edi </span><br><span class="line">movl $0xfff007,%eax  &#x2F;*  16Mb - 4096 + 7 (r&#x2F;w user,p) *&#x2F; </span><br><span class="line">std </span><br><span class="line">1: stosl   &#x2F;* fill pages backwards - more efficient :-) *&#x2F; </span><br><span class="line">subl $0x1000,%eax</span><br><span class="line">jge 1b</span><br><span class="line">xorl %eax,%eax&#x2F;* pg_dir is at 0x0000 *&#x2F;</span><br><span class="line">movl %eax,%cr3&#x2F;* cr3 - page directory start *&#x2F;</span><br><span class="line">movl %cr0,%eax</span><br><span class="line">orl $0x80000000,%eax</span><br><span class="line">movl %eax,%cr0&#x2F;* set paging (PG) bit *&#x2F;</span><br><span class="line">ret&#x2F;* this also flushes prefetch-queue *&#x2F;</span><br></pre></td></tr></table></figure><p>完成页表项与页面的挂接，是从高地址向低地址方向完成挂接的，16M内存全部完成挂接（注意页表从0开始，页表0-页表3）</p><h2 id="在head程序执行结束的时候，在idt的前面有184个字节的head程序的剩余代码，剩余了什么？为什么要剩余？-P31、P36、P40">在head程序执行结束的时候，在idt的前面有184个字节的head程序的剩余代码，剩余了什么？为什么要剩余？(P31、P36、P40)</h2><p>1）剩余的内容：0x5400~0x54b7处，包含代码段：after_page_tables(栈中压入了些参数)、 ignore_int(初始化中断时的中断处理函数) 和 setup_paging(初始化分页)。<br>2）原因：after_page_tables中压入了一些参数，为内核进入main函数的跳转做准备。为了谨慎起见，设计者在栈中压入了L6，以使得系统可能出错时，返回到L6处执行。ignore_int: 使用 ignore_int将 idt全部初始化，因此如果中断开启后，可能使用了未设置的中断向量，那么将默认跳转到 ignore_int处执行。这样做的好处是使得系统不会跳转到随机的地方执行错误的代码，所以ignore_int不能被覆盖。 setup_paging:为设置分页机制的代码，它在分页完成前不能被覆盖。</p><h2 id="为什么不用call，而是用ret“调用”main函数？画出调用路线图，给出代码证据。-P42">为什么不用call，而是用ret“调用”main函数？画出调用路线图，给出代码证据。(P42)</h2><p>call指令会将EIP的值自动压栈，保护返回现场，然后执行被调函数的程序，等到执行被调函数的ret指令时，自动出栈给EIP并还原现场，继续执行call的下一条指令。然而对操作系统的main函数来说，如果用call调用main函数，那么ret时返回给谁呢？因为没有更底层的函数程序接收操作系统的返回。用ret实现的调用操作当然就不需要返回了，call做的压栈和跳转动作需要手工编写代码。<br>2）图在p42仿call示意图下面部分</p><p>3）代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">after_page_tables:</span><br><span class="line">pushl $0# These are the parameters to main :-)</span><br><span class="line">pushl $0</span><br><span class="line">pushl $0</span><br><span class="line">pushl $L6# return address for main, if it decides to.</span><br><span class="line">   pushl $__main; &#x2F;&#x2F;将main的地址压入栈，即EIP</span><br><span class="line">jmp setup_paging</span><br><span class="line">setup_paging:</span><br><span class="line">   ret; &#x2F;&#x2F;弹出EIP，针对EIP指向的值继续执行，即main函数的入口地址。</span><br></pre></td></tr></table></figure><h2 id="用文字和图说明中断描述符表是如何初始化的，可以举例说明（比如：set-trap-gate-0-divide-error-），并给出代码证据。（P52）">用文字和图说明中断描述符表是如何初始化的，可以举例说明（比如：set_trap_gate(0,&amp;divide_error)），并给出代码证据。（P52）</h2><p>对中断描述符表的初始化，就是将中断、异常处理的服务程序与IDT进行挂接，逐步重建中断服务体系。<br>（先画图见P54 图2-9然后解释）<br>以set_trap_gate(0,&amp;divide_error)//除零错误 为例，进行宏展开后得到：</p><p>其中：n是0；gate_addr是&amp;idt[0]，也就是IDT的第一项中断描述符的地址；type是15；dpl（描述符特权级）是0；addr是中断服务程序divide_error(void)的入口地址。</p><h2 id="在IA-32中，有大约20多个指令是只能在0特权级下使用，其他的指令，比如cli，并没有这个约定。奇怪的是，在Linux0-11中，3特权级的进程代码并不能使用cli指令，这是为什么？请解释并给出代码证据。-P68、P79、P92">在IA-32中，有大约20多个指令是只能在0特权级下使用，其他的指令，比如cli，并没有这个约定。奇怪的是，在Linux0.11中，3特权级的进程代码并不能使用cli指令，这是为什么？请解释并给出代码证据。(P68、P79、P92)</h2><p>根据IA-32手册，某些系统指令是禁止应用程序使用的。cli指令用于复位IF标志位，cli指令与CPL和EFLAGS[IOPL]有关。如果CPL的权限高于等于EFLAGS中的IOPL的权限，即数值上CPL&lt;=IOPL，则可以执行该指令，IF位清除为0。如果CPL大于当前程序或过程的IOPL，则产生保护模式异常。<br>由于在内核中IOPL的值初始为0，且未经改变。INIT_TASK的TSS中设置了EFLAGS值，进程0又在move_to_user_mode中，继承了内核的EFLAGS。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\include\linux\sched.h</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_TASK \</span></span><br><span class="line"><span class="comment">//..</span></span><br><span class="line"><span class="comment">/*tss*/</span>&#123;<span class="number">0</span>,PAGE_SIZE+(<span class="keyword">long</span>)&amp;init_task,<span class="number">0x10</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,(<span class="keyword">long</span>)&amp;pg_dir,\</span><br><span class="line"> <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, \     <span class="comment">//eflags的值，决定了cli这类指令只能在0特权级使用</span></span><br><span class="line"> <span class="number">0</span>,<span class="number">0</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>,<span class="number">0x17</span>, \</span><br><span class="line"> _LDT(<span class="number">0</span>),<span class="number">0x80000000</span>, \</span><br><span class="line">&#123;&#125; \</span><br><span class="line">&#125;, \</span><br><span class="line">&#125;</span><br><span class="line">\linux0<span class="number">.11</span>\include\<span class="keyword">asm</span>\system.h</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> move_to_user_mode() \</span></span><br><span class="line">......</span><br><span class="line"><span class="string">&quot;pushfl\n\t&quot;</span> \   <span class="comment">//eflags进栈</span></span><br><span class="line">......</span><br><span class="line"><span class="string">&quot;iret\n&quot;</span> \</span><br><span class="line"><span class="comment">//..</span></span><br><span class="line">:::<span class="string">&quot;ax&quot;</span>)</span><br></pre></td></tr></table></figure><p>在进程0的TSS中，设置了eflags中的IOPL位为0，代码见P68，后续进程如果没有改动的话也是0，即IOPL=0。因此，通过设置IOPL，可以限制3特权级进程代码使用cli指令。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\fork.c</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">copy_process</span><span class="params">(<span class="keyword">int</span> nr,<span class="keyword">long</span> ebp,<span class="keyword">long</span> edi,<span class="keyword">long</span> esi,<span class="keyword">long</span> gs,<span class="keyword">long</span> none,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">long</span> ebx,<span class="keyword">long</span> ecx,<span class="keyword">long</span> edx,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">long</span> fs,<span class="keyword">long</span> es,<span class="keyword">long</span> ds,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">long</span> eip,<span class="keyword">long</span> cs,<span class="keyword">long</span> eflags,<span class="keyword">long</span> esp,<span class="keyword">long</span> ss)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">p-&gt;tss.eip = eip;</span><br><span class="line">p-&gt;tss.eflags = eflags;</span><br><span class="line">p-&gt;tss.eax = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line"><span class="keyword">return</span> last_pid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="在system-h里（见下），读懂代码。这里中断门、陷阱门、系统调用都是通过-set-gate设置的，用的是同一个嵌入汇编代码，比较明显的差别是dpl一个是3，另外两个是0，这是为什么？说明理由。-P55">在system.h里（见下），读懂代码。这里中断门、陷阱门、系统调用都是通过_set_gate设置的，用的是同一个嵌入汇编代码，比较明显的差别是dpl一个是3，另外两个是0，这是为什么？说明理由。(P55)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#define _set_gate(gate_addr,type,dpl,addr) \</span><br><span class="line">__asm__ (&quot;movw %%dx,%%ax\n\t&quot; \</span><br><span class="line">    &quot;movw %0,%%dx\n\t&quot; \</span><br><span class="line">    &quot;movl %%eax,%1\n\t&quot; \</span><br><span class="line">    &quot;movl %%edx,%2&quot; \</span><br><span class="line">    : \</span><br><span class="line">    : &quot;i&quot; ((short) (0x8000+(dpl&lt;&lt;13)+(type&lt;&lt;8))), \</span><br><span class="line">    &quot;o&quot; (*((char *) (gate_addr))), \</span><br><span class="line">    &quot;o&quot; (*(4+(char *) (gate_addr))), \</span><br><span class="line">    &quot;d&quot; ((char *) (addr)),&quot;a&quot; (0x00080000))</span><br><span class="line">#define set_intr_gate(n,addr) \</span><br><span class="line">    _set_gate(&amp;idt[n],14,0,addr)</span><br><span class="line">#define set_trap_gate(n,addr) \</span><br><span class="line">    _set_gate(&amp;idt[n],15,0,addr)</span><br><span class="line">#define set_system_gate(n,addr) \</span><br><span class="line">    _set_gate(&amp;idt[n],15,3,addr)</span><br></pre></td></tr></table></figure><p>1）dpl机制：dpl表示的是特权级，0和3分别表示0特权级和3特权级。异常处理是由内核来完成，Linux处于对内核的保护，不允许用户进程直接访问内核。但是有些情况下，用户进程又需要内核代码的支持，因此就需要系统调用，它是用户进程与内核打交道的接口，是由用户进程直接调用的，因此其在3特权级下。<br>2）题目中：set_trap_gate 和set_intr_gate的dpl是0，set_system_gate的dpl是3。dpl为0表示只能在内核态下允许，dpl为3表示系统调用可以由3特权级调用。<br>当用户程序产生系统调用软中断后， 系统都通过system_call总入口找到具体的系统调用函数。set_system_gate设置系统调用，须将DPL设置为3，允许在用户特权级3的进程调用，否则会引发General Protection异常。set_trap_gate及set_intr_gate设置陷阱和中断为内核使用，需禁止用户进程调用，所以DPL为0。</p><h2 id="进程0-fork进程1之前，为什么先调用move-to-user-mode-？用的是什么方法？解释其中的道理。（P78-80）">进程0 fork进程1之前，为什么先调用move_to_user_mode()？用的是什么方法？解释其中的道理。（P78+80）</h2><p>1）因为在Linux-011中，规定除了进程0以外的所有进程，都必须在特权级为3下创建。所以进程0 fork进程1之前，要调用move_to_user_mode将0特权级翻转到3特权级。<br>2）move_to_user_mode()用的方法是模仿中断硬件压栈，顺序是ss、esp、eflags、cs、eip。然后执行iret，出栈恢复现场，翻转0特权级到3特权级。<br>3）CPU响应中断的时候，根据DPL的设置，可以实现指定的特权级之间的翻转。所以模拟中断硬件压栈可以实现特权级的翻转。</p><h2 id="在Linux操作系统中大量使用了中断、异常类的处理，究竟有什么好处？（P56）">在Linux操作系统中大量使用了中断、异常类的处理，究竟有什么好处？（P56）</h2><p>在未引入中断、异常处理类处理的概念之前，CPU每隔一段时间就要对所有硬件进行轮询，以检测它的工作是否完成，如果没有完成就继续轮询，这样消耗了CPU处理用户程序的时间，降低了系统的综合效率。可见，CPU以“主动轮询”的方式来处理信号是非常不划算的。采用了这种中断方式，以“被动响应”模式代替“主动轮询”模式来处理主机与外设的I/O问题，只有在发生异常的时候CPU才停下正在进行的运算进行处理，其他时候都在做自己的事情。这大大提高了操作系统的综合效率。是计算机历史上的一大进步。</p><h2 id="copy-process函数的参数最后五项是：long-eip-long-cs-long-eflags-long-esp-long-ss。查看栈结构确实有这五个参数，奇怪的是其他参数的压栈代码都能找得到，确找不到这五个参数的压栈代码，反汇编代码中也查不到，请解释原因。（P89）">copy_process函数的参数最后五项是：long eip,long cs,long eflags,long esp,long ss。查看栈结构确实有这五个参数，奇怪的是其他参数的压栈代码都能找得到，确找不到这五个参数的压栈代码，反汇编代码中也查不到，请解释原因。（P89）</h2><p>copy_process执行时因为进程调用了fork函数，fork是一个系统调用，会导致中断，int 0x80中断导致CPU硬件自动将SS、ESP、EFLAGS、CS、EIP的值按照顺序压入进程0内核栈，又因为函数专递参数是使用栈的，所以刚好可以做为copy_process的最后五项参数。</p><h2 id="分析get-free-page-函数的代码，叙述在主内存中获取一个空闲页的技术路线。（P90）">分析get_free_page()函数的代码，叙述在主内存中获取一个空闲页的技术路线。（P90）</h2><p>代码见P90  get_free_page函数</p><p>遍历mem_map[]，找到内存中（从高地址开始）第一个空闲（字节为0）页面，将其置为1。ecx左移12位加LOW_MEM得到该页的物理地址，并将页面清零。最后返回空闲页面物理内存的起始地址。<br>即：<br>(1)将EAX 设置为0，EDI设置指向mem_map的最后一项（mem_map+PAGING_PAGES-1），std设置扫描是从高地址向低地址。遍历mem_map[]，从mem_map的最后一项反向扫描，找出引用次数为0(AL)的页，如果没有则退出；如果找到，则将找到的页设引用数为1；<br>(2) ECX左移12位得到页的相对地址，加LOW_MEM得到物理地址，将此页最后一个字节的地址赋值给EDI（LOW_MEM+4092）；<br>(3) stosl将EAX的值设置到ES:EDI所指内存，即反向清零1024*32bit，将此页清空；<br>(4) 将页的地址（存放在EAX）返回。<br>注意!本函数只是指出在主内存区的一页空闲页面，但并没有映射到某个进程的线性地址去。后面的put_page()函数就是用来作映射的。</p><h2 id="分析copy-page-tables（）函数的代码，叙述父进程如何为子进程复制页表。-P97">分析copy_page_tables（）函数的代码，叙述父进程如何为子进程复制页表。(P97)</h2><p>先为新的页表申请一个空闲页面，并把进程0中第一个页表里面前160个页表项复制到这个页面中进程0和进程1页表暂时都指向了相同的页面，意味着进程1可以控制进程0的页面，之后对进程1的页目录表进行设置。最后重置CR3，刷新页变换高速缓存。设置完毕。</p><h2 id="进程0创建进程1时，为进程1建立了task-struct及内核栈，第一个页表，分别位于物理内存16MB顶端倒数第一页、第二页。请问，这两个页究竟占用的是谁的线性地址空间，内核、进程0、进程1、还是没有占用任何线性地址空间？说明理由（可以图示）并给出代码证据。P99">进程0创建进程1时，为进程1建立了task_struct及内核栈，第一个页表，分别位于物理内存16MB顶端倒数第一页、第二页。请问，这两个页究竟占用的是谁的线性地址空间，内核、进程0、进程1、还是没有占用任何线性地址空间？说明理由（可以图示）并给出代码证据。P99</h2><p>这两个页占用的是内核的线性地址空间。<br>依据在setup_paging（文件head.s）中，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">\linux0.11\boot\head.s</span><br><span class="line">setup_paging:</span><br><span class="line">&#x2F;&#x2F;…</span><br><span class="line">movl $pg3+4092,%edi</span><br><span class="line">movl $0xfff007,%eax&#x2F;*  16Mb - 4096 + 7 (r&#x2F;w user,p) *&#x2F;</span><br><span class="line">std</span><br><span class="line">1:stosl&#x2F;* fill pages backwards - more efficient :-) *&#x2F;</span><br><span class="line">subl $0x1000,%eax</span><br><span class="line">&#x2F;&#x2F;…</span><br><span class="line">上面的代码，指明了内核的线性地址空间为0x000000 ~ 0xffffff（即前16M），且线性地址与物理地址呈现一一对应的关系。</span><br><span class="line">进程0的局部描述符如下include&#x2F;linux&#x2F;sched.h&#x2F;INIT_TASK：</span><br><span class="line">&#x2F;*ldt*&#x2F; </span><br><span class="line">&#123;0x9f,0xc0fa00&#125;.\</span><br><span class="line">&#123;0x9f,0xc0f200&#125;,\</span><br></pre></td></tr></table></figure><p>为进程1分配的这两个页，在16MB的顶端倒数第一页、第二页，因此占用内核的线性地址空间。进程0的线性地址空间是内存前640KB，因为进程0的LDT中的limit属性限制了进程0能够访问的地址空间。进程1拷贝了进程0的页表（160项），而这160个页表项即为内核第一个页表的前160项，指向的是物理内存前640KB，因此无法访问到16MB的顶端倒数的两个页。</p><h2 id="假设：经过一段时间的运行，操作系统中已经有5个进程在运行，且内核分别为进程4、进程5分别创建了第一个页表，这两个页表在谁的线性地址空间？用图表示这两个页表在线性地址空间和物理地址空间的映射关系。-P266、P270">假设：经过一段时间的运行，操作系统中已经有5个进程在运行，且内核分别为进程4、进程5分别创建了第一个页表，这两个页表在谁的线性地址空间？用图表示这两个页表在线性地址空间和物理地址空间的映射关系。(P266、P270)</h2><p>这两个页面均占用内核的线性地址空间。既然是内核线性地址空间，则与物理地址空间为一一对应关系。根据每个进程占用16个页目录表项，则进程4占用从第65～81项（即64-80项）的页目录表项。同理，进程5占用第81～96项（即80-95项）的页目录表项。由于目前只分配了一个页面（用做进程的第一个页表），则分别只需要使用第一个页目录表项即可。</p><h2 id="下面代码中的-ljmp-0-n-t-很奇怪，按理说jmp指令跳转到得位置应该是一条指令的地址，可是这行代码却跳到了-m-tmp-a-，这明明是一个数据的地址，更奇怪的，这行代码竟然能正确执行。请论述其中的道理。P107">下面代码中的&quot;ljmp %0\n\t&quot; 很奇怪，按理说jmp指令跳转到得位置应该是一条指令的地址，可是这行代码却跳到了&quot;m&quot; (*&amp;__tmp.a)，这明明是一个数据的地址，更奇怪的，这行代码竟然能正确执行。请论述其中的道理。P107</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#define switch_to(n) &#123;\</span><br><span class="line">struct &#123;long a,b;&#125; __tmp; \</span><br><span class="line">__asm__(&quot;cmpl %%ecx,_current\n\t&quot; \</span><br><span class="line">    &quot;je 1f\n\t&quot; \</span><br><span class="line">    &quot;movw %%dx,%1\n\t&quot; \</span><br><span class="line">    &quot;xchgl %%ecx,_current\n\t&quot; \</span><br><span class="line">    &quot;ljmp %0\n\t&quot; \</span><br><span class="line">    &quot;cmpl %%ecx,_last_task_used_math\n\t&quot; \</span><br><span class="line">    &quot;jne 1f\n\t&quot; \</span><br><span class="line">    &quot;clts\n&quot; \</span><br><span class="line">    &quot;1:&quot; \</span><br><span class="line">    ::&quot;m&quot; (*&amp;__tmp.a),&quot;m&quot; (*&amp;__tmp.b), \</span><br><span class="line">    &quot;d&quot; (_TSS(n)),&quot;c&quot; ((long) task[n])); \</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>a对应EIP、b对应CS，ljmp通过CPU中的电路进行硬件切换，进程由当前进程切换到进程n。CPU将当前寄存器的值保存到当前进程的TSS中，将进程n的TSS数据和LDT的代码段和数据段描述符恢复给CPU的各个寄存器，实现任务切换。</p><p>进程0-&gt;1：ljmp %0\n\t通过任务门机制并未实际使用任务门，将CPU的各个寄存器值保存在进程0的TSS中，将进程1的TSS数据以LDT的代码段、数据段描述符数据恢复给CPU的各个寄存器，实现从0特权级的内核代码切换到3特权级的进程1代码执行。其中tss.eip也自然恢复给了CPU，此时EIP指向的就是fork中的if(__res &gt;= 0)语句。</p><h2 id="进程0开始创建进程1，调用fork（），跟踪代码时我们发现，fork代码执行了两次，第一次，执行fork代码后，跳过init（）直接执行了for-pause-，第二次执行fork代码后，执行了init（）。奇怪的是，我们在代码中并没有看到向转向fork的goto语句，也没有看到循环语句，是什么原因导致fork反复执行？请说明理由（可以图示），并给出代码证据。P107">进程0开始创建进程1，调用fork（），跟踪代码时我们发现，fork代码执行了两次，第一次，执行fork代码后，跳过init（）直接执行了for(;;) pause()，第二次执行fork代码后，执行了init（）。奇怪的是，我们在代码中并没有看到向转向fork的goto语句，也没有看到循环语句，是什么原因导致fork反复执行？请说明理由（可以图示），并给出代码证据。P107</h2><p>首先在copy_process()函数中，设置TSS“p-&gt;tss.eip = eip;”指向的是if (__res &gt;= 0); 而“p-&gt;tss.eax = 0;”决定main()中if (!fork())后面的分支走向。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\fork.c</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">copy_process</span><span class="params">(<span class="keyword">int</span> nr,<span class="keyword">long</span> ebp,<span class="keyword">long</span> edi,<span class="keyword">long</span> esi,<span class="keyword">long</span> gs,<span class="keyword">long</span> none,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">long</span> ebx,<span class="keyword">long</span> ecx,<span class="keyword">long</span> edx,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">long</span> fs,<span class="keyword">long</span> es,<span class="keyword">long</span> ds,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">long</span> eip,<span class="keyword">long</span> cs,<span class="keyword">long</span> eflags,<span class="keyword">long</span> esp,<span class="keyword">long</span> ss)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">*p = *current;<span class="comment">/* NOTE! this doesn&#x27;t copy the supervisor stack */</span></span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">p-&gt;start_time = jiffies;</span><br><span class="line">p-&gt;tss.back_link = <span class="number">0</span>;</span><br><span class="line">p-&gt;tss.esp0 = PAGE_SIZE + (<span class="keyword">long</span>) p;</span><br><span class="line">p-&gt;tss.ss0 = <span class="number">0x10</span>;</span><br><span class="line">p-&gt;tss.eip = eip;</span><br><span class="line">p-&gt;tss.eflags = eflags;</span><br><span class="line">p-&gt;tss.eax = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line"><span class="keyword">return</span> last_pid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着，copy_process()函数返回后，通过“pushl %eax”将函数返回值，也就是进程1的进程号压栈。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\system_call.s</span><br><span class="line">_system_call:</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">call _sys_call_table(,%eax,<span class="number">4</span>)</span><br><span class="line">pushl %eax</span><br></pre></td></tr></table></figure><p>“: “=a” (__res) \”将eax的值赋值给__res，所以“if (__res &gt;= 0) \”实际上是看此时的eax时多少，由上可知，eax＝1。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\init\main.c</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _syscall0(type,name) \</span></span><br><span class="line">type name(void) \</span><br><span class="line">&#123; \</span><br><span class="line"><span class="keyword">long</span> __res; \</span><br><span class="line"><span class="function">__asm__ <span class="title">volatile</span> <span class="params">(<span class="string">&quot;int $0x80&quot;</span> \</span></span></span><br><span class="line"><span class="function"><span class="params">: <span class="string">&quot;=a&quot;</span> (__res) \</span></span></span><br><span class="line"><span class="function"><span class="params">: <span class="string">&quot;0&quot;</span> (__NR_##name))</span></span>; \</span><br><span class="line"><span class="keyword">if</span> (__res &gt;= <span class="number">0</span>) \</span><br><span class="line"><span class="keyword">return</span> (type) __res; \</span><br><span class="line">errno = -__res; \</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>; \</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>回到if (!fork())处执行，!1为“假”，不会执行init()，直接执行“for(;; ) pause();”。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\sched.c</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span><span class="comment">/* This really IS void, no error here. */</span></span></span><br><span class="line"><span class="function"></span>&#123;<span class="comment">/* The startup routine assumes (well, ...) this */</span></span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">move_to_user_mode();</span><br><span class="line"><span class="keyword">if</span> (!fork()) &#123;<span class="comment">/* we count on this going ok */</span></span><br><span class="line">init();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(;;) pause();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由pause()函数进入“schedule();”开始调度，然后通过“switch_to(next);”准备切换进程。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\sched.c</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sys_pause</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">current-&gt;state = TASK_INTERRUPTIBLE;</span><br><span class="line">schedule();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\sched.c</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">schedule</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">switch_to(next);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行switch_to()函数中，当程序执行到“ljmp %0\n\t”这行时，ljmp通过CPU任务门机制自动将进程1的TSS值恢复给CPU，自然也将其中的tss.eip恢复给CPU，这时EIP指向fork的if(__res &gt;= 0)这行。而此时的__res值就是进程1中TSS的eax的值，这个值在前面被写死为0，即“p-&gt;tss.eax = 0;”所以执行到“return (type)__res;”这行时，返回值为0。返回后，执行到if(!fork())这一行，!0为“真”，调用init()函数！</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\fs\buffer.c</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> switch_to(n) &#123;\</span></span><br><span class="line"><span class="comment">//…</span></span><br><span class="line"><span class="string">&quot;ljmp %0\n\t&quot;</span> \</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结：主要是利用两个系统调用sys_fork和sys_pause对进程状态的设置，以及利用了进程调度机制。</p><h2 id="打开保护模式、分页后，线性地址到物理地址是如何转换的？P97">打开保护模式、分页后，线性地址到物理地址是如何转换的？P97</h2><p>打开保护模式、分页后，线性地址需要通过MMU进行解析，以页目录表、页表、页面三级映射模式映射到物理地址。具体转换过程是这样的：“每个线性地址值是32位，MMU按照10-10-12的长度来识别地址值，分别解析为页目录项号、页表项号、页面内偏移。CR3中存放着页目录表的基址，通过CR3找到页目录表，再找到页目录项，进而找到对应页表，寻取页表项，然后找到页面物理地址，最后加上12位页内偏移形成的地址，才为最终物理地址”。<br>总结：CR3-&gt;页目录表-&gt;页目录项-&gt;页表-&gt;页表项-&gt;页面-&gt;物理地址</p><h2 id="getblk函数中，申请空闲缓冲块的标准就是b-count为0，而申请到之后，为什么在wait-on-buffer-bh-后又执行if（bh-b-count）来判断b-count是否为0？P115">getblk函数中，申请空闲缓冲块的标准就是b_count为0，而申请到之后，为什么在wait_on_buffer(bh)后又执行if（bh-&gt;b_count）来判断b_count是否为0？P115</h2><p>字段b_count：用来标记“每个缓冲块有多少个进程在共享”。只有当b_count=0时，该缓冲块才能被再次分配。<br>举个引发异常例子：每个缓冲块有一个进程等待队列，假设此时B、C两进程在队列中，当该缓冲块被解锁时，进程C被唤醒（它开始使用缓冲区之前需先唤醒进程B，使进程B从挂起进入就绪状态），将缓冲区加锁，一段时间后，进程C又被挂起，但此时缓冲区进程C仍在使用。这时候，进程B被调度，“if (bh-&gt;b_count)”该缓冲区任是加锁状态，进程B重新选择缓冲区…如果不执行该判断，将造成进程B操作一个被加锁的缓冲区，引发异常。<br>总结：等待缓冲区解锁这段时间，缓冲块可能会被别的进程占用，因此需要再次判断一下b_count是否为0，如果不为0，则还得继续等。</p><h2 id="b-dirt已经被置为1的缓冲块，同步前能够被进程继续读、写？给出代码证据。-P331">b_dirt已经被置为1的缓冲块，同步前能够被进程继续读、写？给出代码证据。(P331)</h2><p>同步前可以被进程读写，但不能挪为它用（即关联其它物理块）。b_dirt是针对硬盘方向的，进程与缓冲块方向由b_uptodate标识。只要b_uptodate为1，缓冲块就能被进程读写。读操作不会改变缓冲块中数据的内容，写操作后，改变了缓冲区内容，需要将b_dirt置1。由于此前缓冲块中的数据已经用硬盘数据块更新了，所以后续同步过程中缓冲块没有写入新数据的部分和原来硬盘对应的部分相同，所有的数据都是进程希望同步到硬盘数据块上的，不会把垃圾数据同步到硬盘数据库上去，所以b_uptodate仍为1。<br>所以，b_dirt为1，进程仍能对缓冲区进行读写。</p><p>1）读写文件均与b_dirt无关：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\fs\file_dev.c</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">file_write</span><span class="params">(struct m_inode * inode, struct file * filp, <span class="keyword">char</span> * buf, <span class="keyword">int</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line"><span class="keyword">if</span> (filp-&gt;f_flags &amp; O_APPEND)</span><br><span class="line">pos = inode-&gt;i_size;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">pos = filp-&gt;f_pos;</span><br><span class="line"><span class="keyword">while</span> (i&lt;count) &#123;</span><br><span class="line"><span class="keyword">if</span> (!(block = create_block(inode,pos/BLOCK_SIZE)))</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">if</span> (!(bh=bread(inode-&gt;i_dev,block)))</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">file_read</span><span class="params">(struct m_inode * inode, struct file * filp, <span class="keyword">char</span> * buf, <span class="keyword">int</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line"><span class="keyword">if</span> ((left=count)&lt;=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (left) &#123;</span><br><span class="line"><span class="keyword">if</span> (nr = bmap(inode,(filp-&gt;f_pos)/BLOCK_SIZE)) &#123;</span><br><span class="line"><span class="keyword">if</span> (!(bh=bread(inode-&gt;i_dev,nr)))</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）在获取缓冲块时，亦与b_dirt无任何关系：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\fs\buffer.c</span><br><span class="line"><span class="function">struct buffer_head * <span class="title">bread</span><span class="params">(<span class="keyword">int</span> dev,<span class="keyword">int</span> block)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">buffer_head</span> * <span class="title">bh</span>;</span></span><br><span class="line"><span class="keyword">if</span> (!(bh=getblk(dev,block)))</span><br><span class="line">panic(<span class="string">&quot;bread: getblk returned NULL\n&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (bh-&gt;b_uptodate)</span><br><span class="line"><span class="keyword">return</span> bh;</span><br><span class="line">ll_rw_block(READ,bh);</span><br><span class="line">wait_on_buffer(bh);</span><br><span class="line"><span class="keyword">if</span> (bh-&gt;b_uptodate)</span><br><span class="line"><span class="keyword">return</span> bh;</span><br><span class="line">brelse(bh);</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\fs\buffer.c</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BADNESS(bh) (((bh)-&gt;b_dirt<span class="meta-string">&lt;&lt;1)+(bh)-&gt;b_lock)</span></span></span><br><span class="line"><span class="function">struct buffer_head * <span class="title">getblk</span><span class="params">(<span class="keyword">int</span> dev,<span class="keyword">int</span> block)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">buffer_head</span> * <span class="title">tmp</span>, * <span class="title">bh</span>;</span></span><br><span class="line"></span><br><span class="line">repeat:</span><br><span class="line"><span class="keyword">if</span> (bh = get_hash_table(dev,block))</span><br><span class="line"><span class="keyword">return</span> bh;</span><br><span class="line"><span class="comment">//…</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="分析panic函数的源代码，根据你学过的操作系统知识，完整、准确的判断panic函数所起的作用。假如操作系统设计为支持内核进程（始终运行在0特权级的进程），你将如何改进panic函数？">分析panic函数的源代码，根据你学过的操作系统知识，完整、准确的判断panic函数所起的作用。假如操作系统设计为支持内核进程（始终运行在0特权级的进程），你将如何改进panic函数？</h2><p>1）panic函数是当系统发现无法继续运行下去的故障时调用它，会导致程序终止，由系统显示错误号。如果出现错误的函数不是进程0，则进行数据同步，把缓冲区的数据尽量同步到硬盘上去。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\panic.c</span><br><span class="line"><span class="function"><span class="keyword">volatile</span> <span class="keyword">void</span> <span class="title">panic</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> * s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">printk(<span class="string">&quot;Kernel panic: %s\n\r&quot;</span>,s);</span><br><span class="line"><span class="keyword">if</span> (current == task[<span class="number">0</span>])</span><br><span class="line">printk(<span class="string">&quot;In swapper task - not syncing\n\r&quot;</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">sys_sync();</span><br><span class="line"><span class="keyword">for</span>(;;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>panic()函数的执行分为两种情况，如果是0进程调用了panic（）说明在创建进程1过程中出现了严重错误不可继续执行（copy_page_table()中from没有按照4M对齐）且此时不涉及与硬盘中数据的交互，所以这时直接让系统执行死循环for（；；），且这个时候系统中也不存在可调度的进程，整个电脑死机。如果是其他进程调用了panic（）则说明是某个用户进程出现严重错误，这个过程中可能涉及缓冲区数据没有写回硬盘的情况，所以在死循环之前，需要将缓冲区中为写回的数据写回硬盘，保证程序执行后硬盘中数据的一致性，所以需要调用sys_sync()写回数据，而且这个时候系统也能够完成次工作。<br>2）改进：将死循环改成跳到的内核进程（始终运行在0特权级的进程），让内核继续执行。</p><h2 id="详细分析进程调度的全过程。考虑所有可能（signal、alarm除外）P105">详细分析进程调度的全过程。考虑所有可能（signal、alarm除外）P105</h2><p>首先依据task[64]这个结构，从后往前遍历，寻找进程状态为“就绪态”且时间片最大的进程作为下一个要执行的进程。通过调用switch_to函数跳转到指定进程。在此过程中，如果发现存在状态为就绪态的进程，但没有时间片，则从后往前重新分配时间片。然后重新执行上述过程。寻找状态为就绪态，时间片最大的进程作为下一个要执行的进程。如果没有就绪态，就跳转到进程0。</p><p>答2（具体）：<br>schedule()函数的主要过程为，首先依据task[64]这个结构，第一次遍历所有进程，只要地址指针不为空，就要针对它的signal、alarm分析，这里先不考虑。第二次遍历所有进程，比较进程的状态和时间片，找出处在就绪态且counter最大的进程。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\kernel\sched.<span class="function">cvoid <span class="title">schedule</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i,next,c;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> ** <span class="title">p</span>;</span><span class="comment">/* check alarm, wake up any interruptible tasks that have got a signal */</span></span><br><span class="line"><span class="keyword">for</span>(p = &amp;LAST_TASK ; p &gt; &amp;FIRST_TASK ; --p)</span><br><span class="line"><span class="keyword">if</span> (*p) &#123;</span><br><span class="line"><span class="keyword">if</span> ((*p)-&gt;alarm &amp;&amp; (*p)-&gt;alarm &lt; jiffies) &#123;</span><br><span class="line">(*p)-&gt;signal |= (<span class="number">1</span>&lt;&lt;(SIGALRM<span class="number">-1</span>));</span><br><span class="line">(*p)-&gt;alarm = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (((*p)-&gt;signal &amp; ~(_BLOCKABLE &amp; (*p)-&gt;blocked)) &amp;&amp;</span><br><span class="line">(*p)-&gt;state==TASK_INTERRUPTIBLE)</span><br><span class="line">(*p)-&gt;state=TASK_RUNNING;</span><br><span class="line">&#125;<span class="comment">/* this is the scheduler proper: */</span></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">c = <span class="number">-1</span>;</span><br><span class="line">next = <span class="number">0</span>;</span><br><span class="line">i = NR_TASKS;</span><br><span class="line">p = &amp;task[NR_TASKS];</span><br><span class="line"><span class="keyword">while</span> (--i) &#123;</span><br><span class="line"><span class="keyword">if</span> (!*--p)</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line"><span class="keyword">if</span> ((*p)-&gt;state == TASK_RUNNING &amp;&amp; (*p)-&gt;counter &gt; c)</span><br><span class="line">c = (*p)-&gt;counter, next = i;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (c) <span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">for</span>(p = &amp;LAST_TASK ; p &gt; &amp;FIRST_TASK ; --p)</span><br><span class="line"><span class="keyword">if</span> (*p)</span><br><span class="line">(*p)-&gt;counter = ((*p)-&gt;counter &gt;&gt; <span class="number">1</span>) +</span><br><span class="line">(*p)-&gt;priority;</span><br><span class="line">&#125;</span><br><span class="line">switch_to(next);&#125;</span><br></pre></td></tr></table></figure><p>执行switch_to()函数中，ljmp %0\n\t通过任务门机制并未实际使用任务门，将CPU的各个寄存器值保存在进程0的TSS中，将进程1的TSS数据以LDT的代码段、数据段描述符数据恢复给CPU的各个寄存器，实现从0特权级的内核代码切换到3特权级的进程1代码执行。其中tss.eip也自然恢复给了CPU，此时EIP指向的就是fork中的if(__res &gt;= 0)语句。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\fs\buffer.c<span class="meta">#<span class="meta-keyword">define</span> switch_to(n) &#123;\</span></span><br><span class="line"><span class="comment">//…</span></span><br><span class="line"><span class="string">&quot;ljmp %0\n\t&quot;</span> \</span><br><span class="line"><span class="comment">//…&#125;</span></span><br></pre></td></tr></table></figure><h2 id="wait-on-buffer函数中为什么不用if（）而是用while（）？P125">wait_on_buffer函数中为什么不用if（）而是用while（）？P125</h2><p>有可能很多进程都在等待一个缓冲块。在缓冲块同步完毕后，唤醒等待进程到轮转到某一进程的过程中，很有可能之前等的缓冲块被别的进程占用并加锁。如果使用if，则该进程被唤醒以后回来不会再判断缓冲块是否被占用，而直接使用就会导致出错。使用while，就会再判断一下缓冲块是否被占用，确认未被占用后使用，就不会发生之前的错误。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\linux0<span class="number">.11</span>\fs\buffer.<span class="function">cstatic <span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">wait_on_buffer</span><span class="params">(struct buffer_head * bh)</span></span>&#123;</span><br><span class="line">cli();</span><br><span class="line"><span class="keyword">while</span> (bh-&gt;b_lock)</span><br><span class="line">sleep_on(&amp;bh-&gt;b_wait);</span><br><span class="line">sti();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="操作系统如何利用b-uptodate保证缓冲块数据的正确性？new-block-int-dev-函数新申请一个缓冲块后，并没有读盘，b-uptodate却被置1，是否会引起数据混乱？详细分析理由。P325-329">操作系统如何利用b_uptodate保证缓冲块数据的正确性？new_block (int dev)函数新申请一个缓冲块后，并没有读盘，b_uptodate却被置1，是否会引起数据混乱？详细分析理由。P325+329</h2><p>答1：只要缓冲块的b_uptodate字段被设置为1，缓冲块的数据已经是数据块最新的，就可以放心的支持进程共享缓冲块的数据。反之，如果b_uptodate为0，就提醒内核缓冲块并没有用绑定的数据块中的数据更新，不支持进程共享该缓冲块。值得注意的是b_uptodate被设置为1，是告诉内核，缓冲块中的数据已经用数据块中的数据更新过了，但并不等于两者的数据就完全一致。<br>如题中的，申请一个缓冲块后，并没有读盘，b_uptodate却被置1，这并不会引起数据混乱。这时因为只要为新建的数据块新申请了缓冲块，不管这个缓冲块将来用做什么，反正进程现在不需要里面的数据，干脆全部清零。这样不管与之绑定的数据块用来存储什么信息，都无所谓，将该缓冲块的b_uptodate置为1，更新问题“等效于”以解决。</p><p>答2：<br>b_uptodate：是缓冲块中针对进程方向的标志位，它的作用是告诉内核，缓冲块的数据是否已是数据块中最新的。当b_update置1时，就说明缓冲块中的数据是基于硬盘数据块的，内核可以放心地支持进程与缓冲块进行数据交互；如果b_uptodate为0，就提醒内核缓冲块并没有用绑定的数据块中的数据更新，不支持进程共享该缓冲块。<br>①当为文件创建新数据块，新建一个缓冲块时，b_uptodate被置1，但并不会引起数据混乱。此时，新建的数据块只可能有两个用途，一个是存储文件内容，一个是存储文件的i_zone的间接块管理信息。<br>②如果是存储文件内容，由于新建数据块和新建硬盘数据块，此时都是垃圾数据，都不是硬盘所需要的，无所谓数据是否更新，结果“等效于”更新问题已经解决。<br>③如果是存储文件的间接块管理信息，必须清零，表示没有索引间接数据块，否则垃圾数据会导致索引错误，破坏文件操作的正确性。虽然缓冲块与硬盘数据块的数据不一致，但同样将b_uptodate置1不会有问题。</p><h2 id="分析add-reques（）函数中下列代码">分析add_reques（）函数中下列代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!(tmp = dev-&gt;current_request)) &#123;</span><br><span class="line">    dev-&gt;current_request = req;</span><br><span class="line">    sti();</span><br><span class="line">    (dev-&gt;request_fn)();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>其中的</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!(tmp = dev-&gt;current_request)) &#123;</span><br><span class="line">    dev-&gt;current_request = req;</span><br></pre></td></tr></table></figure><p><strong>是什么意思？P121</strong><br>查看指定设备是否有当前请求项，即查看设备是否忙。<br>if判断首先将当前设备的请求队列的队首赋值给tmp，队列为空的情况下if条件成立，则说明当前没有需要处理的请求项（现在是第一次使用缓冲区，第一次申请空闲请求项与其挂接，当前请求项一定为空）。将当前请求项req作为请求队列的队首（dev-&gt;current_request = req;）。req是此前在make_request中申请的一个空闲请求项，与对应缓冲块挂接。（如果该设备的请求队列不为空，后面代码会将req直接挂在请求队列的队尾。）</p><h2 id="do-hd-request-函数中dev的含义始终一样吗？-P318">do_hd_request()函数中dev的含义始终一样吗？(P318)</h2><p>do_hd_request()函数主要用于处理当前硬盘请求项。但其中的dev含义并不一致。<br>①含义1-逻辑设备号：“dev = MINOR(CURRENT-&gt;dev);”MINOR函数会取CURRENT-&gt;dev的低8位赋值给dev，低8位表示的含义是逻辑设备号。此时dev的值表示的是当前的逻辑设备号，目的是判断设备号是否合法。dev&gt;=5*NR_HD 是判断设备号是否超出了0-4或5-9的范围。block+2 &gt;hd[dev].nr_sects判断是否还有空闲盘块。<br>②含义2-硬盘号：“dev /= 5;”dev代表硬盘号（硬盘0还是硬盘1）：改变了dev的值，若原来在0-4的范围，此时则变成了0，若原来在5-9的范围此时则变成了1。目的是为了确定是第0块还是第一块物理盘。</p><h2 id="read-intr（）函数中，下列代码是什么意思？为什么这样做？-P323">read_intr（）函数中，下列代码是什么意思？为什么这样做？(P323)</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (--CURRENT-&gt;nr_sectors) &#123;</span><br><span class="line">    do_hd = &amp;read_intr;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于Linux0.1.1是PIO模式，PIO模式的做法是一次读一个扇区，读完以后发送中断告诉CPU当前已经读完了，此时CPU进行处理，将数据读到缓冲区。以引导块为例，需要进行两次PIO，相当于执行了两次中断。<br>该代码是为了判断请求项的缓冲块数据是否已经读完，“—CURRENT-&gt;nr_sectors”将递减请求项所需读取的扇区数值，如果没有读完则if条件成立。内核将再次把read_intr()绑定在硬盘中断服务程序上，以待下次使用，之后中断服务程序返回。<br>当所需要硬盘上的数据已经读完，硬盘产生中断，读盘中断服务程序再次响应这个中断，进入read_intr()函数后，仍然会判断请求项对应的缓存块数据是否已经读完，如果读完则不进入这个if 里。</p><h2 id="bread（）函数代码中为什么要做第二次if-bh-b-uptodate-判断？">bread（）函数代码中为什么要做第二次if (bh-&gt;b_uptodate)判断？</h2><pre><code>if (bh-&gt;b_uptodate)    return bh;ll_rw_block(READ,bh);wait_on_buffer(bh);if (bh-&gt;b_uptodate)    return bh;</code></pre><p>答1：bread()函数主要是从块设备上读取数据。调用底层ll_rw_block()函数，产生读设备请求。然后等待指定数据块读入，并等待缓冲块解锁。在睡眠醒来之后，如果缓冲块已更新“if (bh-&gt;b_uptodate)”，则返回缓冲块指针。否则，表明读设备操作失败，于是释放该缓冲块返回NULL。</p><p>答2：第一次从缓冲区取出设备号块号一致的缓冲块，判断缓冲块是否有效，有效则使用；无效则发出读设备数据库请求。<br>第二次等待指定数据块读入，等缓冲块解锁以后，唤醒进程后，要重新判断缓冲块是否有效，如果缓冲区中数据有效，则返回缓冲区头指针退出，否则释放该缓冲区返回Null。<br>在等待过程中，数据可能发生了变化，所以要二次判断。</p><h2 id="getblk（）函数中，两次调用wait-on-buffer（）函数，两次的意思一样吗？-P372">getblk（）函数中，两次调用wait_on_buffer（）函数，两次的意思一样吗？(P372)</h2><p>这里都是等待缓冲块解锁，但是此时缓冲区情况不一样：<br>第一次：已经找到一个比较合适的空闲缓冲块，但可能是加锁的，等待该缓冲块解锁；<br>第二次：如果该缓冲区已被修改，则将数据写盘，并再次等待缓冲块解锁。</p><p>执行到第一次wait_on_buffer时，意味着前面已经找到了空闲的缓冲块，但是这个缓冲块可能是是已经被加锁（其他进程在操作该空闲缓冲块），也可能是脏的，同样也可能是没有被别人占用的缓冲块。所以此时调用wait_on_buffer是检查是否该缓冲块被加锁，若加锁则阻塞当前进程。<br>执行到第二次wait_on_buffer时，会检查缓冲块的内容是否为脏（是否被修改过），若为脏，需要同步逻辑设备（写回设备）。写回设备是一个漫长的过程，不能保证在这个过程中没有其他进程对缓冲块进行加锁。当写回设备后发现该缓冲块被其他进程加锁，同样需要阻塞当前进程。</p><h2 id="getblk（）函数中，说明什么情况下执行continue、break。P372">getblk（）函数中，说明什么情况下执行continue、break。P372</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (tmp-&gt;b_count)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (!bh || BADNESS(tmp)&lt;BADNESS(bh)) &#123;</span><br><span class="line">            bh = tmp;</span><br><span class="line">            <span class="keyword">if</span> (!BADNESS(tmp))</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">/* and repeat until we find something good */</span></span><br><span class="line">    &#125; <span class="keyword">while</span> ((tmp = tmp-&gt;b_next_free) != free_list);</span><br></pre></td></tr></table></figure><p>getblk()函数主要是获取高速缓冲中的指定缓冲块。下面的宏用于判断缓冲块的修改标志，并定义修改标志的权重比锁定标志大。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//BADNESS定义</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BADNESS(bh) (((bh)-&gt;b_dirt<span class="meta-string">&lt;&lt;1)+(bh)-&gt;b_lock)</span></span></span><br></pre></td></tr></table></figure><p>continue：tmp指向的是空闲链表的第一个空闲缓冲块头“tmp = free_list;”。如果该缓冲块正在被使用，引用计数“tmp-&gt;b_count”不等于0，则continue，继续遍历直到找到一个count为0的空闲块。<br>break：接下来，如果缓冲头指针bh为空，或者tmp所指的缓冲头标志（修改、锁定）权重小于bh头标志的权重，则让bh指向tmp缓冲块头。如果BADNESS为0，该tmp缓冲块头表明缓冲块既没有修改也没有锁定标志位，即这个块既没加锁，也没被写过，则说明已为指定设备上的块取得对应的高速缓冲块。然后break跳出循环。（也只有在系统运行初期才会存在这样的块）</p><h2 id="make-request（）函数中，其中的sleep-on-wait-for-request-是谁在等？等什么？">make_request（）函数中，其中的sleep_on(&amp;wait_for_request)是谁在等？等什么？</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (req &lt; request) &#123;</span><br><span class="line">    <span class="keyword">if</span> (rw_ahead) &#123;</span><br><span class="line">        unlock_buffer(bh);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    sleep_on(&amp;wait_for_request);</span><br><span class="line">    <span class="keyword">goto</span> repeat;</span><br></pre></td></tr></table></figure><p>此时wait_for_request指的是：当前进程等待request请求队列腾出空闲项。</p><p>make_request()函数主要功能为创建请求项并插入请求队列。根据具体读写操作，如果request[32]中没有一项是空闲的，则查看此次请求是不是提前读写，如果是则立即放弃此次请求操作。否则让本次请求先睡眠“sleep_on(&amp;wait_for_request);”以等待request请求队列腾出空闲项，一段时间后再次搜索请求队列。</p>]]></content>
    
    
    <summary type="html">国科大杨力祥《操作系统高级教程》思考题整理</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="操作系统高级教程" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="操作系统" scheme="https://enchantedovo.cn/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="Linux Kernel" scheme="https://enchantedovo.cn/tags/Linux-Kernel/"/>
    
  </entry>
  
  <entry>
    <title>深度学习</title>
    <link href="https://enchantedovo.cn/2022/10/03/AdvancedAI-LearningNotes4/"/>
    <id>https://enchantedovo.cn/2022/10/03/AdvancedAI-LearningNotes4/</id>
    <published>2022-10-03T06:58:20.000Z</published>
    <updated>2024-04-08T14:14:55.677Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、玻尔兹曼机系列">一、玻尔兹曼机系列</h1><h2 id="Hopfield">Hopfield</h2><p>Hopfield网络是反馈类型，其神经元的结构功能在网络中的地位是一样的。其学习是基于灌输式学习，即网络的权值不是通过训练出来的，而是按照一定规则计算出来的，<strong>将求解的问题转换成优化问题的能量函数，网络的稳定状态是优化问题的解，其权值一旦确定就不再改变了</strong>。</p><p>简单的来讲就是，Hopfield网络的主要功能是联想记忆。既然如此，首先应该让网络实现“记忆”，我们需要一些数据，然后训练网络，训完练完成之后，可以得到一组可用的权值信息，形成网络的“记忆”功能。当输入数据不完整时，根据训练得到的权重去运算，得到一个稳定的输出状态，这就是联想功能。</p><p>【BM与Hopfield有一定共性，看一下有助理解。】</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p1.png" class title="p1"><h2 id="BM（玻尔兹曼机）">BM（玻尔兹曼机）</h2><p>离散Hopfield神经网络+模拟退火+隐单元=Boltzman机</p><p>Boltzmann机结合多层前馈神经网络和离散Hopfield网络在网络结构、学习算法和动态运行机制方面的优点。它是建立在离散Hopfield网基础上的，具有学习能力，它在神经元状态变化中引入了统计概率，网络的平衡状态服从Boltzmann分布，能够通过一个模拟退火过程寻求最优解。不过，其训练时间比BP网络要长。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p3.png" class title="p3"><p>原理：<br>1.上文提到Hopfield神经元的结构功能在网络中的地位是一样的,BM中一部分神经元与外部相连接，可以起到网络的输入输出作用，或者严格的说可以受到外部条件的约束，另一部分神经元不与外部相连，因而属于隐单元（相对于外部）。<br>2.每个神经元只有1/0两个状态：状态为1代表神经元处于激活(连接)状态，0表示非激活（断开）状态。</p><h2 id="RBM（受限玻尔兹曼机）">RBM（受限玻尔兹曼机）</h2><p>RBM是BM的一个变体，层间全连接，层内无连接，网络中的神经元是随机神经元。限定模型必须为二分图，学习的目标是极大似然。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p2.png" class title="p2"><h2 id="DBN（深度置信网络）">DBN（深度置信网络）</h2><p>DBN模型由若干个RBM堆叠而成，通过非监督的预学习和监督微调训练参数。<br>训练时通过从底到高逐层训练这些RBM来实现：</p><p>1.底部RBM以原始输入数据训练；<br>2.将底部RBM抽取的特征作为顶部RBM的输入训练；<br>3.过程（1）和（2）可以重复训练所需的尽可能多的参数。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p4.png" class title="p4"><h1 id="CNN卷积神经网络">CNN卷积神经网络</h1><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p5.png" class title="p5"><h2 id="特点">特点</h2><p><strong>主要特点</strong>：局部链接、参数共享、空间或时间上的子采样、（非逐层贪婪训练）。这些特性使得卷积神经网络具有一定程度上的平移、缩放和扭曲不变性。</p><p><strong>解释</strong>：CNN神经元之间的连接是非全链接，同一层中神经元之间的链接权重是共享的——减少了权值的数量，降低了网络模型的复杂度。CNN的一个卷积层中，一般包含若干个特征平面，每个特征平面由一些矩阵形排列的神经元组成，同一特征平面神经元共享权值。<br>每个卷积层之后，通常立即会有一个非线性层（激活层），目的是给一个卷积层中刚经过线性计算操作的系统引入非线性特征。</p><h2 id="各层介绍">各层介绍</h2><ul><li>池化层pooling：逐渐降低数据空间尺寸，有效减少网络中参数；</li><li>卷积层cov：通过卷积操作对输入图像进行降维和特征抽取；</li><li>全连接层：整个网络中分类器的作用；</li><li>Relu：1.采用Sigmoid计算量较大，而Relu激活函数可以减少计算过程计算量；2.防止梯度消失；3.Relu会使一部分神经元输出为0，造成网络稀疏性，从而减小参数相互依赖关系，缓解过拟合。</li></ul><h2 id="附：卷积运算">附：卷积运算</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p10.png" class title="p10"><p>本质上是一种加权求和运算，举例：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p9.png" class title="p9"><h1 id="RNN及其变种">RNN及其变种</h1><h2 id="RNN">RNN</h2><p><strong>核心思想</strong>：<br>RNN对前面信息进行记忆并且应用于当前输出计算中，隐藏层节点之间存在链接，并且隐藏层输入不仅包含输入层输出还包含上一层隐含层的输入。（可以看成是权值共享的多层前向网络）</p><p><strong>特点</strong>：</p><ul><li>分布式隐藏状态，可以有效存储过去大量信息；</li><li>以非线性动态方式更新隐藏状态。</li></ul><p><strong>参数学习算法BPTT</strong>（Backpropagation through time）：<br>（实现权值一致）</p><ul><li>前向传递：每个时间步长各单元的输出入栈；</li><li>后向传递：状态出栈，计算每个时间步长误差函数的导数；</li><li>将每个权重的所有时刻导数加和。（所有时刻的损失相加 = 总损失）</li></ul><p><strong>存在问题</strong>：</p><ul><li>梯度消失/爆炸问题；</li><li>长期依赖问题：距当前节点越远的节点对当前节点处理影响越小，无法建模长时间的依赖。</li></ul><p>——&gt;内部单元改进及变形：LSTM、GRU</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p6.png" class title="p6"><h2 id="LSTM">LSTM</h2><p>LSTM通过门结构来除去和增加“细胞状态”的信息，实现了对重要内容的保留或对不重要内容的去除（长期记忆）。通过sigmoid层输入0到1之间概率值，描述有多少信息通过。<br>门结构包括：遗忘门、信息增加门（输入门）和输出门。【长期记忆】</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p7.png" class title="p7"><h2 id="GRU">GRU</h2><p>2门控（重置门、更新门）<br>GRU是LSTM的变体，相比LSTM有更简单的结构。GRU包括了重置门和更新门（输入门+遗忘门）。【计算速度快、远距离传递、更容易训练、容易创建较大的网络】</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p8.png" class title="p8"><h2 id="BRNN">BRNN</h2><p>每个时刻都有一个正向输入的隐层和一个反向输入隐层，两个隐层分别可以表示一个词的上文信息和下文信息。即：每个词对应一个输出，同时用到了同一个词前后的信息。<br>缺点：需要完整的数据序列，你才能预测任意位置。</p><h2 id="DRNNs">DRNNs</h2><p>深度双向RNN采用多个隐层，每个隐层向后一层传递序列信息。</p><h1 id="GAN对抗网络">GAN对抗网络</h1><h2 id="核心思想">核心思想</h2><p>GAN的核心思想来源于博弈论的<strong>纳什均衡</strong>——对抗达到平衡（共同进步）：</p><ul><li>生成器（生成一个数据，会被判别结果优化）：生成器的目的是尽量去学习真实的数据分布。把噪声数据z（也就是我们说的假数据）通过生成模型G，伪装成了真实数据x；</li><li>判别器（判断是否是生成器生成的）：判别器的目的是尽量正确判别输入数据是来自真实数据还是来自生成器。<br>各自提高自己的生成能力和判别能力，这个学习优化过程就是寻找二者之间的一个纳什均衡。</li></ul><p><strong>基本原理</strong>：有一个判别器和一个生成器，生成器生成图片让判别器判别，生成器提升自己让判别器无法判别，判别器则提升自己努力识别出生成器生成的图片/序列，双方对抗达到平衡。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p11.png" class title="p11"><h2 id="学习算法">学习算法</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p12.png" class title="p12"><p>1.固定生成器G0，训练判别器，提升判别器的判别能力得到D1；【G*=arg min max V(G,D)】<br>2.固定判别器D1，训练生成器，提升生成器的生成能力，目标让判别器无法识别，得到G1；【D*=arg max V(G,D)】<br>3.再回到1中用G1训练判别器得到D2，……，依次迭代，直至两者平衡。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes4/p13.png" class title="p13"><p>解释：<br>第一步训练D，D希望V(G、D)越大越好，所以需要加上梯度。（我希望我判断能力越来越好）。<br>第二步训练G，G希望V(G，D)越小越好，所以要减去梯度。（希望让判别模糊，我希望自己的欺骗能力越来越好。<br>整个训练过程由上面两步交替进行。</p>]]></content>
    
    
    <summary type="html">国科大《高级人工智能》笔记3 —— 深度学习</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="高级人工智能" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="人工智能" scheme="https://enchantedovo.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>人工神经网络</title>
    <link href="https://enchantedovo.cn/2022/10/03/AdvancedAI-LearningNotes3/"/>
    <id>https://enchantedovo.cn/2022/10/03/AdvancedAI-LearningNotes3/</id>
    <published>2022-10-03T02:26:08.000Z</published>
    <updated>2024-04-08T13:26:00.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="人工神经网络ANN">人工神经网络ANN</h1><h2 id="结构">结构</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes3/p1.png" class title="p1"><h2 id="学习规则">学习规则</h2><ul><li>能量最小（ENERGY MINIMIZATION）</li><li>对人工神经网络，需要确定合适的能量定义；可以使用数学上的优化技术来发现如何改变神经元间的联接权重。</li></ul><h1 id="多层感知机">多层感知机</h1><p>多层感知机是ANN的一种。指具有至少三层节点，输入层，一些中间层和输出层的神经网络。给定层中的每个节点都连接到相邻层中的每个节点。输入层接收数据，中间层计算数据，输出层输出结果。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes3/p2.png" class title="p2"><h2 id="特点">特点</h2><p>多层感知机特性：</p><ul><li>多层感知机层间神经元全连接；</li><li>Can represent AND, OR, NOT, etc., but not XOR；</li><li>若训练数据集是线性可分的，则感知机模型收敛。</li></ul><h2 id="感知机存在的问题">感知机存在的问题</h2><ul><li>噪声（线性不可分）</li><li>泛化性</li></ul><h3 id="单层感知机不可解决异或问题">单层感知机不可解决异或问题</h3><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes3/p4.png" class title="p4"><blockquote><p>传送门[<a href="https://blog.csdn.net/buracag_mc/article/details/89254808?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-2.nonecase">https://blog.csdn.net/buracag_mc/article/details/89254808?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-2.nonecase</a>]</p></blockquote><h1 id="权重学习算法：BP算法">权重学习算法：BP算法</h1><h2 id="介绍">介绍</h2><p>BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：将输出误差以某种形式反传给各层所有的单元，各层按本层误差修<br>正各单元连接权值。【有监督学习，采用梯度下降法调参】</p><h3 id="特点-2">特点</h3><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/03/AdvancedAI-LearningNotes3/p3.png" class title="p3"><h2 id="BP遇到的困难，为什么会出现梯度消失">BP遇到的困难，为什么会出现梯度消失</h2><h3 id="困难">困难</h3><ul><li>梯度消失，梯度爆炸</li><li>局部极小</li><li>只能用于标注数据</li></ul><h3 id="why梯度消失">why梯度消失</h3><p>因为BP算法采用链式法则，从后层向前层传递信息时，若每层神经元对上一层神经元偏导乘以w均小于1，多次链式法则,多级导数权值相乘结果会越来越小，导致loss传递到越前方越小。</p>]]></content>
    
    
    <summary type="html">国科大《高级人工智能》笔记2 —— 人工神经网络</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="高级人工智能" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="人工智能" scheme="https://enchantedovo.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>搜索算法</title>
    <link href="https://enchantedovo.cn/2022/10/02/AdvancedAI-LearningNotes2/"/>
    <id>https://enchantedovo.cn/2022/10/02/AdvancedAI-LearningNotes2/</id>
    <published>2022-10-02T06:12:20.000Z</published>
    <updated>2024-04-08T13:26:00.854Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>内容杂且多，老师讲的也一言难尽，用的吴恩达的PPT，也没有用全，跳着讲，公式也不解释清楚变量，推导也不推，照着ppt念，太难了QAQ<br>这里仅记录较为重要的部分</p></blockquote><h1 id="一些概念">一些概念</h1><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/02/AdvancedAI-LearningNotes2/p1.png" class title="p1"> <h1 id="无信息搜索">无信息搜索</h1><p>无信息搜索也被称为盲目搜索，意味着该搜索策略没有超出问题定义提供的状态之外的附加信息。所有能做的就是生成后继节点。</p><h2 id="深度优先搜索DFS">深度优先搜索DFS</h2><p><strong>描述</strong>：每一次将深度最深的那个节点进行优先探索，若不是终点，则将其拓展并将它的子节点纳入边缘空间，若是边缘空间中所有的节点都是同一个深度，那么就探索最左边的那个节点<br><strong>优点</strong>：机器每次只需要探索最深的那个点，而且边缘空间中的节点也不会太多，不会占用太多的内存<br><strong>缺点</strong>：无法保证完备性和最优性，比较耗费时间</p><h2 id="广度优先搜索BFS">广度优先搜索BFS</h2><p><strong>描述</strong>：与深度搜索不同的是它每次拓展的是边缘空间节点中最浅的那个节点，都是同一个深度的话就拓展最左边的那个节点<br><strong>优点</strong>：具备完备性<br><strong>缺点</strong>：占用内存，而且也不具备最优性</p><h2 id="DFS和BFS比较">DFS和BFS比较</h2><h2 id="迭代深入搜索">迭代深入搜索</h2><p><strong>特点</strong>：同时结合深度优先于广度优先的优点<br><strong>描述</strong>：预先设定一个探索的层数，然后在这个层数以内采用深度优先算法，当在这个层数里面都没有找到终点的话，然后就对这个层数进行拓展，允许进行更深一层的探索<br><strong>优点</strong>：结合FDFS空间优势+BFS时间优势<br><strong>缺点</strong>：可能会造成一定的浪费冗余</p><h2 id="代价敏感搜索">代价敏感搜索</h2><p><strong>特点</strong>：算法开始关注了路程中的代价问题，两个节点之间的代价不再等效处理</p><h2 id="代价一致搜索">代价一致搜索</h2><p><strong>特点</strong>：代价敏感搜索的一种，一致代价搜索总是扩展路径消耗最小的节点N。N点的路径消耗等于前一节点N-1的路径消耗加上N-1到N节点的路径消耗</p><h1 id="启发式搜索">启发式搜索</h1><p>启发式：对每个状态估计到最近目标的距离</p><h2 id="贪婪搜索">贪婪搜索</h2><p><strong>策略</strong>：扩展你认为最接近目标状态的节点，只使用启发函数来评价节点<br><strong>缺点</strong>：只考虑了后半段的距离而没有考虑前面的距离，不能满足最优性</p><h2 id="A-算法（重要）">A*算法（重要）</h2><h2 id="算法介绍">算法介绍</h2><p><strong>特点</strong>：结合贪婪搜索与代价一致搜索<br><strong>策略</strong>：将代价函数g(n)与启发函数h(n)简单相加，得到一个新的函数f(n)，这个函数就是A*算法判断路径的标准<br><strong>估价函数</strong>：f(n)=g(n)+h(n)，其中：g(n)是到达n节点已花费的代价；f(n)是当前节点到目标节点最小代价路径的估计值（greedy）<br><strong>存在问题</strong>：当启发函数h大于实际耗散时，启发函数失效</p><h3 id="可采纳性">可采纳性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 ≤ h(n)≤ h*(n)</span><br></pre></td></tr></table></figure><p>其中：h*(n) 是从 n 结点到目标结点的最优路径的真实代价，当启发式函数 h(n) 满足以上不等式的时候，我们称该 h(n) 是可采纳的 (admissible) ，即：没有过高估计某点到目标点的花费。</p><h3 id="mark-证明A-树搜索的最优性（重要）-mark"><mark>证明A* 树搜索的最优性（重要）</mark></h3><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/02/AdvancedAI-LearningNotes2/p2.png" class title="p2"> <h3 id="A-算法其他性质">A*算法其他性质</h3><p>Open表中任一具有f(n)&lt;=f*(S0)的节点n，都被A*算法作为扩展节点</p><h3 id="最优条件">最优条件</h3><ul><li>树A*算法最优条件：可采纳性（h(n)&lt;=h*(n)）</li><li>图A*算法最优条件：一致性（f(n)沿着路径非递减）</li></ul><h2 id="A-图搜索">A*图搜索</h2><h3 id="主要思想">主要思想</h3><p>可采纳性：h(A)&lt;=h*(A)真实<br>一致性：沿路径的节点f(n)值单调递增   h(A)&lt;=cost(A to C)+h©</p><h3 id="最优性">最优性</h3><h4 id="条件">条件</h4><p>一致性：</p><ul><li>A*算法扩展节点，f值单调增</li><li>对于每个状态S，到达S最优的节点先于次优的节点扩展</li></ul><h4 id="证明">证明</h4><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/02/AdvancedAI-LearningNotes2/p3.png" class title="p3"> <h2 id="mark-传教士和野人问题的A-搜索-mark"><mark>传教士和野人问题的A*搜索</mark></h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/02/AdvancedAI-LearningNotes2/p4.png" class title="p4"><p><strong>变量解释</strong>：<br>M-左岸传教士数目<br>C-左岸野人数目<br>B-左岸是否有船<br>Pcm-有c个传教士，m个野人从左岸到右岸<br>Qcm-有c个传教士，m个野人从右岸到左岸</p><p><strong>问题有解所必须的特性</strong></p><ul><li>M&gt;=C且（3-M)&gt;=(3-C)&lt;==&gt;M=C</li><li>或者M=0,M=3</li></ul><p><strong>安全状态(以左岸为例)</strong>：</p><ul><li>传教士与野人的数目相等；</li><li>传教士都在左岸；</li><li>传教士都不在左岸。</li></ul><p><strong>完全状态图</strong>：<br>（不满足约束的不在图内）</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/02/AdvancedAI-LearningNotes2/p5.png" class title="p5"><blockquote><p>参考: <a href="https://blog.csdn.net/qq_41296039/article/details/122442921?spm=1001.2014.3001.5501">https://blog.csdn.net/qq_41296039/article/details/122442921?spm=1001.2014.3001.5501</a></p></blockquote><h1 id="局部搜索">局部搜索</h1><ul><li>爬山法：任意位置开始，可能是局部最优解；</li><li>模拟退火搜索：引入随机因素，避免局部极大；</li><li>遗传算法：适应度函数，每步保留N个最好状态。</li></ul><h1 id="其他">其他</h1><ul><li>未知梯度时，用蚁群算法/粒子群算法；</li><li>贪婪最优搜索不是完备的。</li></ul>]]></content>
    
    
    <summary type="html">国科大《高级人工智能》笔记2 —— 搜索算法</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="高级人工智能" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="人工智能" scheme="https://enchantedovo.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>设备环境初始化及激活进程0</title>
    <link href="https://enchantedovo.cn/2022/10/01/OS-LearningNotes4/"/>
    <id>https://enchantedovo.cn/2022/10/01/OS-LearningNotes4/</id>
    <published>2022-10-01T09:23:32.000Z</published>
    <updated>2024-04-08T13:26:00.860Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言">引言</h1><h2 id="设置根设备、硬盘">设置根设备、硬盘</h2><p>用<code>bootsect.s</code>中写入机器系统数据的信息设置软盘为根设备，并设置内核中的硬盘信息。</p><h2 id="规划物理内存格局">规划物理内存格局</h2><p>设置内存中除内核代码和数据之外三部分的位置和大小：</p><ul><li>主内存区：进程代码运行的空间；</li><li>缓冲区：主机与外设（块设备）交换数据的中转站；</li><li>虚拟盘区：可选，可将外设数据先复制至虚拟盘区，然后加以使用。</li></ul><blockquote><p>注：&lt;&lt;12或&gt;&gt;12相当于乘或除以4KB——页</p></blockquote><h2 id="设置虚拟盘空间并初始化">设置虚拟盘空间并初始化</h2><p>设定内存16MB，虚拟盘2MB（缓冲区末端2MB设为虚拟盘）。调用<code>rd_init()</code>函数，对虚拟盘设置：<br>将虚拟盘请求项处理函数<code>do_rd_request</code>与 请求项处理函数控制结构<code>blk_dev[7]</code>第二项挂钩，意味着内核能通过调用<code>do_rd_request</code>函数处理与虚拟盘相关的请求项操作。</p><h2 id="内存管理结构-mem-map-初始化">内存管理结构 mem_map 初始化</h2><p>调用<code>mem_init()</code>函数，通过<code>mem_map[]</code>对 1MB (内核所在区域)以上的内存分页进行管理，记录一个页面的使用次数</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/01/OS-LearningNotes4/p1.png" class title="p1"><h2 id="异常处理类中断服务程序挂接">异常处理类中断服务程序挂接</h2><p><code>trap_init()</code>函数将中断、异常处理的服务程序与IDT进行挂接，逐步重建中断服务体系。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/01/OS-LearningNotes4/p2.png" class title="p2"><p>其中，还需要拼接中断描述符：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/01/OS-LearningNotes4/p3.png" class title="p3"><h2 id="初始化块设备请求项结构">初始化块设备请求项结构</h2><p>linux外设分为块设备（硬盘、软盘）和字符设备（键盘、黑屏命令行显示器），进程想与块设备进行沟通，必须经过主机内存的缓冲区。<br>请求项管理结构<code>request[32]</code>就是操作系统管理缓冲区与块设备逻辑块读写关系的数据结构。操作系统决定缓冲块与块设备间的读写操作，并把需要操作的缓冲块记录在请求项中，得到读写块设备操作指令后，只根据请求项中的记录决定要处理哪个设备的哪个逻辑块。</p><p>初始化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">request[i].dev&#x3D;-1; &#x2F;&#x2F;请求项未对应哪个设备</span><br><span class="line">request[i].next&#x3D;NULL; &#x2F;&#x2F;还未形成请求项队列</span><br></pre></td></tr></table></figure><h2 id="与人机交互界面相关外设中断服务程序挂接">与人机交互界面相关外设中断服务程序挂接</h2><p>对串行口、显示器、键盘、开机启动时间设置，以及与此相关的中断服务程序与IDT挂接。</p><h2 id="初始化0进程（重要）">初始化0进程（重要）</h2><p>大致过程如下，都由<code>sched_init()</code>函数实现：</p><ol><li>初始化进程0<ul><li>将进程0的task_struct中LDT、TSS与GDT挂接</li><li>初始化GDT、task[64]以及与进程调度相关寄存器</li></ul></li><li>时钟中断设置 —&gt; 多进程轮转</li><li>系统调用：通过set_system_gate将system_call与IDT挂接（特权3，set_trap_gate为特权0）</li></ol><h3 id="初始化进程0">初始化进程0</h3><p>首先在GDT中初始化进程0所占的4、5项，即初始化TSS0（任务状态段）和LDT0（局部描述符）。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/01/OS-LearningNotes4/p4.png" class title="p4"><p>另外，需要拼接对应的段描述符：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/10/01/OS-LearningNotes4/p5.png" class title="p5"><p>然后还需要通过INIT_TASK指针初始化进程0的task_struct（内核栈）。每个进程都有自己的用户栈和内核栈（跑内核代码时用这个）。</p><h3 id="设置时钟中断">设置时钟中断</h3><p>时钟中断是进程0及其他由它创建的进程轮转的基础。</p><h3 id="设置系统调用总入口">设置系统调用总入口</h3><p>将系统调用函数 system_call 与 int 0x80 中断描述符表挂接。</p>]]></content>
    
    
    <summary type="html">国科大杨力祥《操作系统高级教程》课堂笔记4 —— 设备环境初始化及激活进程0</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="操作系统高级教程" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="操作系统" scheme="https://enchantedovo.cn/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="Linux Kernel" scheme="https://enchantedovo.cn/tags/Linux-Kernel/"/>
    
  </entry>
  
  <entry>
    <title>补充知识整理</title>
    <link href="https://enchantedovo.cn/2022/09/20/OS-LearningNotes3/"/>
    <id>https://enchantedovo.cn/2022/09/20/OS-LearningNotes3/</id>
    <published>2022-09-20T13:07:17.000Z</published>
    <updated>2024-04-08T13:26:00.860Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分段和分页">分段和分页</h1><blockquote><p>起因是上《操作系统高级教程》时，突然发现对这块的知识已经混沌了，所以重新总结下。<br>参考：<br><a href="https://blog.csdn.net/yzy1103203312/article/details/78529067">https://blog.csdn.net/yzy1103203312/article/details/78529067</a><br><a href="https://blog.csdn.net/qq_32740495/article/details/102924136">https://blog.csdn.net/qq_32740495/article/details/102924136</a></p></blockquote><h2 id="为何需要分段">为何需要分段</h2><p>在8086处理器诞生之前，内存寻址方式就是直接访问物理地址（实模式）。8086处理器为了寻址1M的内存空间，把<strong>地址总线扩展到了20位</strong>。但是，ALU的宽度只有16位，即ALU不能计算20位的地址。为了解决这个问题，从而引入了<strong>分段机制</strong>。</p><h2 id="IA32框架的内存寻址">IA32框架的内存寻址</h2><h3 id="三类地址">三类地址</h3><p>IA32的三类地址如下：</p><ul><li><strong>逻辑地址</strong>：<strong>机器语言指令</strong>用这类地址指定<strong>一个操作数的地址或一条指令的地址</strong>，最原始的地址就是逻辑地址。</li><li><strong>线性地址</strong>：将逻辑地址经过<strong>分段机制转换</strong>之后，便得到了线性地址，每个线性地址都<strong>由一个段基址和段内偏移量组成</strong>。</li><li><strong>物理地址</strong>：线性地址<strong>经过分页单元的处理</strong>之后得到一个实际物理地址，也就是<strong>内存单元的实际地址</strong>，用于芯片级内存单元寻址。</li></ul><blockquote><p><strong>地址空间</strong>：操作系统给每个进程用的一段连续的虚拟内存空间。这个地址范围不是真实的，是虚拟地址的范围，有时甚至会超过实际物理内存的大小。</p></blockquote><h3 id="三类地址的转化">三类地址的转化</h3><p>以上3类地址通过MMU（内存管理单元）来进行转换。其中MMU处理时包含2个过程，分段和分页。在这里简单的说明下具体过程：</p><ol><li>当一条<strong>机器指令</strong>给出一个地址时，这时候的地址便是<strong>逻辑地址</strong>；</li><li>为了得到线性地址，需要从相应的段寄存器中取出16位的段标识符（段选择符），通过这个段标识符可以得到一个段基址。然后将得到的<strong>段基址与指令中的地址相加</strong>，从而得到一个<strong>线性地址</strong>；</li><li>有了线性地址之后，再通过<strong>分页单元</strong>得到<strong>实际的物理地址</strong>。<br><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://img-blog.csdnimg.cn/76d5c9e629654292b413216ab1ed2c4a.png" alt="在这里插入图片描述"></li></ol><h2 id="分段">分段</h2><h3 id="硬件中的分段">硬件中的分段</h3><p>段是虚拟地址空间的基本单位，分段机制必须把虚拟地址空间的一个地址转换为线性地址空间的一个线性地址。<br>为了实现这种映射，仅仅用段寄存器来确定一个基地址是不够的，至少还得描述段的长度，并且还需要段的一些其他信息，比如访问权之类。所以，这里需要的是一个数据结构——段描述符，它包括三个方面的内容：</p><ul><li>段的基地址(Base Address)：在线性地址空间中段的起始地址。</li><li>段的界限(Limit)：在虚拟地址空间中，段内可以使用的最大偏移量。</li><li>段的保护属性(Attribute)：表示段的特性。例如，该段是否可被读出或写入，或者该段是否作为一个程序来执行，以及段的特权级等等。</li></ul><p>多个段描述符组成的表称为段描述符表。</p><p>逻辑地址的段寄存器中的值提供<strong>段描述符</strong>，然后从段描述符中得到<strong>段基址</strong>和<strong>段界限</strong>，然后<strong>加上逻辑地址的偏移量</strong>，就得到了线性地址，线性地址通过分页机制得到物理地址。</p><h3 id="Linux的分段">Linux的分段</h3><p>为了支持分段，8086处理器设置了四个段寄存器：CS, DS, SS, ES。每个段寄存器都是16位的，都包含着相应段的基址。访存指令中的地址也是16位的，但是，在送入地址总线之前，CPU先把它与某个段寄存器内的值按以下方式相加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际物理地址 &#x3D; （段寄存器地址 &lt;&lt; 4） + （指令访存地址）</span><br></pre></td></tr></table></figure><p>这四个段寄存器的段首地址均是0，也就是说，段首地址+逻辑地址=线性地址，这个公式里面的段首地址为0，也就意味着在linux中，<strong>逻辑地址=线性地址</strong>，这就是linux的分段技术。</p><h2 id="分页">分页</h2><p>对于物理内存，分页单元把它分为固定长度的页框（page frame），每一个页框包含一个页（page）。对于虚拟地址空间，也把它分为一个个的页。为了访问每一个物理页，需要有一个页表，记录每个物理页的起始地址，简单说通过页表就可以将一个虚拟内存中的页与具体的物理页一一对应起来。虚拟地址可以分为两部分，页号和页内偏移，页号为页表的索引，得到页的基地址后，加上偏移地址就可以得到具体的物理地址。</p><p>以32位环境为例，<strong>虚拟地址空间为4G</strong>，一般<strong>一页为4KB</strong>，这样4G内存可以分为1M个页，由于<strong>每个页表项需要4个字节</strong>来描述，因此一共需要4M的存储空间，因为每个进程都有自己的页表，所以1个进程就需要4M的存储空间。</p><h1 id="特权级CPL、DPL和RPL">特权级CPL、DPL和RPL</h1><h2 id="相关概念">相关概念</h2><p>x86 处理器中，提供了4个特权级别：0，1，2，3。数字越小，特权级别越高！一般来说，操作系统是的重要性、可靠性是最高的，需要运行在0特权级；应用程序工作在最上层，来源广泛、可靠性最低，工作在3特权级别。中间的1和2两个特权级别，一般很少使用。这几个个特权级均由两位（bit）组成，可以表示0～3共4个等级。</p><p>在处理器中，有3个相关的术语与特权级密切相关：</p><ul><li>CPL：<strong>当前进程</strong>的权限级别（Current Privilege Level），是当前<strong>正在执行的代码所在的段</strong>的特权级，存在于cs寄存器的低两位。</li><li>RPL: 说明的是<strong>进程对段访问的请求权限</strong>（Request Privilege Level），是对于段选择子而言的，每个段选择子有自己的RPL，它说明的是进程对段访问的请求权限，有点像函数参数。而且<strong>RPL对每个段来说不是固定的</strong>，两次访问同一段时的RPL可以不同。RPL可能会削弱CPL的作用，例如当前CPL=0的进程要访问一个数据段，它把段选择符中的RPL设为3，这样虽然它是0特权，但对该段仍然只有特权为3的访问权限。</li><li>DPL: 存储在段描述符中，规定<strong>访问该段的权限级别</strong>(Descriptor Privilege Level)，每个段的DPL固定。当进程访问一个段时，需要进程特权级检查，一般要求 DPL &gt;= max {CPL, RPL}</li></ul><p>在保护模式下，cpu利用cpl/rpl/dpl对程序的<strong>访问操作</strong>进行特权级检查，数据段和代码段的特权级检查规则有所不同。</p><h2 id="对数据段和堆栈段访问时的特权级控制：">对数据段和堆栈段访问时的特权级控制：</h2><p>要求访问数据段或堆栈段的程序的CPL ≤ 待访问的数据段或堆栈段的DPL，同时选择子的 RPL ≤ 待访问的数据段或堆栈段的 DPL。即程序访问数据段或堆栈段要遵循一个准则：<strong>只有相同或更高特权级的代码才能访问相应的数据段</strong>。这里，RPL可能会削弱CPL的作用，访问数据段或堆栈段时，默认用CPU和RPL中的最小特权级去访问数据段，所以max {CPL, RPL} ≤ DPL，否则访问失败。</p><h2 id="对代码段访问的特权级控制（代码执行权的特权转移）：">对代码段访问的特权级控制（代码执行权的特权转移）：</h2><p>一些“定律”：</p><ul><li>所有的程序转跳，CPU都不会把段选择子的RPL赋给转跳后程序的CS.RPL.</li><li>转跳后程序的CPL(CS.RPL)只会有下面的2种可能：<ul><li>转跳后程序的CPL(CS.RPL) = 转跳前程序的CPL(CS.RPL)</li><li>转跳后程序的CPL(CS.RPL) =　转跳后程序的CodeDescriptor.DPL</li></ul></li><li>CPU不允许程序向低特权级跳转（认为低特权级的代码不可靠，有风险）</li><li>只有一种方式能够使特权级发生改变，call + 调用门 + 非一致代码段，且当前cpl大于目标段dpl，且特权级只能向上跳转。</li></ul><h1 id="GDT-TSS-IDT-LDT">GDT TSS IDT LDT</h1><h2 id="回顾Linux寻址">回顾Linux寻址</h2><p>Linux中的寻址: logical addr --&gt; linear addr --&gt; physical addr<br>第一个转换是通过GDT的分段机制,第二个转换是通过分页机制。CPU使用logical addr, CPU中的MMU部件使用physical addr。比如一个程序编译后，代码段的指令地址是0x08048888，这就是logical addr，CPU就取这个地址。GDT是一个表，用来实现logical addr–&gt; linear addr的转化，也就是分段思想的实现。gdtr寄存器指向GDT在内存中的首地址，用CS,DS中的内容做为index，这个index的学名叫segment selector。</p><p>CS：在保护模式下的段选择器,我们一直都只把它看做一个段描述符的“索引号”，用来在 GDT (全局描述描述符表) 中查找一个段描述符。<br>用户程序拥有自己私有的描述符表 LDT(Local Descriptor Table),并且拥有自己的特权级别(总不能让用户程序与操作系统一样,工作在非常高的 0 特权级别)。</p><p>正如处理器中有一个寄存器 GDTR，保存着 GDT 的开始地址和长度；处理器中还有一个寄存器 LDTR，存储着当前正在执行的那个应用程序的 LDT 开始地址和长度。</p><h3 id="GDT">GDT</h3><p>在Protected Mode下，对一个段的描述则包括3方面因素：【Base Address, Limit, Access】，它们加在一起被放在一个64-bit长的数据结构中，被称为段描述符。但是，无法通过16-bit长度的段寄存器来直接引用64-bit的段描述符。解决的方法就是把这些长度为64-bit的段描述符放入一个数组中，而将段寄存器中的值作为下标索引来间接引用（事实上，是将段寄存器中的高13 -bit的内容作为索引）。这个全局的数组就是GDT。</p><h3 id="LDT">LDT</h3><p>除了GDT之外，IA-32还允许程序员构建与GDT类似的数据结构，它们被称作LDT（Local Descriptor Table，局部描述符表），但与GDT不同的是，LDT在系统中可以存在多个，并且从LDT的名字可以得知，LDT不是全局可见的，它们只对引用它们的任务可见，每个任务最多可以拥有一个LDT。另外，每一个LDT自身作为一个段存在，它们的段描述符被放在GDT中。</p><p>由于每个进程都有自己的一套程序段、数据段、堆栈段，有了局部描述符表则可以将每个进程的程序段、数据段、堆栈段封装在一起，只要改变LDTR就可以实现对不同进程的段进行访问。</p><h3 id="段选择子是什么？">段选择子是什么？</h3><p>保护模式下，处理器提供段寄存器，处理器提供了6个段寄存器来保存段描述符。这些段寄存器称为cs、ss、ds、es、fs和gs。每个段寄存器都由可见部分和不可见部分组成，当段选择子（可见部分）被加载至段寄存器时，处理器也通过段选择子所指向的段描述符获取了这个段的不可见部分。</p><p>段选择子段选择符为16位，描述段的一些信息，它不是直接指向段，指向在GDT或LDT中的段描述符。它的高13位作为被引用的段描述符在GDT/LDT中的下标索引，bit 2用来指定被引用段描述符被放在GDT中还是到LDT中，bit 0和bit 1是RPL——请求特权等级，被用来做保护目的。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/20/OS-LearningNotes3/p1.png" class title="p1"><p>段选择子包括三部分：描述符索引（index）、TI、请求特权级（RPL）。它的index（描述符索引）部分表示<strong>所需要的段描述符在描述符表的位置</strong>，由这个位置再根据在GDTR中存储的描述符表基址就可以找到相应的描述符。然后用描述符表中的段基址加上逻辑地址（SEL:OFFSET）的OFFSET就可以转换成线性地址，段选择子中的TI值只有一位0或1，0代表选择子是在GDT选择，1代表选择子是在LDT选择。请求特权级（RPL）则代表选择子的特权级，共有4个特权级（0级、1级、2级、3级）。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/20/OS-LearningNotes3/p2.png" class title="p2"><h3 id="TSS-任务状态段">TSS: 任务状态段</h3><p>顾名思义，任务状态段就是用来存储和恢复任务的状态信息。</p><p>经常听到一个术语:任务上下文。<br>TSS是一个特殊的段。在Linux中，CPU从系统态切换到用户态时，会用到TSS里面的ss0和esp0。每个CPU只维护一个TSS。TR寄存器指向这个TSS，切换时里面的ss0和esp0会有改变。相应有一个TSSD放在GDT中，是GDT的一个表项。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/20/OS-LearningNotes3/p3.png" class title="p3"><p>可以看到：任务寄存器中可见部分的段选择符加载TSS描述符的段选择符，访问GDT中TSS描述符，通过TSS描述符访问TSS。同时TSS描述符中的基址和界限字段的值又加载到任务寄存器的不可见部分的基址和界限字段，这样做的目的是下次访问该TSS时可以不用通过GDT中的TSS描述符访问TSS，而是直接通过缓存在任务寄存器中的基址和界限字段访问TSS，加快了系统对TSS的访问。</p>]]></content>
    
    
    <summary type="html">国科大杨力祥《操作系统高级教程》课堂笔记3 —— 补充知识</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="操作系统高级教程" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="操作系统" scheme="https://enchantedovo.cn/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="Linux Kernel" scheme="https://enchantedovo.cn/tags/Linux-Kernel/"/>
    
  </entry>
  
  <entry>
    <title>链路层协议安全</title>
    <link href="https://enchantedovo.cn/2022/09/18/NetProtoSec-LearningNotes3/"/>
    <id>https://enchantedovo.cn/2022/09/18/NetProtoSec-LearningNotes3/</id>
    <published>2022-09-18T09:01:19.000Z</published>
    <updated>2024-04-08T13:26:00.857Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本节分为三个部分：</p><ul><li>有线网络协议安全</li><li>无线网络协议安全</li><li>VLAN安全</li></ul></blockquote><h1 id="链路层协议安全">链路层协议安全</h1><h2 id="有线网络协议安全">有线网络协议安全</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p1.png" class title="p1"><h2 id="无线网络协议安全">无线网络协议安全</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p2.png" class title="p2"><h2 id="VLAN安全">VLAN安全</h2><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p3.png" class title="p3"><h3 id="特点">特点</h3><p>VLAN与传统LAN相比，具有以下特点：</p><ul><li>隔离广播域</li><li>创建虚拟工作组，超越传统网络的工作方式，减少改变的代价，安全性⬆，健壮性⬆</li></ul><p>VLAN的划分方法有以下几种：</p><ul><li>基于端口（静态）</li><li>基于MAC（动态）</li><li>基于协议</li><li>基于子网</li></ul><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p4.png" class title="p4"><h3 id="三层交换机">三层交换机</h3><p>不同VLAN的信息必须通过三层路由处理才能转发到端口上。</p><h4 id="二层交换机与路由器">二层交换机与路由器</h4><p>二层交换机：数据交换靠硬件，但不能处理不同子网(包括VLAN)间数据交换；<br>路由器：主要功能是路由转发，通过软件实现，所以转发效率低。</p><h4 id="三层交换技术">三层交换技术</h4><p>三层交换技术其实就是：二层交换技术+三层转发技术。它能做到<strong>一次路由，多次转发</strong>，对于数据包转发等规律性的过程由硬件高速实现，而像路由信息更新、路由表维护、路由计算等功能，由软件实现。</p><h4 id="工作过程">工作过程</h4><p>1.第一个包过来（包目的MAC是自己，交给IP层，发现IP地址不是自己），硬件转发表无表项，交给路由进程处理，查找下一跳IP，通过ARP得到MAC；<br>2.转发时：修改IP包ttl；修改原mac；建立硬件转发表；<br>3.下一包，直接查看硬件转发表直接转发，而不会经过路由表查询。</p><h3 id="链路类型">链路类型</h3><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p5.png" class title="p5"><p>1.干道链路<br>可以承载多个不同VLAN数据，用于交换机间、交换机与路由器间的互连，<strong>它不属于任何一个具体VLAN</strong>。所有在干道链路上传输的帧都是打上标记的帧。</p><p>2.接入链路<br>用于连接主机和交换机的链路，接入链路属于某一个特定的端口，这个端口属于一个并且只能是一个VLAN。</p><h3 id="帧在网络通信中的变化">帧在网络通信中的变化</h3><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p6.png" class title="p6"><h3 id="本征VLAN">本征VLAN</h3><p>相当于默认VLAN，可以在Trunk链路上指定一个Native VLAN。Access端口只属于一个VLAN，所以它的缺省ID就是它所在的VLAN，不用设置；Trunk端口属于多个VLAN， 所以需要设置缺省VLAN ID，缺省情况下为VLAN 1。<br>来自Native VLAN的数据帧通过Trunk链路时不重新封装，以原有的帧传输（不重新打标签）。</p><h3 id="转发原则-由交换机实现">转发原则(由交换机实现)</h3><h4 id="Access-Link">Access-Link</h4><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p7.png" class title="p7"><h4 id="Trunk-Link">Trunk-Link</h4><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p8.png" class title="p8"><h3 id="VLAN安全威胁">VLAN安全威胁</h3><h4 id="双标签跳跃攻击">双标签跳跃攻击</h4><p>原理：构造含有双层标签的帧，其外层标记为Trunk链路的 Native VLAN 号，交换机trunk端口发送帧时，将外层标签去掉<br>缺点：只能单项攻击<br>防范：native VLAN设置为一个不存在的VLAN</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p9.png" class title="p9"><h4 id="DTP跳跃攻击">DTP跳跃攻击</h4><p>DTP：有设备主动向接口发起协议，接口将形成Trunk，该链路可以双向传输任何VLAN数据<br>特点：可双向访问，绕过防火墙</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/18/NetProtoSec-LearningNotes3/p10.png" class title="p10"><h4 id="VPT攻击">VPT攻击</h4><p>原理：VLAN信息的同步是通过VTP通告来实现的， VTP通告只能在Trunk链路上传输（因此交换机之间的Trunk链路必须成功配置了Trunk）<br>思路：攻击者发送高的Revision的VTP通告，就能把网络中的VLAN信息覆盖了。</p>]]></content>
    
    
    <summary type="html">国科大《网络协议安全》链路层协议安全学习记录</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="网络协议安全" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%AE%89%E5%85%A8/"/>
    
    
    <category term="网络安全" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
    <category term="网络协议" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>网络与系统安全随记</title>
    <link href="https://enchantedovo.cn/2022/09/09/NetSysSec-LearningNotes/"/>
    <id>https://enchantedovo.cn/2022/09/09/NetSysSec-LearningNotes/</id>
    <published>2022-09-09T11:06:26.000Z</published>
    <updated>2024-04-08T13:26:00.858Z</updated>
    
    <content type="html"><![CDATA[<h1 id="20220909">20220909</h1><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/09/NetSysSec-LearningNotes/p1.png" class title="p1"> ]]></content>
    
    
    <summary type="html">网络与系统安全课堂随记</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="网络与系统安全" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"/>
    
    
    <category term="网络与系统安全" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>主机和端口扫描</title>
    <link href="https://enchantedovo.cn/2022/09/07/NetProtoSec-LearningNotes2/"/>
    <id>https://enchantedovo.cn/2022/09/07/NetProtoSec-LearningNotes2/</id>
    <published>2022-09-07T11:51:56.000Z</published>
    <updated>2024-04-08T13:26:00.857Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>实验环境：Kali+Wireshark+Nmap</p><ul><li>Kali：是一个基于Debian的Linux发行版，预装了许多渗透测试软件，是一个很酷的“黑客操作系统”；</li><li>Wireshark：是一个网络封包分析软件，功能是截取网络封包，并尽可能显示出最为详细的网络封包资料。Wireshark使用WinPCAP作为接口，直接与网卡进行数据报文交换；</li><li>Nmap：即Network Mapper，是Linux下的网络扫描和嗅探工具包。</li></ul></blockquote><h1 id="实验内容">实验内容</h1><p>1.在Nmap中，使用nmap –sP xx指令实现主机扫描，用wireshark抓取扫描过程中的ARP请求和响应报文；通过ping指令发起ICMP扫描，用wireshark抓取扫描过程中的ICMP报文；</p><p>2.在Nmap中，使用nmap –sS xx指令实现端口扫描，用wireshark抓取扫描过程中的TCP报文。</p><h1 id="kali网络配置">kali网络配置</h1><p>由于我使用的是校园网，因此我这里使用NAT模式。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p1.png" class title="p1"> <h2 id="1-查看主机地址">1.查看主机地址</h2><p>首先通过<code>win</code>+<code>R</code>输入cmd打开终端，然后通过<code>ipconfig</code>查看自己主机的ip地址，网关，网段（我这里选择VMnet8）。</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p2.png" class title="p2"> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IPv4 地址: 192.168.253.1</span><br><span class="line">子网掩码: 255.255.255.0</span><br></pre></td></tr></table></figure><h2 id="2-修改-etc-network-interfaces文件">2.修改/etc/network/interfaces文件</h2><p>然后，通过<code>vim /etc/network/interfaces</code>，设置静态IP：<br>address：把ip地址设置为自己主机网段中的一个，如：我的主机ip是192.168.253.1，这里设为192.168.253.36<br>gateway：在其中加上自己主机的网关，如：192.168.253.2<br>netmask：子网掩码照抄自己主机的，如：255.255.255.0</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet static</span><br><span class="line">address 192.168.253.36</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line">gateway 192.168.253.2</span><br></pre></td></tr></table></figure><h2 id="3-修改dns">3.修改dns</h2><p><code>vim /etc/resolv.conf</code>添加几个常用DNS：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">domain localdomain</span><br><span class="line">search localdomain</span><br><span class="line">nameserver 8.8.8.8</span><br><span class="line">nameserver 114.114.114.114</span><br><span class="line">nameserver 192.168.253.2</span><br></pre></td></tr></table></figure><h2 id="4-重启网络">4.重启网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart networking</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;etc&#x2F;init.d&#x2F;networking restart</span><br></pre></td></tr></table></figure><h2 id="5-测试">5.测试</h2><p>输入<code>ifconfig -a</code>查看ip：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p3.png" class title="p3"> <h2 id="nmap参数速查">nmap参数速查</h2><table><thead><tr><th style="text-align:center">命令</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">nmap IP</td><td style="text-align:center">扫描IP</td></tr><tr><td style="text-align:center">nmap -v IP</td><td style="text-align:center">加强扫描</td></tr><tr><td style="text-align:center">nmap IP1 IP2 …</td><td style="text-align:center">扫描多IP</td></tr><tr><td style="text-align:center">nmap a.b.c.*</td><td style="text-align:center">扫描整个子网</td></tr><tr><td style="text-align:center">nmap a.b.c.x,y,…</td><td style="text-align:center">扫描多子网地址</td></tr><tr><td style="text-align:center">nmap -iL xxx.txt</td><td style="text-align:center">根据文件扫描多IP</td></tr><tr><td style="text-align:center">nmap a.b.c.x-y</td><td style="text-align:center">扫描子网IP范围</td></tr><tr><td style="text-align:center">nmap a.b.c.* --exclude IP</td><td style="text-align:center">排除指定IP扫描整个子网</td></tr><tr><td style="text-align:center">nmap -A IP</td><td style="text-align:center">全面的系统扫描,包括操作系统探测、版本探测、脚本扫描、路径跟踪</td></tr><tr><td style="text-align:center">nmap -O IP</td><td style="text-align:center">探测操作系统</td></tr><tr><td style="text-align:center">nmap -sA/-PN IP</td><td style="text-align:center">探测防火墙</td></tr><tr><td style="text-align:center">nmap -sP a.b.c.*</td><td style="text-align:center">探测在线主机</td></tr><tr><td style="text-align:center">nmap -F IP</td><td style="text-align:center">快速扫描</td></tr><tr><td style="text-align:center">nmap -r IP</td><td style="text-align:center">按顺序扫描</td></tr><tr><td style="text-align:center">nmap -iflist</td><td style="text-align:center">显示接口和路由信息</td></tr><tr><td style="text-align:center">nmap -p n1,n2… IP</td><td style="text-align:center">扫描指定端口</td></tr><tr><td style="text-align:center">nmap -p T:n1,n2… IP</td><td style="text-align:center">扫描TCP端口</td></tr><tr><td style="text-align:center">nmap -sU n1,n2… IP</td><td style="text-align:center">扫描UDP端口</td></tr><tr><td style="text-align:center">nmap -sV IP</td><td style="text-align:center">查看服务的版本</td></tr><tr><td style="text-align:center">nmap -PS IP</td><td style="text-align:center">TCP ACK扫描</td></tr><tr><td style="text-align:center">nmap -PA IP</td><td style="text-align:center">TCP SYN扫描</td></tr><tr><td style="text-align:center">nmap -sS IP</td><td style="text-align:center">隐蔽扫描</td></tr><tr><td style="text-align:center">nmap -sN IP</td><td style="text-align:center">TCP空扫描欺骗防火墙</td></tr></tbody></table><h1 id="主机扫描">主机扫描</h1><blockquote><p>开两个虚拟机：192.168.253.36 + 192.168.253.37</p></blockquote><h2 id="主机扫描参数">主机扫描参数</h2><p>主机发现有时候也叫做ping扫描，有以下选项：<br><code>-sL</code>（列表扫描）：它仅仅列出指定网络上的每台主机，不发送任何报文到目标主机；<br><code>-sP</code>（Ping扫描）：该选项告诉Nmap仅仅进行ping扫描（主机发现），然后打印出对扫描做出响应的那些主机；Nmap会发一个ICMP ECHO请求和一个TCP报文到目标端口；<br><code>-Pn</code>（无ping）：无Ping扫描通常用于防火墙禁止Ping的情况下，完全跳过Nmap发现阶段，且对目标主机进行端口扫描；<br><code>-PS</code>（TCP SYN ping）：该选项发送了一个设置了SYN标志位的空TCP报文，需要root权限；<br><code>-PA</code>（TCP ACK ping）：它与TCP SYN Ping扫描类似，不同的只是设置的TCP报文的标志位是ACK；<br><code>-PU</code>（UDP  ping）：发送一个空的UDP报文到指定端口，很少用；<br><code>-PE/PP/PM</code>（ICMP ECHO/时间戳/地址掩码Ping）；<br><code>-ARP</code>（ARP ping）：该选项通常在扫描局域网时使用，在内网中使用ARP Ping是非常有效的。</p><h2 id="实验过程">实验过程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nmap -P 192.168.253.1/24</span><br></pre></td></tr></table></figure><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p4.png" class title="p4"> <p>wireshark抓取ARP请求和响应报文：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p5.png" class title="p5"><p>ping命令发起ICMP扫描，wireshark抓取ICMP报文：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p6.png" class title="p6"><h1 id="端口扫描">端口扫描</h1><h2 id="端口扫描基础">端口扫描基础</h2><h3 id="端口状态">端口状态</h3><p>Nmap所识别的6个端口状态：</p><ul><li><code>Opend</code>：端口开启；</li><li><code>Closed</code>： 端口关闭；</li><li><code>Filtered</code>：端口被过滤，数据没有到达主机，返回的结果为空，数据被防火墙拦截了；</li><li><code>Unfiltered</code>：未被过滤，数据有到达主机，但是不能识别端口的当前状态；</li><li><code>Open|filtered</code>：开放或者被过滤，端口没有返回值，主要发生在UDP、IP、FIN、NULL和X mas扫描中；</li><li><code>Closed|filtered</code>：关闭或者被过滤，只发生在IP ID idle扫描。</li></ul><h3 id="命令参数">命令参数</h3><p>-<code>sS</code>（TCP SYN扫描）：半开扫描，很少有系统能把它记入系统日志。不过，需要Root权限；<br>-<code>sT</code>（TCP connect()扫描）:当SYN扫描不能用时，CP Connect()扫描就是默认的TCP扫描；<br>-<code>sN/-sF/-sX</code>（TCP Null/FIN/Xmas扫描）:能躲过一些无状态防火墙和报文过滤路由器；<br>-<code>p</code>：指定范围性扫描端口；<br>-<code>v</code>：详细信息。</p><h2 id="实验过程-2">实验过程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">┌──(root㉿kali)-[~]</span><br><span class="line">└─<span class="comment"># nmap -sS 192.168.253.1</span></span><br><span class="line">Starting Nmap 7.92 ( https://nmap.org ) at 2022-09-07 22:42 EDT</span><br><span class="line">Nmap scan report <span class="keyword">for</span> 192.168.253.1</span><br><span class="line">Host is up (0.00019s latency).</span><br><span class="line">Not shown: 996 filtered tcp ports (no-response)</span><br><span class="line">PORT     STATE SERVICE</span><br><span class="line">135/tcp  open  msrpc</span><br><span class="line">139/tcp  open  netbios-ssn</span><br><span class="line">445/tcp  open  microsoft-ds</span><br><span class="line">4001/tcp open  newoak</span><br><span class="line">MAC Address: 00:50:56:C0:00:08 (VMware)</span><br><span class="line"></span><br><span class="line">Nmap <span class="keyword">done</span>: 1 IP address (1 host up) scanned <span class="keyword">in</span> 4.70 seconds</span><br></pre></td></tr></table></figure><p>打开wireshark抓取tcp包，可看到：</p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/2022/09/07/NetProtoSec-LearningNotes2/p7.png" class title="p7"><p>扫描主机的135和139端口已经回发数据了。</p>]]></content>
    
    
    <summary type="html">国科大《网络协议安全》实验1 —— 主机和端口扫描</summary>
    
    
    
    <category term="国科大课程笔记" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    <category term="网络协议安全" scheme="https://enchantedovo.cn/categories/%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%AE%89%E5%85%A8/"/>
    
    
    <category term="网络安全" scheme="https://enchantedovo.cn/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
    <category term="Wireshark" scheme="https://enchantedovo.cn/tags/Wireshark/"/>
    
    <category term="Nmap" scheme="https://enchantedovo.cn/tags/Nmap/"/>
    
  </entry>
  
</feed>
